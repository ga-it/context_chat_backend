<!DOCTYPE html>
<!-- saved from url=(0040)https://r2r-docs.sciphi.ai/llms-full.txt -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    
    <link rel="icon" href="https://files.buildwithfern.com/https://sciphi.docs.buildwithfern.com/2025-05-20T17:18:43.700Z/favicon.ico">
  </head>
  <body>
    <pre># Introduction

&gt; The most advanced AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.


![r2r](file:4a056473-78e9-4ae3-bca3-dce2127a0d78)

R2R is an all-in-one solution for AI Retrieval-Augmented Generation (RAG) with production-ready features, including multimodal content ingestion, hybrid search functionality, configurable GraphRAG, and user/document management.

R2R also includes a **Deep Research API**, a multi-step reasoning system that fetches relevant data from your knowledgebase and/or the internet to deliver richer, context-aware answers for complex queries.

***

# Cloud Documentation

## Getting Started

* üöÄ **[Quickstart](/documentation/quickstart)** A quick introduction to R2R's core features.
* ‚ùáÔ∏è **[API &amp; SDKs](/api-and-sdks/introduction)** API reference and Python/JS SDKs for interacting with R2R.

## Key Features

### Ingestion &amp; Retrieval

* **üìÅ [Multimodal Ingestion](/self-hosting/configuration/ingestion)** Parse `.txt`, `.pdf`, `.json`, `.png`, `.mp3`, and more.
* **üîç [Hybrid Search](/documentation/search-and-rag)** Combine semantic and keyword search with reciprocal rank fusion for enhanced relevancy.
* **üîó [Knowledge Graphs](/cookbooks/graphs)** Automatically extract entities and relationships to build knowledge graphs.
* **ü§ñ [Agentic RAG](/documentation/retrieval/agentic-rag)** R2R's powerful Deep Research agent integrated with RAG over your knowledgebase.

### Application Layer

* üíª **[Web Development](/cookbooks/web-dev)** Building web apps using R2R.
* üîê **[User Auth](/documentation/user-auth)** Authenticating users.
* üìÇ **[Collections](/self-hosting/collections)** Document collections management.
* üåê **[Web Application](/cookbooks/web-dev)** Connecting with the R2R Application.

### Self-Hosting

* üêã **[Docker](/self-hosting/installation/full)** Use Docker to easily deploy the full R2R system into your local environment
* üß© **[Configuration](/self-hosting/configuration/overview)** Set up your application using intuitive configuration files.

***

# Community

[Join our Discord server](https://discord.gg/p6KqD2kjtB) to get support and connect with both the R2R team and other developers. Whether you're encountering issues, seeking best practices, or sharing your experiences, we're here to help.

***

# About

* **üåê [SciPhi Website](https://sciphi.ai/)** Explore a managed AI solution powered by R2R.
* **‚úâÔ∏è [Contact Us](mailto:founders@sciphi.ai)** Get in touch with our team to discuss your specific needs.


# System

&gt; Learn about the R2R system architecture

## System Diagram

```mermaid
graph TD
    U((User)) --&gt;|Query| GW[Traefik Gateway]
    GW --&gt;|Route| API[R2R API Cluster]
    API --&gt;|Authenticate| AS[Auth Service]

    R2R[R2R Application] --&gt;|Use| API

    subgraph "Core Services"
        AS
        ReS[Retrieval Service]
        IS[Ingestion Service]
        GBS[Graph Builder Service]
        AMS[App Management Service]
    end

    subgraph "Providers"
        EP[Embedding Provider]
        LP[LLM Provider]
        AP[Auth Provider]
        IP[Ingestion Provider]
    end

    IS &amp; GBS &amp; ReS &lt;--&gt;|Coordinate| O

    ReS --&gt;|Use| EP
    ReS --&gt;|Use| LP
    IS --&gt;|Use| EP
    IS --&gt;|Use| IP
    GBS --&gt;|Use| LP
    AS --&gt;|Use| AP

    subgraph "Orchestration"
        O[Orchestrator]
        RMQ[RabbitMQ]
        O &lt;--&gt;|Use| RMQ
    end

    subgraph "Storage"
        PG[(Postgres + pgvector)]
        FS[File Storage]
    end

    AS &amp; AMS &amp; ReS --&gt;|Use| PG
    GBS &amp; ReS --&gt;|Use| Neo
    IS --&gt;|Use| FS

    classDef gateway fill:#2b2b2b,stroke:#ffffff,stroke-width:2px;
    classDef api fill:#4444ff,stroke:#ffffff,stroke-width:2px;
    classDef orchestrator fill:#007acc,stroke:#ffffff,stroke-width:2px;
    classDef messagequeue fill:#2ca02c,stroke:#ffffff,stroke-width:2px;
    classDef storage fill:#336791,stroke:#ffffff,stroke-width:2px;
    classDef providers fill:#ff7f0e,stroke:#ffffff,stroke-width:2px;
    classDef auth fill:#ff0000,stroke:#ffffff,stroke-width:2px;
    classDef application fill:#9932cc,stroke:#ffffff,stroke-width:2px;
    class GW gateway;
    class API api;
    class O orchestrator;
    class RMQ messagequeue;
    class PG,Neo,FS storage;
    class EP,LP,AP,IP providers;
    class AS auth;
    class R2R application;
```

## System Overview

R2R is built on a modular, service-oriented architecture designed for scalability and flexibility:

1. **API Layer**: A RESTful API cluster handles incoming requests, routing them to appropriate services.

2. **Core Services**: Specialized services for authentication, retrieval, ingestion, graph building, and app management.

3. **Orchestration**: Manages complex workflows and long-running tasks using a message queue system.

4. **Storage**: Utilizes Postgres with `pgvector` and full-text search for vector storage and search, and graph search.

5. **Providers**: Pluggable components for parsing, embedding, authenticating, and retrieval-augmented generation.

6. **R2R Application**: A React+Next.js app providing a user interface for interacting with the R2R system.

This architecture enables R2R to handle everything from simple RAG applications to complex, production-grade systems with advanced features like hybrid search and GraphRAG.

Ready to get started? Check out our [Docker installation guide](/self-hosting/installation/full) and [Quickstart tutorial](/documentation/quickstart) to begin your R2R journey.


# What's New

&gt; Changelog

## Version 0.3.5 ‚Äî March. 2025

### New Features

* Improved API released for Agentic RAG  [(agentic-rag)](/documentation/retrieval/agentic-rag)
* SSE streaming output for RAG
* Improved citations for robust provenance

### Bug Fixes

* Minor bug fixes around local LLM setup / operation


# What is R2R?

**On this page**

1. What does R2R do?
2. What can R2R do for my applications?
3. What can R2R do for my developers?
4. What can R2R do for my business?
5. Getting started

Companies like OpenAI, Anthropic, and Google have shown the incredible potential of AI for understanding and generating human language. But building reliable AI applications that can work with your organization's specific knowledge and documents requires significant expertise and infrastructure. Your company isn't an AI infrastructure company: **it doesn't make sense for you to build a complete AI retrieval ([RAG](/introduction/rag)) system from scratch.**

R2R provides the infrastructure and tools to help you implement **efficient, scalable, and reliable AI-powered document understanding** in your applications.

## What does R2R do?

R2R consists of three main components: **document processing**, **AI-powered search and generation**, and **analytics**. The document processing and search capabilities make it easier for your developers to create intelligent applications that can understand and work with your organization's knowledge. The analytics tools enable your teams to monitor performance, understand usage patterns, and continuously improve the system.

## What can R2R do for my applications?

R2R provides your applications with production-ready RAG capabilities:

* Fast and accurate document search using both semantic and keyword matching
* Intelligent document processing that works with PDFs, images, audio, and more
* Automatic relationship extraction to build knowledge graphs
* Built-in user management and access controls
* Simple integration through REST APIs and SDKs

## What can R2R do for my developers?

R2R provides a complete toolkit that simplifies building AI-powered applications:

* [**Ready-to-use Docker deployment**](/self-hosting/installation/overview) for quick setup and testing
* [**Python and JavaScript SDKs**](/api-and-sdks/introduction) for easy integration
* **RESTful API** for language-agnostic access
* [**Flexible configuration**](/self-hosting/configuration/overview) through intuitive config files
* **Comprehensive documentation** and examples
* [**Local deployment option**](/self-hosting/local-rag) for working with sensitive data

## What can R2R do for my business?

R2R provides the infrastructure to build AI applications that can:

* **Make your documents searchable** with state of the art AI
* **Answer questions** using your organization's knowledge
* **Process and understand** documents at scale
* **Secure sensitive information** through built-in access controls
* **Monitor usage and performance** through analytics
* **Scale efficiently** as your needs grow

## Getting Started

The fastest way to start with R2R is through Docker:

```zsh
pip install r2r
r2r serve --docker
```

This gives you a complete RAG system running at [http://localhost:7272](http://localhost:7272) with:

* Document ingestion and processing
* Vector search capabilities
* GraphRAG features
* User management
* Analytics dashboard

Visit our [Quickstart Guide](/documentation/quickstart) to begin building with R2R.


# More about RAG

**On this page**

1. Before you begin
2. What is RAG?
3. Set up RAG with R2R
4. Configure RAG settings
5. How RAG works in R2R

RAG (Retrieval-Augmented Generation) combines the power of large language models with precise information retrieval from your own documents. When users ask questions, RAG first retrieves relevant information from your document collection, then uses this context to generate accurate, contextual responses. This ensures AI responses are both relevant and grounded in your specific knowledge base.

**Before you begin**

RAG in R2R has the following requirements:

* A running R2R instance (local or deployed)
* Access to an LLM provider (OpenAI, Anthropic, or local models)
* Documents ingested into your R2R system
* Basic configuration for document processing and embedding generation

## What is RAG?

RAG operates in three main steps:

1. **Retrieval**: Finding relevant information from your documents
2. **Augmentation**: Adding this information as context for the AI
3. **Generation**: Creating responses using both the context and the AI's knowledge

Benefits over traditional LLM applications:

* More accurate responses based on your specific documents
* Reduced hallucination by grounding answers in real content
* Ability to work with proprietary or recent information
* Better control over AI outputs

## Set up RAG with R2R

To start using RAG in R2R:

1. Install and start R2R:

```zsh
pip install r2r
r2r serve --docker
```

2. Ingest your documents:

```zsh
r2r documents create --file-paths /path/to/your/documents
```

3. Test basic RAG functionality:

```zsh
r2r retrieval rag --query="your question here"
```

## Configure RAG settings

R2R offers several ways to customize RAG behavior:

1. **Retrieval Settings**:

```python
# Using hybrid search (combines semantic and keyword search)
client.retrieval.rag(
    query="your question",
    vector_search_settings={"use_hybrid_search": True}
)

# Adjusting number of retrieved chunks
client.retrieval.rag(
    query="your question",
    vector_search_settings={"limit": 30}
)
```

2. **Generation Settings**:

```python
# Adjusting response style
client.retrieval.rag(
    query="your question",
    rag_generation_config={
        "temperature": 0.7,
        "model": "openai/gpt-4"
    }
)
```

## How RAG works in R2R

R2R's RAG implementation uses a sophisticated process:

**Document Processing**

* Documents are split into semantic chunks
* Each chunk is embedded using AI models
* Chunks are stored with metadata and relationships

**Retrieval Process**

* Queries are processed using hybrid search
* Both semantic similarity and keyword matching are considered
* Results are ranked by relevance scores

**Response Generation**

* Retrieved chunks are formatted as context
* The LLM generates responses using this context
* Citations and references can be included

**Advanced Features**

* GraphRAG for relationship-aware responses
* Multi-step RAG for complex queries
* Agent-based RAG for interactive conversations

## Best Practices

1. **Document Processing**
   * Use appropriate chunk sizes (256-1024 tokens)
   * Maintain document metadata
   * Consider document relationships

2. **Query Optimization**
   * Use hybrid search for better retrieval
   * Adjust relevance thresholds
   * Monitor and analyze search performance

3. **Response Generation**
   * Balance temperature for creativity vs accuracy
   * Use system prompts for consistent formatting
   * Implement error handling and fallbacks

For more detailed information, visit our [RAG Configuration Guide](/self-hosting/configuration/retrieval/rag) or try our [Quickstart](/documentation/quickstart).


# Overview

R2R is the most advanced AI retrieval system. And with R2R, getting your AI application started is simple.

R2R offers powerful features for your applications, including:

* **Cutting Edge Search**: Advanced RAG techniques like [hybrid search](/documentation/hybrid-search), [knowledge graphs](/documentation/graphs), [advanced RAG](/documentation/advanced-rag), and [agentic retrieval](/documentation/retrieval/agentic-rag).
* **Flexibility**: Runtime configuration makes it easy to adjust and tune R2R to fit your needs.
* **Scale**: Handle increasing workloads and large datasets, designed specifically for performance.
* **Auth &amp; Collection**: Production must-haves like user [auth](/documentation/user-auth) and [document collections](/documentation/collections).

<cardgroup cols="{2}">
  <card title="Cloud" icon="cloud" href="/documentation/quickstart">
    Get started using R2R through SciPhi Cloud, free of charge. **Perfect for fast serverless deployment**.
  </card>

  <card title="Self Hosted" icon="server" href="/self-hosting/installation/overview">
    Host your own full-featured R2R system. Ideal **for on premise use cases**.
  </card>
</cardgroup>

Choose the system that best aligns with your requirements and proceed with the documentation.


# Quickstart

Getting started with R2R is easy.

<steps>
  ### Create an Account

  Create an account with [SciPhi Cloud](https://app.sciphi.ai). It's free!

  <note>
    For those interested in deploying R2R locally, please <a href="https://r2r-docs.sciphi.ai/self-hosting/installation/overview"> refer here</a>.
  </note>

  ### Install the SDK

  R2R offers Python and JavaScript SDKs to interact with.

  <tabs>
    <tab title="Python">
      ```zsh
      pip install r2r
      ```
    </tab>

    <tab title="JavaScript">
      ```zsh
      npm i r2r-js
      ```
    </tab>
  </tabs>

  ### Environment

  After signing into [SciPhi Cloud](https://app.sciphi.ai), navigate to the homepage and click `Create New Key`  (*for the self-hosted quickstart, [refer here](/self-hosting/quickstart)*):
  ![API Key](file:14113b57-06ae-4af0-ac47-a40e4072030c)

  Next, set your local environment variable `R2R_API_KEY`. Be sure to include the entire API key \``pk_..`**.**`sk_...`\`. Alternatively, you may login directly with the client.

  ### Client

  <tabs>
    <tab title="Python">
      ```python
      # export R2R_API_KEY=...
      from r2r import R2RClient

      client = R2RClient() # can set remote w/ R2RClient(base_url=...)

      # or, alternatively, client.users.login("my@email.com", "my_strong_password")
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      // export R2R_API_KEY=...
      const { r2rClient } = require('r2r-js');

      const client = new r2rClient(); // can set baseURL=...

      // or, alternatively, client.users.login("my@email.com", "my_strong_password")

      ```
    </tab>
  </tabs>

  ### Ingesting files

  When you ingest files into R2R, the server accepts the task, processes and chunks the file, and generates a summary of the document.

  <tabs>
    <tab title="Python">
      ```python
      client.documents.create_sample(hi_res=True)
      # to ingest your own document, client.documents.create(file_path="/path/to/file")
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      client.documents.createSample({ ingestionMode: "hi-res" })
      // to ingest your own document, client.documents.create({filePath: })
      ```
    </tab>
  </tabs>

  Example output:

  ```plaintext
  IngestionResponse(message='Document created and ingested successfully.', task_id=None, document_id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'))
  ```

  ### Getting file status

  After file ingestion is complete, you can check the status of your documents by listing them.

  <tabs>
    <tab title="Python">
      ```python
      client.documents.list()
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      client.documents.list()
      ```
    </tab>

    <tab title="Curl">
      ```zsh
      curl -X GET https://api.sciphi.ai/v3/documents \
        -H "Content-Type: application/json"
      ```
    </tab>
  </tabs>

  Example output:

  ```plaintext
  [
    DocumentResponse(
      id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'), 
      collection_ids=[UUID('122fdf6a-e116-546b-a8f6-e4cb2e2c0a09')], 
      owner_id=UUID('2acb499e-8428-543b-bd85-0d9098718220'), 
      document_type=<documenttype.pdf: 'pdf'="">, 
      metadata={'title': 'DeepSeek_R1.pdf', 'version': 'v0'}, 
      version='v0', 
      size_in_bytes=1768572, 
      ingestion_status=<ingestionstatus.success: 'success'="">, 
      extraction_status=<graphextractionstatus.pending: 'pending'="">, 
      created_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 126759, tzinfo=TzInfo(UTC)), 
      updated_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 160114, tzinfo=TzInfo(UTC)), 
      ingestion_attempt_number=None, 
      summary="The document contains a comprehensive overview of DeepSeek-R1, a series of reasoning models developed by DeepSeek-AI, which includes DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero utilizes large-scale reinforcement learning (RL) without supervised fine-tuning, showcasing impressive reasoning capabilities but facing challenges like readability and language mixing. To enhance performance, DeepSeek-R1 incorporates multi-stage training and cold-start data, achieving results comparable to OpenAI's models on various reasoning tasks. The document details the models' training processes, evaluation results across multiple benchmarks, and the introduction of distilled models that maintain reasoning capabilities while being smaller and more efficient. It also discusses the limitations of current models, such as language mixing and sensitivity to prompts, and outlines future research directions to improve general capabilities and efficiency in software engineering tasks. The findings emphasize the potential of RL in developing reasoning abilities in large language models and the effectiveness of distillation techniques for smaller models.", summary_embedding=None, total_tokens=29673)] total_entries=1
    ), ...
  ]
  ```

  ### Executing a search

  Perform a search query:

  <tabs>
    <tab title="Python">
      ```python

      client.retrieval.search(
        query="What is DeepSeek R1?",
      )
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      client.retrieval.search({
        query: "What is DeepSeek R1?",
      })
      ```
    </tab>

    <tab title="Curl">
      ```zsh
      curl -X POST https://api.sciphi.ai/v3/retrieval/search \
        -H "Content-Type: application/json" \
        -d '{
          "query": "What is DeepSeek R1?"
        }'
      ```
    </tab>
  </tabs>

  The search query will use basic similarity search to find the most relevant documents. You can use advanced search methods like [hybrid search](/documentation/hybrid-search) or [graph search](/documentation/graphs) depending on your use case.

  Example output:

  ```plaintext
  AggregateSearchResult(
    chunk_search_results=[
      ChunkSearchResult(
        score=0.643, 
        text="Document Title: DeepSeek_R1.pdf
        Text: could achieve an accuracy of over 70%.
        DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a
        models ability to follow format instructions. These improvements can be linked to the inclusion
        of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL
        training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,
        indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its
        significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale
        RL, which not only boosts reasoning capabilities but also improves performance across diverse
        domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an
        average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that
        DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying
        its robustness across multiple tasks."
      ), ...
    ],
    graph_search_results=[],
    web_search_results=[],
    context_document_results=[]
  )
  ```

  ### RAG

  Generate a RAG response:

  <tabs>
    <tab title="Python">
      ```python
      client.retrieval.rag(
        query="What is DeepSeek R1?",
      )
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      client.retrieval.rag({
        query: "What is DeepSeek R1?",
      })
      ```
    </tab>

    <tab title="Curl">
      ```zsh
      curl -X POST https://api.sciphi.ai/v3/retrieval/rag \
        -H "Content-Type: application/json" \
        -d '{
          "query": "What is DeepSeek R1?"
        }'
      ```
    </tab>
  </tabs>

  Example output:

  ```plaintext
  RAGResponse(
    generated_answer='DeepSeek-R1 is a model that demonstrates impressive performance across various tasks, leveraging reinforcement learning (RL) and supervised fine-tuning (SFT) to enhance its capabilities. It excels in writing tasks, open-domain question answering, and benchmarks like IF-Eval, AlpacaEval2.0, and ArenaHard [1], [2]. DeepSeek-R1 outperforms its predecessor, DeepSeek-V3, in several areas, showcasing its strengths in reasoning and generalization across diverse domains [1]. It also achieves competitive results on factual benchmarks like SimpleQA, although it performs worse on the Chinese SimpleQA benchmark due to safety RL constraints [2]. Additionally, DeepSeek-R1 is involved in distillation processes to transfer its reasoning capabilities to smaller models, which perform exceptionally well on benchmarks [4], [6]. The model is optimized for English and Chinese, with plans to address language mixing issues in future updates [8].', 
    search_results=AggregateSearchResult(
      chunk_search_results=[ChunkSearchResult(score=0.643, text=Document Title: DeepSeek_R1.pdf ...)]
    ),
    citations=[Citation(id='cit_3a35e39', object='citation', payload=ChunkSearchResult(score=0.676, text=Document Title: DeepSeek_R1.pdf\n\nText: However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.)), Citation(id='cit_ec89403', object='citation', payload=ChunkSearchResult(score=0.664, text=Document Title: DeepSeek_R1.pdf\n\nText: - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.)), ...],
    metadata={'id': 'chatcmpl-B0BaZ0vwIa58deI0k8NIuH6pBhngw', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}}], 'created': 1739384247, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion', 'service_tier': 'default', 'system_fingerprint': 'fp_4691090a87', ...}
  )
  ```

  ### Streaming RAG

  Generate a streaming RAG response:

  <tabs>
    <tab title="Python">
      ```python
      from r2r import (
          CitationEvent,
          FinalAnswerEvent,
          MessageEvent,
          SearchResultsEvent,
          R2RClient,
      )


      result_stream = client.retrieval.rag(
          query="What is DeepSeek R1?",
          search_settings={"limit": 25},
          rag_generation_config={"stream": True},
      )

      # can also do a switch on `type` field
      for event in result_stream:
          if isinstance(event, SearchResultsEvent):
              print("Search results:", event.data)
          elif isinstance(event, MessageEvent):
              print("Partial message:", event.data.delta)
          elif isinstance(event, CitationEvent):
              print("New citation detected:", event.data)
          elif isinstance(event, FinalAnswerEvent):
              print("Final answer:", event.data.generated_answer)
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      // 1) Initiate a streaming RAG request
      const resultStream = await client.retrieval.rag({
      query: "What is DeepSeek R1?",
      searchSettings: { limit: 25 },
      ragGenerationConfig: { stream: true },
      });

      // 2) Check if we got an async iterator (streaming)
      if (Symbol.asyncIterator in resultStream) {
      // 2a) Loop over each event from the server
      for await (const event of resultStream) {
          switch (event.event) {
          case "search_results":
              console.log("Search results:", event.data);
              break;
          case "message":
              console.log("Partial message delta:", event.data.delta);
              break;
          case "citation":
              console.log("New citation event:", event.data);
              break;
          case "final_answer":
              console.log("Final answer:", event.data.generated_answer);
              break;
          // ... add more cases if you have other event types, e.g. tool_call / tool_result
          default:
              console.log("Unknown or unhandled event:", event);
          }
      }
      } else {
      // 2b) If streaming was NOT enabled or server didn't send SSE,
      //     we'd get a single response object instead.
      console.log("Non-streaming RAG response:", resultStream);
      }
      ```
    </tab>

    <tab title="Curl">
      ```bash
      ...
      ```
    </tab>
  </tabs>

  Example output:

  ```plaintext
  Search results: id='run_1' object='rag.search_results' data={'chunk_search_results': [{'id': '1e40ee7e-2eef-524f-b5c6-1a1910e73ccc', 'document_id': '652075c0-3a43-519f-9625-f581e7605bc5', 'owner_id': '2acb499e-8428-543b-bd85-0d9098718220', 'collection_ids': ['122fdf6a-e116-546b-a8f6-e4cb2e2c0a09'], 'score': 0.7945216641038179, 'text': 'data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately ... 
  ...
  Partial message: {'content': [MessageDelta(type='text', text={'value': 'Deep', 'annotations': []})]}
  Partial message: {'content': [MessageDelta(type='text', text={'value': 'Seek', 'annotations': []})]}
  Partial message: {'content': [MessageDelta(type='text', text={'value': '-R', 'annotations': []})]}
  ...
  New Citation Detected: 'cit_3a35e39'
  ...
  Final answer: DeepSeek-R1 is a large language model developed by the DeepSeek-AI research team. It is a reasoning model that has been trained using multi-stage training and cold-start data before reinforcement learning (RL). The model demonstrates superior performance on various benchmarks, including MMLU, MMLU-Pro, GPQA Diamond, and FRAMES, particularly in STEM-related questions. ...
  ```

  ### Streaming Agentic RAG

  R2R offers a powerful `agentic` retrieval mode that performs in-depth analysis of documents through iterative research and reasoning. This mode can leverage a variety of tools to thoroughly investigate your data and the web:

  <tabs>
    <tab title="Python">
      ```python
      from r2r import (
          ThinkingEvent,
          ToolCallEvent,
          ToolResultEvent,
          CitationEvent,
          FinalAnswerEvent,
          MessageEvent,
          R2RClient,
      )

      results = client.retrieval.agent(
          message={"role": "user", "content": "What does deepseek r1 imply for the future of AI?"},
          rag_generation_config={
              "model": "anthropic/claude-3-7-sonnet-20250219",
              "extended_thinking": True,
              "thinking_budget": 4096,
              "temperature": 1,
              "top_p": None,
              "max_tokens_to_sample": 16000,
              "stream": True
          },
      )

      # Process the streaming events
      for event in results:
          if isinstance(event, ThinkingEvent):
              print(f"üß† Thinking: {event.data.delta.content[0].payload.value}")
          elif isinstance(event, ToolCallEvent):
              print(f"üîß Tool call: {event.data.name}({event.data.arguments})")
          elif isinstance(event, ToolResultEvent):
              print(f"üìä Tool result: {event.data.content[:60]}...")
          elif isinstance(event, CitationEvent):
              print(f"üìë Citation: {event.data}")
          elif isinstance(event, MessageEvent):
              print(f"üí¨ Message: {event.data.delta.content[0].payload.value}")
          elif isinstance(event, FinalAnswerEvent):
              print(f"‚úÖ Final answer: {event.data.generated_answer[:100]}...")
              print(f"   Citations: {len(event.data.citations)} sources referenced")



      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      const resultStream = await client.retrieval.agent({
        message: {role: "user", content: "What does deepseek r1 imply for the future of AI?"},
        generationConfig: { stream: true }
      });

      // Process the streaming events
      if (Symbol.asyncIterator in resultStream) {
        for await (const event of resultStream) {
          switch(event.event) {
            case "thinking":
              console.log(`üß† Thinking: ${event.data.delta.content[0].payload.value}`);
              break;
            case "tool_call":
              console.log(`üîß Tool call: ${event.data.name}(${JSON.stringify(event.data.arguments)})`);
              break;
            case "tool_result":
              console.log(`üìä Tool result: ${event.data.content.substring(0, 60)}...`);
              break;
            case "citation":
              console.log(`üìë Citation event: ${event.data}`);
              break;
            case "message":
              console.log(`üí¨ Message: ${event.data.delta.content[0].payload.value}`);
              break;
            case "final_answer":
              console.log(`‚úÖ Final answer: ${event.data.generated_answer.substring(0, 100)}...`);
              console.log(`   Citations: ${event.data.citations.length} sources referenced`);
              break;
          }
        }
      }
      ```
    </tab>
  </tabs>

  Example of streaming output:

  ```plaintext
  üß† Thinking: Analyzing the query about DeepSeek R1 implications...
  üîß Tool call: search_file_knowledge({"query":"DeepSeek R1 capabilities advancements"})
  üìä Tool result: DeepSeek-R1 is a reasoning-focused LLM that uses reinforcement learning...
  üß† Thinking: The search provides valuable information about DeepSeek R1's capabilities
  üß† Thinking: Need more specific information about its performance in reasoning tasks
  üîß Tool call: search_file_knowledge({"query":"DeepSeek R1 reasoning benchmarks performance"})
  üìä Tool result: DeepSeek-R1 achieves strong results on reasoning benchmarks including MMLU...
  üìë Citation: cit_54c45c8
  üß† Thinking: Now I need to understand the implications for AI development
  üîß Tool call: web_search({"query":"AI reasoning capabilities future development"})
  üìä Tool result: Advanced reasoning capabilities are considered a key milestone toward...
  üìë Citation: cit_d1152e7
  üí¨ Message: DeepSeek-R1 has several important implications for the future of AI development:
  üí¨ Message: 1. **Reinforcement Learning as a Key Approach**: DeepSeek-R1's success demonstrates...
  üìë Citation: cit_eb5ba04
  üí¨ Message: 2. **Efficiency Through Distillation**: The model shows that reasoning capabilities...
  ‚úÖ Final answer: DeepSeek-R1 has several important implications for the future of AI development: 1. Reinforcement Learning...
    Citations: 3 sources referenced    
  ```
</graphextractionstatus.pending:></ingestionstatus.success:></documenttype.pdf:></steps>

## Additional Features

R2R offers the additional features below to enhance your document management and user experience.

### Graphs

R2R provides powerful entity and relationshipo extraction capabilities that enhance document understanding and retrieval. These can leveraged to construct knowledge graphs inside R2R. The system can automatically identify entities, build relationships between them, and create enriched knowledge graphs from your document collection.

<cardgroup cols="{2}">
  <card title="Knowledge Graphs" icon="diagram-project" href="/documentation/graphs">
    Automatically extract entities and relationships from documents to form knowledge graphs.
  </card>
</cardgroup>

### Users and Collections

R2R provides a complete set of user authentication and management features, allowing you to implement secure and feature-rich authentication systems or integrate with your preferred authentication provider. Further, collections exist to enable efficient access control and organization of users and documents.

<cardgroup cols="{2}">
  <card title="User Auth Cookbook" icon="key" href="/documentation/user-auth">
    Learn how to implement user registration, login, email verification, and more using R2R's built-in authentication capabilities.
  </card>

  <card title="Collections Cookbook" icon="database" href="/documentation/collections">
    Discover how to create, manage, and utilize collections in R2R for granular access control and document organization.
  </card>
</cardgroup>

## Next Steps

Now that you have a basic understanding of R2R's core features, you can explore more advanced topics:

* Dive into [document ingestion](/documentation/documents) and [the document reference](/api-and-sdks/documents/documents).
* Learn about [search and RAG](/documentation/hybrid-search) and the [retrieval reference](/api-and-sdks/retrieval/retrieval).
* Try advanced techniques like [knowledge-graphs](/documentation/graphs) and refer to the [graph reference](/api-and-sdks/graphs/graphs).
* Learn about [user authentication](/documentation/user-auth) to secure your application permissions and [the users API reference](/api-and-sdks/users/users).
* Organize your documents using [collections](/api-and-sdks/collections/collections) for granular access control.


# Walkthrough

&gt; A detailed step-by-step cookbook of the core features provided by R2R.

This guide shows how to use R2R to:

1. Ingest files into R2R
2. Search over ingested files
3. Use your data as input to RAG (Retrieval-Augmented Generation) / Deep Research
4. Extract entities and relationships from your data to create a graph.
5. Perform basic user auth
6. Observe and analyze an R2R deployment

## Introduction

R2R is an engine for building user-facing Retrieval-Augmented Generation (RAG) applications. At its core, R2R provides this service through an architecture of providers, services, and a centralized RESTful API. This cookbook provides a detailed walkthrough of how to interact with R2R.

[Refer here](/introduction/system) for a deeper dive on the R2R system architecture.

## Hello R2R

R2R gives developers configurable vector search and RAG right out of the box, as well as direct method calls instead of the client-server architecture seen throughout the docs:

```python core/examples/hello_r2r.py
from r2r import R2RClient

client = R2RClient() # optional, pass in "http://localhost:7272" or "https://api.sciphi.ai"

with open("test.txt", "w") as file:
    file.write("John is a person that works at Google.")

client.documents.create(file_path="test.txt")

# Call RAG directly
rag_response = client.retrieval.rag(
    query="Who is john",
    rag_generation_config={"model": "openai/gpt-4o-mini", "temperature": 0.0},
)

print(f"Search Results:\n{rag_response.results.search_results}")
# AggregateSearchResult(chunk_search_results=[ChunkSearchResult(score=0.685, text=John is a person that works at Google.)], graph_search_results=[], web_search_results=[], context_document_results=[])

print(f"Completion:\n{rag_response.results.generated_answer}")
# John is a person that works at Google [e123456].

print(f"Citations:\n{rag_response.results.citations}")
# [Citation(id='e123456', object='citation', payload=ChunkSearchResult(...))]
```

## Document Ingestion and Management

R2R efficiently handles diverse document types using Postgres with pgvector, combining relational data management with vector search capabilities. This approach enables seamless ingestion, storage, and retrieval of multimodal data, while supporting flexible document management and user permissions.

Key features include:

* Unique [`Document`](/api-and-sdks/documents/documents), with corresponding `id`, created for each ingested file or context, which contains the downstream [`Chunks`](/api-and-sdks/chunks/chunks) and [`Entities` &amp; `Relationships`](/api-and-sdks/graphs/graphs).
* [`User`](/api-and-sdks/users/users) and [`Collection`](/api-and-sdks/collections/collections) objects for comprehensive document permissions.
* [`Graph`](/api-and-sdks/graphs/graphs), construction and maintenance.
* Flexible document deletion and update mechanisms at global document and chunk levels.

<note>
   Note, all document related commands are gated to documents the user has uploaded or has access to through shared collections, with the exception of superusers. 
</note>

<accordiongroup>
  <accordion icon="database" title="Create Documents" defaultopen="{true}">
    R2R offers a powerful data ingestion process that handles various file types including `html`, `pdf`, `png`, `mp3`, and `txt`.

    The ingestion process parses, chunks, embeds, and stores documents efficiently. A durable orchestration workflow coordinates the entire process.

    <tabs>
      <tab title="Python">
        ```python
        # export R2R_API_KEY=...
        from r2r import R2RClient

        client = R2RClient() # or set base_url=...
        # when using auth, do client.users.login(...)

        client.documents.create_sample(hi_res=True)
        # to ingest your own document, client.documents.create(file_path="/path/to/file")
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        // export R2R_API_KEY=...
        const { r2rClient } = require('r2r-js');

        const client = new r2rClient(); // or set baseURL=...

        clients.documents.createSample({ ingestionMode: "hi-res" })
        // to ingest your own document, client.documents.create({filePath: })
        ```
      </tab>
    </tabs>

    This command initiates the ingestion process, producing output similar to:

    ```plaintext
    IngestionResponse(message='Document created and ingested successfully.', task_id=None, document_id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'))
    ```

    Key features of the ingestion process:

    1. Unique `document_id` generation for each file
    2. Metadata association, including `user_id` and `collection_ids` for document management
    3. Efficient parsing, chunking, and embedding of diverse file types
  </accordion>

  <accordion icon="folder-open" title="Retrieving Documents">
    R2R allows retrieval of high-level document information stored in a relational table within the Postgres database. To fetch this information:

    <tabs>
      <tab title="Python">
        ```python
        result = client.documents.list(
            limit=10,
            offset=0
        )
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        const response = await client.documents.list({
            limit: 10,
            offset: 0,
        });
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X GET https://api.sciphi.ai/v3/documents \
          -H "Content-Type: application/json" \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>

    This command returns document metadata, including:

    ```plaintext
    [
      DocumentResponse(
        id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'), 
        collection_ids=[UUID('122fdf6a-e116-546b-a8f6-e4cb2e2c0a09')], 
        owner_id=UUID('2acb499e-8428-543b-bd85-0d9098718220'), 
        document_type=<documenttype.pdf: 'pdf'="">, 
        metadata={'title': 'DeepSeek_R1.pdf', 'version': 'v0'}, 
        version='v0', 
        size_in_bytes=1768572, 
        ingestion_status=<ingestionstatus.success: 'success'="">, 
        extraction_status=<graphextractionstatus.pending: 'pending'="">, 
        created_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 126759, tzinfo=TzInfo(UTC)), 
        updated_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 160114, tzinfo=TzInfo(UTC)), 
        ingestion_attempt_number=None, 
        summary="The document contains a comprehensive overview of DeepSeek-R1, a series of reasoning models developed by DeepSeek-AI, which includes DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero utilizes large-scale reinforcement learning (RL) without supervised fine-tuning, showcasing impressive reasoning capabilities but facing challenges like readability and language mixing. To enhance performance, DeepSeek-R1 incorporates multi-stage training and cold-start data, achieving results comparable to OpenAI's models on various reasoning tasks. The document details the models' training processes, evaluation results across multiple benchmarks, and the introduction of distilled models that maintain reasoning capabilities while being smaller and more efficient. It also discusses the limitations of current models, such as language mixing and sensitivity to prompts, and outlines future research directions to improve general capabilities and efficiency in software engineering tasks. The findings emphasize the potential of RL in developing reasoning abilities in large language models and the effectiveness of distillation techniques for smaller models.", summary_embedding=None, total_tokens=29673)] total_entries=1
      ), ...
    ]
    ```

    This overview provides quick access to document versions, sizes, and associated metadata, facilitating efficient document management.
  </graphextractionstatus.pending:></ingestionstatus.success:></documenttype.pdf:></accordion>

  <accordion icon="file" title="Retrieving Document Chunks">
    R2R enables retrieval of specific document chunks and associated metadata. To fetch chunks for a particular document by id:

    <tabs>
      <tab title="Python">
        ```python
        client.documents.list_chunks(id="9fbe403b-c11c-5aae-8ade-ef22980c3ad1")
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.documents.listChunks({
          id: "9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
        }),
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X GET https://api.sciphi.ai/v3/documents/9fbe403b-c11c-5aae-8ade-ef22980c3ad1/chunks \
          -H "Content-Type: application/json" \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>

    This command returns detailed chunk information:

    ```plaintext
    results=[ChunkResponse(id=UUID('27a2e605-2916-59fe-a4da-b19853713298'), document_id=UUID('30f950f0-c692-57c5-b6ec-ff78ccf5ccdc'), owner_id=UUID('2acb499e-8428-543b-bd85-0d9098718220'), collection_ids=[UUID('122fdf6a-e116-546b-a8f6-e4cb2e2c0a09')], text='John is a person that works at Google.', metadata={'version': 'v0', 'chunk_order': 0, 'document_type': 'txt'}, vector=None)] total_entries=1
    ```

    These features allow for granular access to document content.
  </accordion>

  <accordion icon="trash" title="Deleting Documents">
    R2R supports flexible document deletion through a method that can run arbitrary deletion filters. To delete a document by its ID:

    <tabs>
      <tab title="Python">
        ```python
        client.documents.delete(id="9fbe403b-c11c-5aae-8ade-ef22980c3ad1")
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.documents.delete({
          id: "9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
        });
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X DELETE https://api.sciphi.ai/v3/documents/9fbe403b-c11c-5aae-8ade-ef22980c3ad1 \
          -H "Content-Type: application/json" \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>

    This command produces output similar to:

    ```plaintext
    GenericBooleanResponse(success=True)
    ```

    Key features of the deletion process:

    1. Deletion by document ID,
    2. Cascading deletion of associated chunks and metadata
    3. Deletion by filter, e.g. by text match, user id match, or other with `documents/by-filter`.

    This flexible deletion mechanism ensures precise control over document management within the R2R system.
  </accordion>
</accordiongroup>

For more advanced document management techniques and user authentication details, refer to [the user documentation](/documentation/user-auth).

## AI Powered Search

R2R offers powerful and highly configurable search capabilities, including vector search, hybrid search, and knowledge graph-enhanced search. These features allow for more accurate and contextually relevant information retrieval.

### Vector Search

Vector search parameters inside of R2R can be fine-tuned at runtime for optimal results. Here's how to perform a basic vector search:

<tabs>
  <tab title="Python">
    ```python
    client.retrieval.search(
      query="What is DeepSeek R1?",
    )
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    client.retrieval.search({
      query: "What is DeepSeek R1?",
    })
    ```
  </tab>

  <tab title="Curl">
    ```zsh
    curl -X POST https://api.sciphi.ai/v3/retrieval/search \
      -H "Content-Type: application/json" \
      -d '{
        "query": "What is DeepSeek R1?"
      }' \
      -H "Authorization: Bearer your_token_here"
    ```
  </tab>
</tabs>

<accordiongroup>
  <accordion title="Expected Output">
    ```plaintext
    AggregateSearchResult(
      chunk_search_results=[
        ChunkSearchResult(
          score=0.643, 
          text="Document Title: DeepSeek_R1.pdf
          Text: could achieve an accuracy of over 70%.
          DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a
          models ability to follow format instructions. These improvements can be linked to the inclusion
          of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL
          training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,
          indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its
          significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale
          RL, which not only boosts reasoning capabilities but also improves performance across diverse
          domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an
          average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that
          DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying
          its robustness across multiple tasks."
        ), ...
      ],
      graph_search_results=[],
      web_search_results=[],
      context_document_results=[]
    )
    ```
  </accordion>
</accordiongroup>

Key configurable parameters for vector search can be inferred from the [retrieval API reference](/api-and-sdks/retrieval/retrieval).

### Hybrid Search

R2R supports hybrid search, which combines traditional keyword-based search with vector search for improved results. Here's how to perform a hybrid search:

<tabs>
  <tab title="Python">
    ```python
    client.retrieval.search(
        "What was Uber's profit in 2020?",
        search_settings={
            "index_measure": "l2_distance",
            "use_hybrid_search": True,
            "hybrid_settings": {
                "full_text_weight": 1.0,
                "semantic_weight": 5.0,
                "full_text_limit": 200,
                "rrf_k": 50,
            }
        },
    )
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    await client.retrieval.search({
      query: "What was Uber's profit in 2020? ",
      searchSettings: {
        indexMeasure: "l2_distance",
        useHybridSearch: true,
        hybridSettings: {
            fullTextWeight: 1.0,
            semanticWeight: 5.0,
            fullTextLimit: 200,
        },
        filters: {"title": {"$in": ["DeepSeek_R1.pdf"]}},
      },
    });
    ```
  </tab>

  <tab title="Curl">
    ```zsh
    curl -X POST https://api.sciphi.ai/v3/retrieval/search \
      -H "Content-Type: application/json" \
      -d '{
        "query": "What was Uber'\''s profit in 2020?",
        "search_settings": {
          "use_hybrid_search": true,
          "hybrid_settings": {
            "full_text_weight": 1.0,
            "semantic_weight": 5.0,
            "full_text_limit": 200,
            "rrf_k": 50
          },
          "filters": {"title": {"$in": ["DeepSeek_R1.pdf", "uber_2021.pdf"]}},
          "limit": 10,
          "chunk_settings": {
            "index_measure": "l2_distance",
            "probes": 25,
            "ef_search": 100
          }      
        }
      }' \
      -H "Authorization: Bearer your_token_here"
    ```
  </tab>
</tabs>

## AI Retrieval (RAG)

R2R is built around a comprehensive Retrieval-Augmented Generation (RAG) engine, allowing you to generate contextually relevant responses based on your ingested documents. The RAG process combines all the search functionality shown above with Large Language Models to produce more accurate and informative answers.

<accordiongroup>
  <accordion icon="brain" title="Basic RAG" defaultopen="{true}">
    To generate a response using RAG, use the following command:

    <tabs>
      <tab title="Python">
        ```python
        client.retrieval.rag(query="What is DeepSeek R1?")
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.rag({ query: "What is DeepSeek R1?" });
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/retrieval/rag \
          -H "Content-Type: application/json" \
          -d '{
            "query": "What is DeepSeek R1?"
          }' \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>

    **Example Output:**

    ```zsh
    RAGResponse(
        generated_answer='DeepSeek-R1 is a model that demonstrates impressive performance across various tasks, leveraging reinforcement learning (RL) and supervised fine-tuning (SFT) to enhance its capabilities. It excels in writing tasks, open-domain question answering, and benchmarks like IF-Eval, AlpacaEval2.0, and ArenaHard [1], [2]. DeepSeek-R1 outperforms its predecessor, DeepSeek-V3, in several areas, showcasing its strengths in reasoning and generalization across diverse domains [1]. It also achieves competitive results on factual benchmarks like SimpleQA, although it performs worse on the Chinese SimpleQA benchmark due to safety RL constraints [2]. Additionally, DeepSeek-R1 is involved in distillation processes to transfer its reasoning capabilities to smaller models, which perform exceptionally well on benchmarks [4], [6]. The model is optimized for English and Chinese, with plans to address language mixing issues in future updates [8].', 
        search_results=AggregateSearchResult(
          chunk_search_results=[ChunkSearchResult(score=0.643, text='Document Title: DeepSeek_R1.pdf...')]
        ),
        citations=[
          Citation(
            id='123456', 
            object='citation', 
            payload=ChunkSearchResult(score=0.643, text='Document Title: DeepSeek_R1.pdf...', id='e760bb76-1c6e-52eb-910d-0ce5b567011b', document_id='e43864f5-a36f-548e-aacd-6f8d48b30c7f', owner_id='2acb499e-8428-543b-bd85-0d9098718220', collection_ids=['122fdf6a-e116-546b-a8f6-e4cb2e2c0a09'])
          )
        ],
        metadata={'id': 'chatcmpl-B0BaZ0vwIa58deI0k8NIuH6pBhngw', 'choices': [...], 'created': 1739384247, 'model': 'gpt-4o-2024-08-06', ...}
    )
    ```

    This command performs a search on the ingested documents and uses the retrieved information to generate a response.
  </accordion>

  <accordion icon="layer-group" title="RAG w/ Hybrid Search">
    R2R also supports hybrid search in RAG, combining the power of vector search and keyword-based search. To use hybrid search in RAG, simply add the `use_hybrid_search` flag to your search settings input:

    <tabs>
      <tab title="Python">
        ```javascript
        results = client.retrieval.rag("Who is Jon Snow?", {"use_hybrid_search": True})
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.rag({
          query: "Who is Jon Snow?",
          searchSettings: {
            useHybridSearch: true
          },
        });
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/retrieval/rag \
          -H "Content-Type: application/json" \
          -d '{
            "query": "Who is Jon Snow?",
            "chunk_settings": {
              "use_semantic_search": true,
              "filters": {},
              "limit": 10,
              "use_hybrid_search": true
            }
          }' \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>

    This example demonstrates how hybrid search can enhance the RAG process by combining semantic understanding with keyword matching, potentially providing more accurate and comprehensive results.
  </accordion>

  <accordion icon="screencast" title="Streaming RAG">
    R2R also supports streaming RAG responses, which can be useful for real-time applications.

    When using streaming RAG, you'll receive different types of events:

    1. `SearchResultsEvent` - Contains the initial search results from your documents
    2. `MessageEvent` - Streams partial tokens of the response as they are generated
    3. `CitationEvent` - Indicates when a citation is added to the response, with relevant metadata including:
       * `id` - Unique identifier for the citation
       * `object` - Always "citation"
       * `source_type` - The type of source (chunk, graph, web, etc.)
       * `source_title` - Title of the source document when available
    4. `FinalAnswerEvent` - Contains the complete generated answer and structured citations
    5. `ThinkingEvent` - For reasoning agents, contains the model's step-by-step reasoning process

    The citations in the final response are structured objects that link specific passages in the response to their source documents, enabling proper attribution and verification. To use streaming RAG:

    Generate a streaming RAG response:

    <tabs>
      <tab title="Python">
        ```python
        from r2r import (
            CitationEvent,
            FinalAnswerEvent,
            MessageEvent,
            SearchResultsEvent,
            R2RClient,
        )


        result_stream = client.retrieval.rag(
            query="What is DeepSeek R1?",
            search_settings={"limit": 25},
            rag_generation_config={"stream": True},
        )

        # can also do a switch on `type` field
        for event in result_stream:
            if isinstance(event, SearchResultsEvent):
                print("Search results:", event.data)
            elif isinstance(event, MessageEvent):
                print("Partial message:", event.data.delta)
            elif isinstance(event, CitationEvent):
                print("New citation detected:", event.data)
            elif isinstance(event, FinalAnswerEvent):
                print("Final answer:", event.data.generated_answer)
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        // 1) Initiate a streaming RAG request
        const resultStream = await client.retrieval.rag({
        query: "What is DeepSeek R1?",
        searchSettings: { limit: 25 },
        ragGenerationConfig: { stream: true },
        });

        // 2) Check if we got an async iterator (streaming)
        if (Symbol.asyncIterator in resultStream) {
        // 2a) Loop over each event from the server
        for await (const event of resultStream) {
            switch (event.event) {
            case "search_results":
                console.log("Search results:", event.data);
                break;
            case "message":
                console.log("Partial message delta:", event.data.delta);
                break;
            case "citation":
                console.log("New citation event:", event.data);
                break;
            case "final_answer":
                console.log("Final answer:", event.data.generated_answer);
                break;
            // ... add more cases if you have other event types, e.g. tool_call / tool_result
            default:
                console.log("Unknown or unhandled event:", event);
            }
        }
        } else {
        // 2b) If streaming was NOT enabled or server didn't send SSE,
        //     we'd get a single response object instead.
        console.log("Non-streaming RAG response:", resultStream);
        }
        ```
      </tab>

      <tab title="Curl">
        ```bash
        ...
        ```
      </tab>
    </tabs>

    Example output:

    ```plaintext
    Search results: id='run_1' object='rag.search_results' data={'chunk_search_results': [{'id': '1e40ee7e-2eef-524f-b5c6-1a1910e73ccc', 'document_id': '652075c0-3a43-519f-9625-f581e7605bc5', 'owner_id': '2acb499e-8428-543b-bd85-0d9098718220', 'collection_ids': ['122fdf6a-e116-546b-a8f6-e4cb2e2c0a09'], 'score': 0.7945216641038179, 'text': 'data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately ... 
    ...
    Partial message: {'content': [MessageDelta(type='text', text={'value': 'Deep', 'annotations': []})]}
    Partial message: {'content': [MessageDelta(type='text', text={'value': 'Seek', 'annotations': []})]}
    Partial message: {'content': [MessageDelta(type='text', text={'value': '-R', 'annotations': []})]}
    ...
    New Citation Detected: 'cit_3a35e39'
    ...
    Final answer: DeepSeek-R1 is a large language model developed by the DeepSeek-AI research team. It is a reasoning model that has been trained using multi-stage training and cold-start data before reinforcement learning (RL). The model demonstrates superior performance on various benchmarks, including MMLU, MMLU-Pro, GPQA Diamond, and FRAMES, particularly in STEM-related questions. ...
    ```

    Streaming allows the response to be generated and sent in real-time, chunk by chunk.
  </accordion>

  <accordion icon="gears" title="Customizing RAG">
    R2R offers extensive customization options for its Retrieval-Augmented Generation (RAG) functionality:

    1. **Search Settings**: Customize vector and knowledge graph search parameters using `VectorSearchSettings` and `KGSearchSettings`.

    2. **Generation Config**: Fine-tune the language model's behavior with `GenerationConfig`, including:
       * Temperature, top\_p, top\_k for controlling randomness
       * Max tokens, model selection, and streaming options
       * Advanced settings like beam search and sampling strategies

    3. **Multiple LLM Support**: Easily switch between different language models and providers:
       * OpenAI models (default)
       * Anthropic's Claude models
       * Local models via Ollama
       * Any provider supported by LiteLLM

    Example of customizing the model:

    <tabs>
      <tab title="Python">
        ```python
        # requires ANTHROPIC_API_KEY is set
        response = client.retrieval.rag(
          "Who was Aristotle?",
          rag_generation_config={"model":"anthropic/claude-3-haiku-20240307", "stream": True}
        )
        for chunk in response:
            print(chunk, flush=False)
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.rag({
          query: query,
          ragGenerationConfig: {
            model: 'claude-3-haiku-20240307',
            temperature: 0.7,
          }
        });
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        # requires ANTHROPIC_API_KEY is set
        curl -X POST https://api.sciphi.ai/v3/retrieval/rag \
            -H "Content-Type: application/json" \
            -d '{
                "query": "Who is Jon Snow?",
                "rag_generation_config": {
                    "model": "claude-3-haiku-20240307",
                    "temperature": 0.7
                }
            }' \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>

    This flexibility allows you to optimize RAG performance for your specific use case and leverage the strengths of various LLM providers.
  </accordion>

  <accordion icon="microscope" title="Streaming Agent (Deep Research Mode)">
    R2R offers a powerful `agentic` retrieval mode that performs in-depth analysis of documents through iterative research and reasoning. This mode can replicate Deep Research-like results by leveraging a variety of tools to thoroughly investigate your data and the web:

    <tabs>
      <tab title="Python">
        ```python
        from r2r import (
            ThinkingEvent,
            ToolCallEvent,
            ToolResultEvent,
            CitationEvent,
            FinalAnswerEvent,
            MessageEvent,
            R2RClient,
        )
        client = R2RClient("http://localhost:7272")

        results = client.retrieval.agent(
            message={"role": "user", "content": "What does deepseek r1 imply for the future of AI?"},
            rag_generation_config={
                "model": "anthropic/claude-3-7-sonnet-20250219",
                "extended_thinking": True,
                "thinking_budget": 4096,
                "temperature": 1,
                "top_p": None,
                "max_tokens_to_sample": 16000,
                "stream": True
            },
            mode="research" # for `deep research`, otherwise omit
        )

        # Process the streaming events
        for event in results:
            if isinstance(event, ThinkingEvent):
                print(f"üß† Thinking: {event.data.delta.content[0].payload.value}")
            elif isinstance(event, ToolCallEvent):
                print(f"üîß Tool call: {event.data.name}({event.data.arguments})")
            elif isinstance(event, ToolResultEvent):
                print(f"üìä Tool result: {event.data.content[:60]}...")
            elif isinstance(event, CitationEvent):
                print(f"üìë Citation: {event.data}")
            elif isinstance(event, MessageEvent):
                print(f"üí¨ Message: {event.data.delta.content[0].payload.value}")
            elif isinstance(event, FinalAnswerEvent):
                print(f"‚úÖ Final answer: {event.data.generated_answer[:100]}...")
                print(f"   Citations: {len(event.data.citations)} sources referenced")



        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        const resultStream = await client.retrieval.agent({
          query: "What does deepseek r1 imply for the future of AI?",
          message: {role: "user", content: "What does deepseek r1 imply for the future of AI?"},
          generationConfig: { stream: true }
          mode: "research" // for `deep research`, otherwise omit
        });

        // Process the streaming events
        if (Symbol.asyncIterator in resultStream) {
          for await (const event of resultStream) {
            switch(event.event) {
              case "thinking":
                console.log(`üß† Thinking: ${event.data.delta.content[0].payload.value}`);
                break;
              case "tool_call":
                console.log(`üîß Tool call: ${event.data.name}(${JSON.stringify(event.data.arguments)})`);
                break;
              case "tool_result":
                console.log(`üìä Tool result: ${event.data.content.substring(0, 60)}...`);
                break;
              case "citation":
                console.log(`üìë Citation event: ${event.data}`);
                break;
              case "message":
                console.log(`üí¨ Message: ${event.data.delta.content[0].payload.value}`);
                break;
              case "final_answer":
                console.log(`‚úÖ Final answer: ${event.data.generated_answer.substring(0, 100)}...`);
                console.log(`   Citations: ${event.data.citations.length} sources referenced`);
                break;
            }
          }
        }
        ```
      </tab>
    </tabs>

    Example of streaming output:

    ```plaintext
    üß† Thinking: Analyzing the query about DeepSeek R1 implications...
    üîß Tool call: search_file_knowledge({"query":"DeepSeek R1 capabilities advancements"})
    üìä Tool result: DeepSeek-R1 is a reasoning-focused LLM that uses reinforcement learning...
    üß† Thinking: The search provides valuable information about DeepSeek R1's capabilities
    üß† Thinking: Need more specific information about its performance in reasoning tasks
    üîß Tool call: search_file_knowledge({"query":"DeepSeek R1 reasoning benchmarks performance"})
    üìä Tool result: DeepSeek-R1 achieves strong results on reasoning benchmarks including MMLU...
    üìë Citation: cit_54c45c8
    üß† Thinking: Now I need to understand the implications for AI development
    üîß Tool call: web_search({"query":"AI reasoning capabilities future development"})
    üìä Tool result: Advanced reasoning capabilities are considered a key milestone toward...
    üìë Citation: cit_d1152e7
    üí¨ Message: DeepSeek-R1 has several important implications for the future of AI development:
    üí¨ Message: 1. **Reinforcement Learning as a Key Approach**: DeepSeek-R1's success demonstrates...
    üìë Citation: cit_eb5ba04
    üí¨ Message: 2. **Efficiency Through Distillation**: The model shows that reasoning capabilities...
    ‚úÖ Final answer: DeepSeek-R1 has several important implications for the future of AI development: 1. Reinforcement Learning...
      Citations: 3 sources referenced    
    ```
  </accordion>
</accordiongroup>

Behind the scenes, R2R's RetrievalService handles RAG requests, combining the power of vector search, optional knowledge graph integration, and language model generation.

## Graphs in R2R

R2R implements a Git-like model for knowledge graphs, where each collection has a corresponding graph that can diverge and be independently managed. This approach allows for flexible knowledge management while maintaining data consistency.

### Graph-Collection Relationship

* Each collection has an associated graph that acts similar to a Git branch
* Graphs can diverge from their underlying collections through independent updates
* The `pull` operation syncs the graph with its collection, similar to a Git pull
* This model enables experimental graph modifications without affecting the base collection

### Knowledge Graph Workflow

<accordiongroup>
  <accordion icon="brain" title="Extract Document Knowledge">
    Extract entities and relationships from the previously ingested document:

    <tabs>
      <tab title="Python">
        ```python
        client.documents.extract(document_id)
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.documents.extract(documentId);
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/documents/${document_id}/extract \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>

    This step processes the document to identify entities and their relationships.
  </accordion>

  <accordion icon="arrows-rotate" title="Initialize and Populate Graph">
    Sync the graph with the collection and view extracted knowledge:

    <tabs>
      <tab title="Python">
        ```python
        collection_id="122fdf6a-e116-546b-a8f6-e4cb2e2c0a09" # default collection_id for admin

        # Sync graph with collection
        pull_response = client.graphs.pull(collection_id)

        # View extracted knowledge
        entities = client.graphs.list_entities(collection_id)
        relationships = client.graphs.list_relationships(collection_id)
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        // Sync graph with collection
        await client.graphs.pull(collectionId);

        // View extracted knowledge
        const entities = await client.graphs.listEntities(collectionId);
        const relationships = await client.graphs.listRelationships(collectionId);
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        # Sync graph with collection
        curl -X POST https://api.sciphi.ai/v3/graphs/${collection_id}/pull \
          -H "Authorization: Bearer your_token_here"

        # View extracted knowledge
        curl -X GET https://api.sciphi.ai/v3/graphs/${collection_id}/entities \
          -H "Authorization: Bearer your_token_here"

        curl -X GET https://api.sciphi.ai/v3/graphs/${collection_id}/relationships \
          -H "Authorization: Bearer your_token_here"

        ```
      </tab>
    </tabs>
  </accordion>

  <accordion icon="diagram-project" title="Build Graph Communities">
    Build and list graph communities:

    <tabs>
      <tab title="Python">
        ```python
        # Build communities
        build_response = client.graphs.build(collection_id, settings={})

        # List communities
        communities = client.graphs.list_communities(collection_id)
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        // Build communities
        await client.graphs.build(collectionId, {});

        // List communities
        const communities = await client.graphs.listCommunities(collectionId);
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        # Build communities
        curl -X POST https://api.sciphi.ai/v3/graphs/${collection_id}/communities/build \
          -H "Content-Type: application/json" \
          -d '{}' \
          -H "Authorization: Bearer your_token_here"

        # List communities
        curl -X GET https://api.sciphi.ai/v3/graphs/${collection_id}/communities \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>

    ```plaintext
    [
      Community(
        name='Large Language Models and AGI Community', 
        summary='The Large Language Models and AGI Community focuses on the development and implications of advanced AI technologies, particularly in the pursuit of Artificial General Intelligence.', 
        level=None, 
        findings=['Large Language Models (LLMs) are rapidly evolving towards capabilities akin to Artificial General Intelligence (AGI) [Data: Descriptions (1579a46f-be12-4e60-a96b-e5b5afe026d9)].', 'The primary aim of LLMs is to achieve functionalities that closely resemble AGI [Data: Relationships (22bb116d-ab0b-4390-a68f-6ef1a1c99999)].', 'AGI systems are designed to outperform humans in most economically valuable tasks, indicating their potential impact on various industries [Data: Descriptions (80a34efa-d569-488f-91fd-db08fd93667b)].', 'The development of LLMs is a critical step towards realizing the goals of AGI, highlighting the interconnectedness of these technologies [Data: Relationships (22bb116d-ab0b-4390-a68f-6ef1a1c99999)].', 'Research in LLMs is essential for understanding the ethical implications of AGI deployment in society [Data: Descriptions (1579a46f-be12-4e60-a96b-e5b5afe026d9)].'], 
        id=UUID('62fd3478-f303-47ba-941a-fcf41576615d'), 
        community_id=None, 
        collection_id=UUID('122fdf6a-e116-546b-a8f6-e4cb2e2c0a09'), 
        rating=9.0,
        rating_explanation='This community has a significant impact on the future of AI, as it drives research towards achieving AGI capabilities.',
        ...
      ), ...
    ]
    ```
  </accordion>

  <accordion icon="magnifying-glass" title="Knowledge Graph Search">
    Perform knowledge graph-enhanced search (enabled by default):

    <tabs>
      <tab title="Python">
        ```python
        client.retrieval.search(
            "What was DeepSeek R1"
        )
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.retrieval.search({
          query: "who was aristotle?",
          graphSearchSettings: {
            useKgSearch: true,
            kgSearchType: "local"
          }
        });
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/retrieval/search \
          -H "Content-Type: application/json" \
          -d '{
            "query": "who was aristotle?",
            "graph_search_settings": {
              "use_graph_search": true,
              "kg_search_type": "local"
            }
          }' \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>
  </accordion>

  <accordion icon="trash" title="Cleanup">
    Reset the graph to a clean state:

    <tabs>
      <tab title="Python">
        ```python
        client.graphs.reset(collection_id)
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.graphs.reset(collectionId);
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/graphs/${collection_id}/reset \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>
    </tabs>
  </accordion>
</accordiongroup>

### Best Practices

1. **Graph Synchronization**
   * Always `pull` before attempting to list or work with entities
   * Keep track of which documents have been added to the graph

2. **Community Management**
   * Build communities after significant changes to the graph
   * Use community information to enhance search results

3. **Version Control**
   * Treat graphs like Git branches - experiment freely
   * Use `reset` to start fresh if needed
   * Maintain documentation of graph modifications

This Git-like model provides a flexible framework for knowledge management while maintaining data consistency and enabling experimental modifications.

## User Management

R2R provides robust user auth and management capabilities. This section briefly covers user authentication features and how they relate to document management.

<accordiongroup>
  <accordion icon="user-plus" title="User Registration">
    To register a new user:

    <tabs>
      <tab title="Python">
        ```python
        from r2r import R2RClient

        client.users.create("test@example.com", "password123")
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.users.create("test@gmail.com", "password123")
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/users/create \
          -H "Content-Type: application/json" \
          -d '{
            "email": "test@example.com",
            "password": "password123"
          }'
        ```
      </tab>
    </tabs>

    Example output:

    ```zsh
    User(
      id=UUID('fcbcbc64-f85c-5025-877c-37f4c7a12d6e'),
      email='test@example.com',
      is_active=True,
      is_superuser=False,
      created_at=datetime.datetime(2025, 2, 8, 5, 8, 17, 376293,
      tzinfo=TzInfo(UTC)),
      updated_at=datetime.datetime(2025, 2, 8, 5, 8, 17, 376293,
      tzinfo=TzInfo(UTC)),
      is_verified=False,
      collection_ids=[UUID('d3ef9c77-cb13-59a9-be70-0db46de619db')],
      graph_ids=[],
      document_ids=[],
      limits_overrides={},
      metadata={},
      verification_code_expiry=None,
      name=None,
      bio=None, 
      rofile_picture=None,
      total_size_in_bytes=None,
      num_files=None,
      account_type='password',
      hashed_password='JDJiJDEyJDE4UFdOTWZTSHNxdzRRMDdKZXU2Nk9qMFNNbXFxVFZldmpHaGhjdTcwdk5hNDZubEMxblVD',
      google_id=None,
      github_id=None
    )
    ```
  </accordion>

  <accordion icon="envelope" title="Email Verification">
    After registration, users need to verify their email:

    <tabs>
      <tab title="Python">
        ```python
        client.users.verify_email("123456")  # Verification code sent to email
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.users.verify_email("123456")
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/users/verify_email/123456
        ```
      </tab>
    </tabs>
  </accordion>

  <accordion icon="arrow-right-to-bracket" title="User Login">
    To log in and obtain access tokens:

    <tabs>
      <tab title="Python">
        ```python
        client.users.login("test@example.com", "password123")
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.users.login("test@example.com", "password123")
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/users/login \
          -H "Content-Type: application/x-www-form-urlencoded" \
          -d "username=test@example.com&amp;password=password123"
        ```
      </tab>
    </tabs>

    ```zsh
    LoginResponse(access_token=Token(token='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ0ZXN0QGV4YW1wbGUuY29tIiwidG9rZW5fdHlwZSI6ImFjY2VzcyIsImV4cCI6MTc0MjU5MTQ0Ni43MTY2MzcsImlhdCI6MTczODk5MTQ0Ni43MTY3MDUsIm5iZiI6MTczODk5MTQ0Ni43MTY3MDUsImp0aSI6IkhkWWVfeWxOSm9Yc2tvaU5ZVkdoNHc9PSIsIm5vbmNlIjoiMkhOOUs3bU40QVNfVnkzOTdXR2Vpdz09In0.gG_9oa-7_ZHqfHHo-bE1ooynCm7YCQFCYbJoiEgGmTg', token_type='access'), refresh_token=Token(token='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ0ZXN0QGV4YW1wbGUuY29tIiwidG9rZW5fdHlwZSI6InJlZnJlc2giLCJleHAiOjE3Mzk1OTYyNDYuNzE3MzQxLCJpYXQiOjE3Mzg5OTE0NDYuNzE3MzQ5LCJuYmYiOjE3Mzg5OTE0NDYuNzE3MzQ5LCJqdGkiOiJybXltZTk5bGNtZklOWDZLQWNaTmpBPT0iLCJub25jZSI6InExRGdqZm96YkpjYXpDbzdTcE5XcWc9PSJ9.Zn-2pncsEdvyuig36N4APO_U9AWDQcJi6E5EjglN16U', token_type='refresh'))
    ```
  </accordion>

  <accordion icon="magnifying-glass" title="User-Specific Search">
    Once authenticated, search results are automatically filtered to include only documents associated with the current user:

    <tabs>
      <tab title="Python">
        ```python
        # requires client.users.login(...)
        client.retrieval.search(query="What was DeepSeek R1 about?"
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.retrieval.search("What was DeepSeek R1 about?")
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/retrieval/search \
          -H "Authorization: Bearer YOUR_ACCESS_TOKEN" \
          -H "Content-Type: application/json" \
          -d '{
            "query": "What was DeepSeek R1 about?"
          }'
        ```
      </tab>
    </tabs>

    ```zsh
    AggregateSearchResult(chunk_search_results=[], graph_search_results=[], web_search_results=[], context_document_results=[])
    ```
  </accordion>

  <accordion icon="arrows-rotate" title="Refresh Access Token">
    To refresh an expired access token:

    <tabs>
      <tab title="Python">
        ```python
        # requires client.users.login(...)
        client.users.refresh_access_token()["results"]
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.refreshAccessToken()
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/users/refresh_access_token \
          -H "Authorization: Bearer YOUR_REFRESH_TOKEN" \
          -H "Content-Type: application/json" \
          -d '{
            "refresh_token": "YOUR_REFRESH_TOKEN"
          }'
        ```
      </tab>
    </tabs>
  </accordion>

  <accordion icon="arrow-right-from-bracket" title="User Logout">
    To log out and invalidate the current access token:

    <tabs>
      <tab title="Python">
        ```python
        # requires client.users.login(...)
        client.users.logout()
        ```
      </tab>

      <tab title="Curl">
        ```zsh
        curl -X POST https://api.sciphi.ai/v3/users/logout \
          -H "Authorization: Bearer your_token_here"
        ```
      </tab>

      <tab title="JavaScript">
        ```javascript
        await client.users.logout()
        ```
      </tab>
    </tabs>
  </accordion>
</accordiongroup>

These authentication features ensure that users can only access and manage their own documents. When performing operations like search, RAG, or document management, the results are automatically filtered based on the authenticated user's permissions.

Remember to replace `YOUR_ACCESS_TOKEN` and `YOUR_REFRESH_TOKEN` with actual tokens obtained during the login process.

These observability and analytics features provide valuable insights into your R2R application's performance and usage, enabling data-driven optimization and decision-making.

## Next Steps

Now that you have a basic understanding of R2R's core features, you can explore more advanced topics:

* Dive into [document ingestion](/documentation/documents) and [the document reference](/api-and-sdks/documents/documents).
* Learn about [search and RAG](/documentation/hybrid-search) and the [retrieval reference](/api-and-sdks/retrieval/retrieval).
* Try advanced techniques like [knowledge-graphs](/documentation/graphs) and refer to the [graph reference](/api-and-sdks/graphs/graphs).
* Learn about [user authentication](/documentation/user-auth) to secure your application permissions and [the users API reference](/api-and-sdks/users/users).
* Organize your documents using [collections](/api-and-sdks/collections/collections) for granular access control.


# Documents

&gt; Ingest and manage your documents

R2R provides a powerful and flexible ingestion to process and manage various types of documents. It supports a wide range of file formats‚Äîtext, documents, PDFs, images, audio, and even video‚Äîand transforms them into searchable, analyzable content.

The ingestion process includes parsing, chunking, embedding, and optionally extracting entities and relationships for knowledge graph construction.

This documentation will guide you through:

* Ingesting files, raw text, or pre-processed chunks
* Choosing an ingestion mode (`fast`, `hi-res`, or `custom`)
* Updating and deleting documents and chunks

Refer to the [documents API and SDK reference](/api-and-sdks/documents/documents) for detailed examples for interacting with documents.

## Synchronous vs Asynchronous SDK

The R2R SDK offers both synchronous and asynchronous clients to suit different application needs. Both provide the same functionality with slightly different usage patterns.

<tabs>
  <tab title="Synchronous Client">
    The standard client for most applications. Operations block until completed, making your code sequential and easier to reason about.

    ```python
    from r2r import R2RClient

    client = R2RClient()
    # when using auth, do client.login(...)

    response = client.documents.create(
        file_path="document.pdf",
        metadata={"source": "research paper"},
        id=None
    )

    # Code here runs after document creation completes
    print(response)
    ```
  </tab>

  <tab title="Asynchronous Client">
    Ideal for high-throughput applications or when processing multiple files concurrently. Uses Python's async/await syntax.

    ```python
    from r2r import R2RAsyncClient

    async def process_documents():
        aclient = R2RAsyncClient()
        # when using auth, do await aclient.login(...)
        
        response = await aclient.documents.create(
            file_path="document.pdf",
            metadata={"source": "research paper"},
            id=None
        )
        
        # Other operations can run while waiting for document creation
        return response

    # Must be called from an async context
    # import asyncio
    # asyncio.run(process_documents())
    ```
  </tab>
</tabs>

### When to Use Each Client

* **Synchronous Client (`R2RClient`)**: Best for scripts, notebooks, and applications with simple workflows. Provides straightforward, sequential code execution.

* **Asynchronous Client (`R2RAsyncClient`)**: Recommended for:
  * Processing multiple documents simultaneously
  * Building responsive web applications
  * Integrating with other async frameworks or services
  * High-throughput document processing pipelines

Both clients support the same methods and parameters, differing only in execution model. The async client requires using `await` with all API calls and running in an async context.

## Ingesting Documents

A `Document` represents ingested content in R2R. When you ingest a file, text, or chunks:

1. The file (or text) is parsed into text.
2. Text is chunked into manageable units.
3. Embeddings are generated for semantic search.
4. Content is stored for retrieval and optionally linked to the knowledge graph.

Ingestion inside R2R is asynchronous. You can monitor ingestion status and confirm when documents are ready:

```zsh
client.documents.list()

```

```plaintext
[
  DocumentResponse(
    id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'), 
    collection_ids=[UUID('122fdf6a-e116-546b-a8f6-e4cb2e2c0a09')], 
    owner_id=UUID('2acb499e-8428-543b-bd85-0d9098718220'), 
    document_type=<documenttype.pdf: 'pdf'="">, 
    metadata={'title': 'DeepSeek_R1.pdf', 'version': 'v0'}, 
    version='v0', 
    size_in_bytes=1768572, 
    ingestion_status=<ingestionstatus.success: 'success'="">, 
    extraction_status=<graphextractionstatus.pending: 'pending'="">, 
    created_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 126759, tzinfo=TzInfo(UTC)), 
    updated_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 160114, tzinfo=TzInfo(UTC)), 
    ingestion_attempt_number=None, 
    summary="The document contains a comprehensive overview of DeepSeek-R1, a series of reasoning models developed by DeepSeek-AI, which includes DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero utilizes large-scale reinforcement learning (RL) without supervised fine-tuning, showcasing impressive reasoning capabilities but facing challenges like readability and language mixing. To enhance performance, DeepSeek-R1 incorporates multi-stage training and cold-start data, achieving results comparable to OpenAI's models on various reasoning tasks. The document details the models' training processes, evaluation results across multiple benchmarks, and the introduction of distilled models that maintain reasoning capabilities while being smaller and more efficient. It also discusses the limitations of current models, such as language mixing and sensitivity to prompts, and outlines future research directions to improve general capabilities and efficiency in software engineering tasks. The findings emphasize the potential of RL in developing reasoning abilities in large language models and the effectiveness of distillation techniques for smaller models.", summary_embedding=None, total_tokens=29673)] total_entries=1
  ), ...
]
```

An `ingestion_status` of `"success"` confirms the document is fully ingested. You can also check your R2R dashboard for ingestion progress and status.

### Supported File Types

R2R supports ingestion of the following document types:

| Category          | File types                                |
| ----------------- | ----------------------------------------- |
| Image             | `.bmp`, `.heic`, `.jpeg`, `.png`, `.tiff` |
| MP3               | `.mp3`                                    |
| PDF               | `.pdf`                                    |
| CSV               | `.csv`                                    |
| E-mail            | `.eml`, `.msg`, `.p7s`                    |
| EPUB              | `.epub`                                   |
| Excel             | `.xls`, `.xlsx`                           |
| HTML              | `.html`                                   |
| Markdown          | `.md`                                     |
| Org Mode          | `.org`                                    |
| Open Office       | `.odt`                                    |
| Plain text        | `.txt`                                    |
| PowerPoint        | `.ppt`, `.pptx`                           |
| reStructured Text | `.rst`                                    |
| Rich Text         | `.rtf`                                    |
| TSV               | `.tsv`                                    |
| Word              | `.doc`, `.docx`                           |
| Code              | `.py`, `.js`, `.ts`, `.css`               |

For more details on creating documents, [refer to the create document API](/api-and-sdks/documents/create-document).

## Ingestion Modes

R2R offers three modes of ingestion to allow for maximal customization:

## Unprocessed files

<tabs>
  <tab title="fast">
    A speed-oriented ingestion mode that prioritizes rapid processing with minimal enrichment. Summaries and some advanced parsing are skipped, making this ideal for quickly processing large volumes of documents.

    ```python
      file_path = 'path/to/file.txt'

      # export R2R_API_KEY='sk-....'

      ingest_response = client.documents.create(
          file_path=file_path,
          ingestion_mode="fast" # fast mode for quick processing
      )
    ```
  </tab>

  <tab title="hi-res">
    A comprehensive, high-quality ingestion leverages multimodal foundation models (visual language models) for parsing complex documents and PDFs, even integrating image-based content.

    ```python
      file_path = 'path/to/file.txt'

      # hi-res mode for thorough extraction
      ingest_response = client.documents.create(
          file_path=file_path,
          ingestion_mode="hi-res",
          ingestion_config={ # custom chunking settings
              "chunking_strategy": "by_title",
              "new_after_n_chars": 256, # average chunk
              "max_characters": 512, # max chunk
              "combine_under_n_chars": 64, # chunk merge threshold
          }
      )
    ```
  </tab>

  <tab title="custom">
    For advanced users who require fine-grained control. In `custom` mode, you provide a full `ingestion_config` dict or object to specify every detail: parser options, chunking strategy, character limits, and more.

    ```python
      file_path = 'path/to/file.txt'

      # custom mode for full control
      ingest_response = client.documents.create(
          file_path=file_path,
          ingestion_mode="custom",
          ingestion_config={
              "strategy": "auto",
              "chunking_strategy": "by_title",
              "new_after_n_chars": 256, # average chunk
              "max_characters": 512, # max chunk
              "combine_under_n_chars": 64, # chunk merge threshold
              "overlap": 100,
          }
      )
    ```
  </tab>
</tabs>

## Pre-Processed Chunks

If you have pre-processed chunks from your own pipeline, you can directly ingest them. This is especially useful if you've already divided content into logical segments.

### Raw text

```python
raw_text = "This is my first document."
client.documents.create(
    raw_text=raw_text,
)
```

### Pre-Processed Chunks

```python
chunks = ["This is my first parsed chunk", "This is my second parsed chunk"]
client.documents.create(
    chunks=chunks,
)
```

## Deleting Documents and Chunks

To remove documents or chunks, call their respective `delete` methods:

```python
# Delete a document
client.documents.delete(document_id)

# Delete a chunk
client.chunks.delete(chunk_id)
```

You can also delete documents by specifying filters using the [`by-filter`](/api-and-sdks/documents/delete-document-by-filter) route.

## Conclusion

R2R's ingestion is flexible and efficient, allowing you to tailor ingestion to your needs:

* Use `fast` for quick processing.
* Use `hi-res` for high-quality, multimodal analysis.
* Use `custom` for advanced, granular control.

You can easily ingest documents or pre-processed chunks, update their content, and delete them when no longer needed. Combined with powerful retrieval and knowledge graph capabilities, R2R enables seamless integration of advanced document management into your applications.


# Conversations

&gt; Organize and manage multi-turn chat sessions

R2R Conversations enable multi-turn interactions between users and the system, storing messages and preserving context across interactions. They serve as containers for chat sessions, agent interactions, and collaborative discussions.

Refer to the [conversations API and SDK reference](/api-and-sdks/conversations/conversations) for detailed examples for interacting with conversations.

## Core Concepts

Conversations in R2R maintain context through three key mechanisms:

1. **Message Threading** - Messages are stored in chronological order with optional parent-child relationships, enabling threaded discussions and branching conversations.

2. **Context Preservation** - The system preserves conversation context across messages, allowing for coherent multi-turn interactions and advanced retrieval capabilities.

3. **User Association** - Each conversation is owned by a specific user and can be shared with other users, enabling both private and collaborative chat sessions.

## Message Management

### Creating Messages

Messages represent individual turns in a conversation. Each message includes:

* Content (the actual message text)
* Role (user, assistant, or system)
* Optional parent message reference
* Metadata for additional context

Messages can be added to conversations at any time, and the system maintains their chronological order while preserving threading relationships.

### Updating Messages

The system allows for message editing while maintaining conversation integrity:

* Content can be updated
* Metadata can be modified or enriched
* Threading relationships remain intact
* Edit history is preserved in metadata

## Conversation Features

### Organization

Conversations can be organized and managed through:

* Custom naming and descriptions
* Filtering and search capabilities
* Metadata tags and annotations
* Chronological or threaded views

### Access Control

R2R implements straightforward access controls for conversations:

* Private conversations visible only to their owner
* Shared conversations accessible to specified users
* Superuser access for system management

## Integration with Agents

<note>
  Conversations integrate deeply with R2R's [Agentic RAG](/documentation/retrieval/agentic-rag) system for advanced AI interactions and automated processing.
</note>

When used with agents, conversations enable:

* Persistent context for AI interactions
* Multi-turn query processing
* Knowledge graph integration
* Automated content analysis

## Superuser Features

Superusers have access to additional conversation management capabilities:

* Bulk export of conversations
* Usage analytics and reporting
* System-wide conversation search
* Advanced filtering and organization

## Data Management

The system provides tools for effective conversation management:

1. **Retrieval** - Fetch conversations by ID, filter by date, or search content
2. **Updates** - Modify conversation properties and message content
3. **Deletion** - Remove conversations while preserving system integrity
4. **Export** - Download conversation data in standard formats

## Conclusion

R2R Conversations provide a robust foundation for managing multi-turn interactions. Through careful message threading, context preservation, and integration with other R2R systems, conversations enable sophisticated chat applications, agent interactions, and collaborative discussions.


# Collections

&gt; Organize documents and manage permissions

R2R provides a powerful and flexible collection system to organize documents and manage access permissions. Collections serve as containers that help you group related content, control access among users, and build knowledge graphs across document sets.

Refer to the [collections API and SDK reference](/api-and-sdks/collections/collections) for detailed examples for interacting with collections.

## Overview

Collections in R2R serve three core purposes:

1. Group documents into logical categories for better organization and discovery
2. Control access permissions at a collection level rather than per document
3. Enable knowledge extraction and insights across related document sets

Each user receives a default collection upon joining R2R, and can create additional collections as needed to organize different types of content.

## Collection Management

When you create a collection in R2R, you become its owner and gain full control over its configuration and contents. Collections can contain any number of documents, and a single document can belong to multiple collections simultaneously.

The system maintains a complete audit trail of collection changes, tracking modifications to:

* Collection metadata (name, description, settings)
* Document additions and removals
* User access grants and revocations
* Knowledge graph extractions and updates

When deleting a collection, R2R preserves all documents while removing only the collection structure and associated permissions. This ensures no content is accidentally lost during collection management.

## Permission Model

R2R implements a straightforward permission model for collections:

* **Collection Owners** have full control, including deletion rights and user management
* **Collection Members** can access and interact with documents based on granted permissions
* **Non-Members** have no access to the collection or its contents

The permission model ensures documents remain secure while making sharing straightforward. When you add a document to a collection, it automatically inherits the collection's permission settings.

## Document Management

The collection system provides comprehensive tools for organizing and managing documents. You can view collection contents, filter documents by various criteria, and perform batch operations across document sets.

Key document management features include:

* Adding documents during or after initial ingestion
* Removing documents without affecting the original content
* Filtering and searching within collection contents
* Exporting document lists and collection metadata

## Knowledge Graph Integration

Collections in R2R integrate deeply with the knowledge graph system. When enabled, R2R processes documents within collections to:

1. Extract entities and relationships from document content
2. Build semantic connections between related documents
3. Generate collection-level insights and summaries
4. Enable semantic search across collection contents

## Enterprise Features

<warning>
  The following features are restricted to:

  * Self-hosted instances
  * Enterprise tier cloud accounts

  Contact our sales team for pricing and availability.
</warning>

Enterprise deployments gain access to advanced collection capabilities including:

* Hierarchical collections for complex organizational structures
* Collection templates for standardized content organization
* Automated permission sync with external systems
* Advanced analytics and reporting
* Custom metadata fields
* Comprehensive audit logging

## Conclusion

Collections form the foundation of document organization and access control in R2R. Through their flexible design and powerful features, they enable teams to create organized, secure, and collaborative document management systems. Whether managing a small team's documents or implementing enterprise-wide content organization, collections provide the tools needed for effective document management.


# Knowledge Graphs in R2R

&gt; Building and managing knowledge graphs through collections

R2R's knowledge graph system automatically extracts entities and relationships from documents, organizing them into rich semantic networks for improved search, analysis and knowledge discovery. The system integrates tightly with collections to enable flexible organization and access control.

<note>
  For an end-to-end example of building a graph, check out our [graph cookbook](/cookbooks/graphs)
</note>

Refer to the [graphs API and SDK reference](/api-and-sdks/graphs/graphs) for detailed examples for interacting with graphs.

## Core Concepts

Graphs in R2R operate at two levels:

1. **Document Level**: Individual documents undergo entity and relationship extraction using advanced language models. This captures key concepts, people, organizations, and connections within each document.

2. **Collection Level**: Collections act as containers for documents and maintain unified graphs. Collection graphs combine and deduplicate entities across documents while preserving source information.

## Building Graphs

### Element Extraction

When you extract the entities and relationships from a document, R2R:

1. Analyzes document content using language models to identify entities
2. Extracts relationships between entities
3. Generates rich metadata and descriptions
4. Creates embeddings for semantic search

These are then used to populate a graph.

For example, after extraction from a research paper:

```python
# View extracted entities
entities = client.documents.list_entities(document_id)
# [
#     Entity(
#         name='DeepSeek-R1', 
#         description='DeepSeek-R1 is a series of reasoning models developed by DeepSeek-AI, which incorporates multi-stage training and cold-start data to enhance reasoning performance.', 
#         category='Model', 
#         metadata=None, 
#         id=UUID('b0aab314-1289-41b9-9fb4-85e067ef640c'), 
#         parent_id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'), 
#         description_embedding=None, 
#         chunk_ids=[UUID('3a35e39f-818b-5763-a909-657253d43577'), UUID('caeee102-0b6d-5ee6-9552-f42e75b8e1a9')]
#     ), 
#     Entity(...), ...
# ]

# View relationships between entities 
relationships = client.documents.list_relationships(document_id)
# [
#     Relationship(
#         id=UUID('b09b17dd-65ee-4cf3-92a3-28b09afcc2d5'), 
#         subject='DeepSeek-R1', 
#         predicate='Includes', 
#         object='DeepSeek-R1-Zero', 
#         description='DeepSeek-R1 includes DeepSeek-R1-Zero as a variant that showcases impressive reasoning capabilities.', 
#         subject_id=UUID('b0aab314-1289-41b9-9fb4-85e067ef640c'), 
#         object_id=UUID('b5d5a457-aa54-4b66-8757-da7bdbf423c4'), 
#         weight=9.0, 
#         chunk_ids=None,
#         parent_id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'), 
#         description_embedding=None, 
#         metadata=None), 
#         Relationship(...), ...
# ]
```

### Collection Graphs

Collections maintain unified knowledge graphs that combine entities and relationships across documents. The system:

1. Deduplicates entities and relationships
2. Preserves document source information
3. Updates automatically as documents are added
4. Enables graph-wide analysis

## Knowledge Graph Communities

R2R automatically analyzes graph structure to identify logical groupings of related entities called communities. This enables:

1. Higher-level understanding of themes across many documents
2. Discovery of hidden connections
3. Improved knowledge navigation
4. Semantic topic clustering


  <img src="file://de8d9036-583f-4a59-bad2-1c31c845b7d8/">


## Using Knowledge Graphs

### Enhanced Search

Knowledge graphs automatically improve search by:

1. Providing rich entity and relationship context
2. Enabling semantic similarity matching
3. Supporting concept-based navigation
4. Surfacing related content through graph connections

```python
# Search with knowledge graph context
results = client.retrieval.search(
    "What is deep learning?",
    search_settings={
        "graph_settings": {"enabled": True}
    }
)
```

### RAG Integration

Knowledge graphs enhance RAG responses by providing:

* Structured entity information
* Relationship context
* Community-level insights
* Cross-document connections

```python
# RAG with knowledge graph context
response = client.retrieval.rag(
    "Explain deep learning's relationship to ML",
    graph_settings={"enabled": True}
)
```

## Enterprise Features

<warning>
  The following features are restricted to:

  * Self-hosted instances
  * Enterprise tier cloud accounts

  Contact our sales team for pricing and availability.
</warning>

Advanced knowledge graph capabilities include:

* Custom entity extraction rules
* Manual graph curation tools
* Graph export and import
* Advanced graph analytics
* Custom visualization tools

## Conclusion

R2R's knowledge graphs provide powerful document analysis and knowledge discovery capabilities through automatic entity extraction and graph construction. Deep integration with collections enables flexible organization, while community detection uncovers hidden patterns and relationships in your content.


# Prompts

&gt; Create and manage reusable prompt templates

<warning>
  Prompt management features are currently restricted to:

  * Self-hosted instances
  * Enterprise tier cloud accounts

  Contact our sales team for Enterprise pricing and features.
</warning>

R2R provides a powerful prompt management system that enables you to create, store, and reuse prompt templates across your application. The system supports variable substitution, input validation, and efficient caching for high-performance applications.

Refer to the [prompts API and SDK reference](/api-and-sdks/prompts/prompts) for detailed examples for interacting with prompts.

## Core Concepts

The prompt system operates using three main components:

1. **Templates** - Reusable prompt patterns with variable placeholders
2. **Input Types** - Type definitions for template variables ensuring proper usage
3. **Caching** - Performance optimization for frequently used prompts

## Template Management

### Creating Templates

Templates are prompt patterns that can include variable placeholders. Each template includes:

* A unique name for identification
* The template text with variable placeholders
* Input type definitions specifying expected variable types

For example, a simple greeting template:

```python
template = "Hello {name}, welcome to {company}!"
input_types = {
    "name": "string",
    "company": "string"
}
```

### Input Validation

R2R automatically validates inputs against defined types before rendering templates. This ensures:

* Required variables are provided
* Values match their expected types
* Invalid or missing variables are caught early

### Template Inheritance

Templates can build on each other through:

1. Base templates for common patterns
2. Specialized templates that extend base templates
3. Override capabilities for customization

## Using Prompts

### Basic Usage

Templates can be used directly with input values:

```python
greeting = prompts.get(
    name="welcome_template",
    inputs={
        "name": "John",
        "company": "Acme Inc"
    }
)
# -&gt; "Hello John, welcome to Acme Inc!"
```

### System Prompts

Special system prompts can be defined for consistent AI interactions across your application. These provide:

1. Base context for AI models
2. Standard instruction sets
3. Common constraints or rules

### Task Prompts

Task-specific prompts build on system prompts to:

1. Define specific operations or questions
2. Include relevant context
3. Guide model responses

## Performance Optimization

The prompt system includes built-in performance features:

1. **Template Caching** - Frequently used templates are cached in memory
2. **Render Caching** - Common prompt/input combinations are cached
3. **Smart Invalidation** - Cache updates when templates change

## Conclusion

R2R's prompt management system provides a robust foundation for working with AI models. Through templates, input validation, and performance optimization, it enables consistent and efficient prompt usage across your application.


# Users

&gt; Manage users and authentication

<warning>
  User management features are currently restricted to:

  * Self-hosted instances
  * Enterprise tier cloud accounts

  Contact our sales team for Enterprise pricing and features.
</warning>

R2R provides a comprehensive user management and authentication system that enables secure access control, user administration, and profile management. This system serves as the foundation for document ownership, collection permissions, and collaboration features throughout R2R.

Refer to the [users API and SDK reference](/api-and-sdks/users/users) for detailed examples for interacting with users.

## Core Concepts

R2R's user system is built around three fundamental principles. First, it ensures secure authentication through multiple methods including email/password and API keys. Second, it provides flexible authorization with role-based access control. Third, it maintains detailed user profiles that integrate with R2R's document and collection systems.

## Authentication

Users can authenticate with R2R through several secure methods. Traditional email and password authentication provides standard access, while API keys enable programmatic integration. The system supports session management with refresh tokens for extended access and automatic session expiration for security.

When email verification is enabled, new users must verify their email address before gaining full system access. This verification process helps prevent unauthorized accounts and ensures reliable communication channels for important system notifications.

## User Management

### Profile Information

Each user in R2R has a comprehensive profile that includes:

1. Core Identity
   * Email address (unique identifier)
   * Display name
   * Optional biography and profile picture

2. System Status
   * Account creation date
   * Active/inactive status
   * Verification status
   * Last activity timestamp

### Role-Based Access

R2R implements a straightforward but powerful role system:

Regular users can manage their own content, including:

* Creating and managing documents
* Participating in collections they're granted access to
* Managing their profile and authentication methods

Superusers have additional system-wide capabilities:

* Managing other user accounts
* Accessing system settings and configurations
* Viewing usage analytics and audit logs
* Overriding standard permission limits

## API Access

R2R provides flexible API access through dedicated API keys. Users can:

* Generate multiple API keys for different applications
* Name and track individual keys
* Monitor key usage and last-access times
* Rotate or revoke keys as needed

The system maintains a clear audit trail of API key creation, usage, and deletion to help users manage their programmatic access securely.

## Security Features

### Account Protection

R2R implements multiple security measures to protect user accounts:

* Strong password requirements
* Secure password reset flows
* Session management and forced logout capabilities
* Activity monitoring and suspicious behavior detection

### Email Security

The email system handles several security-critical functions:

* Account verification for new users
* Secure password reset workflows
* Important security notifications
* System alerts and updates

## Document Management

Users automatically become owners of documents they create, granting them full control over those resources. Through collections, users can:

* Share documents with other users
* Set document permissions
* Track document usage and access
* Manage document lifecycles

## Enterprise Features

<warning>
  The following features require an Enterprise license or self-hosted installation. Contact our sales team for details.
</warning>

Enterprise deployments gain access to advanced user management features including:

* Single Sign-On (SSO) integration
* Advanced user analytics and reporting
* Custom user fields and metadata
* Bulk user management tools
* Enhanced security policies and controls

## Conclusion

The R2R user system provides a secure and flexible foundation for document management and collaboration. Through careful design and robust security measures, it enables both simple user management and complex enterprise scenarios while maintaining strong security standards.


# Search and RAG

R2R provides powerful search and retrieval capabilities through vector search, full-text search, hybrid search, and Retrieval-Augmented Generation (RAG). The system supports multiple search modes and extensive runtime configuration to help you find and contextualize information effectively.

Refer to the [retrieval API and SDK reference](/api-and-sdks/retrieval/retrieval) for detailed retrieval examples.

## Search Modes and Settings

When using the Search (`/retrieval/search`) or RAG (`/retrieval/rag`) endpoints, you control the retrieval process using `search_mode` and `search_settings`.

* **`search_mode`** (Optional, defaults to `custom`): Choose between pre-configured modes or full customization.
  * `basic`: Defaults to a simple semantic search configuration. Good for quick setup.
  * `advanced`: Defaults to a hybrid search configuration combining semantic and full-text. Offers broader results.
  * `custom`: Allows full control via the `search_settings` object. If `search_settings` are omitted in `custom` mode, default vector search settings are applied.
* **`search_settings`** (Optional): A detailed configuration object. If provided alongside `basic` or `advanced` modes, these settings will override the mode's defaults. Key settings include:
  * `use_semantic_search`: Boolean to enable/disable vector-based semantic search (default: `true` unless overridden).
  * `use_fulltext_search`: Boolean to enable/disable keyword-based full-text search (default: `false` unless using hybrid).
  * `use_hybrid_search`: Boolean to enable hybrid search, combining semantic and full-text (default: `false`). Requires `hybrid_settings`.
  * `filters`: Apply complex filtering rules using MongoDB-like syntax (see "Advanced Filtering" below).
  * `limit`: Integer controlling the maximum number of results to return (default: `10`).
  * `hybrid_settings`: Object to configure weights (`semantic_weight`, `full_text_weight`), limits (`full_text_limit`), and fusion (`rrf_k`) for hybrid search.
  * `chunk_settings`: Object to fine-tune vector index parameters like `index_measure` (distance metric), `probes`, `ef_search`.
  * `search_strategy`: String to enable advanced RAG techniques like `"hyde"` or `"rag_fusion"` (default: `"vanilla"`). See [Advanced RAG](/documentation/advanced-rag).
  * `include_scores`: Boolean to include relevance scores in the results (default: `true`).
  * `include_metadatas`: Boolean to include metadata in the results (default: `true`).

## AI Powered Search (`/retrieval/search`)

R2R offers powerful and highly configurable search capabilities. This endpoint returns raw search results without LLM generation.

### Basic Search Example

This performs a search using default configurations or a specified mode.

<tabs>
  <tab title="Python">
    ```python
    # Uses default settings (likely semantic search in 'custom' mode)
    results = client.retrieval.search(
      query="What is DeepSeek R1?",
    )

    # Explicitly using 'basic' mode
    results_basic = client.retrieval.search(
      query="What is DeepSeek R1?",
      search_mode="basic",
    )
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    // Uses default settings
    const results = await client.retrieval.search({
      query: "What is DeepSeek R1?",
    });

    // Explicitly using 'basic' mode
    const resultsBasic = await client.retrieval.search({
      query: "What is DeepSeek R1?",
      searchMode: "basic",
    });
    ```
  </tab>

  <tab title="Curl">
    ```bash
    # Uses default settings
    curl -X POST "https://api.sciphi.ai/v3/retrieval/search" \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer YOUR_API_KEY" \
      -d '{
        "query": "What is DeepSeek R1?"
      }'

    # Explicitly using 'basic' mode
    curl -X POST "https://api.sciphi.ai/v3/retrieval/search" \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer YOUR_API_KEY" \
      -d '{
        "query": "What is DeepSeek R1?",
        "search_mode": "basic"
      }'
    ```
  </tab>
</tabs>

**Response Structure (`WrappedSearchResponse`):**

The search endpoint returns a `WrappedSearchResponse` containing an `AggregateSearchResult` object with fields like:

* `results.chunk_search_results`: A list of relevant text `ChunkSearchResult` objects found (containing `id`, `document_id`, `text`, `score`, `metadata`).
* `results.graph_search_results`: A list of relevant `GraphSearchResult` objects (entities, relationships, communities) if graph search is active and finds results.
* `results.web_search_results`: A list of `WebSearchResult` objects (if web search was somehow enabled, though typically done via RAG/Agent).

```json
// Simplified Example Structure
{
  "results": {
    "chunk_search_results": [
      {
        "score": 0.643,
        "text": "Document Title: DeepSeek_R1.pdf...",
        "id": "chunk-uuid-...",
        "document_id": "doc-uuid-...",
        "metadata": { ... }
      },
      // ... more chunks
    ],
    "graph_search_results": [
      // Example: An entity result if graph search ran
      {
         "id": "graph-entity-uuid...",
         "content": { "name": "DeepSeek-R1", "description": "A large language model...", "id": "entity-uuid..." },
         "result_type": "ENTITY",
         "score": 0.95,
         "metadata": { ... }
      }
      // ... potentially relationships or communities
    ],
    "web_search_results": []
  }
}
```

### Hybrid Search Example

Combine keyword-based (full-text) search with vector search for potentially broader results.

<tabs>
  <tab title="Python">
    ```python
    hybrid_results = client.retrieval.search(
        query="What was Uber's profit in 2020?",
        search_settings={
            "use_hybrid_search": True,
            "hybrid_settings": {
                "full_text_weight": 1.0,
                "semantic_weight": 5.0,
                "full_text_limit": 200, # How many full-text results to initially consider
                "rrf_k": 50, # Parameter for Reciprocal Rank Fusion
            },
            "filters": {"metadata.title": {"$in": ["uber_2021.pdf"]}}, # Filter by metadata field
            "limit": 10 # Final number of results after fusion/ranking
        },
    )
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    const hybridResults = await client.retrieval.search({
      query: "What was Uber's profit in 2020?",
      searchSettings: {
        useHybridSearch: true,
        hybridSettings: {
            fullTextWeight: 1.0,
            semanticWeight: 5.0,
            fullTextLimit: 200,
            rrfK: 50 // Assuming camelCase mapping in JS SDK
        },
        filters: {"metadata.title": {"$in": ["uber_2021.pdf"]}},
        limit: 10
      },
    });
    ```
  </tab>

  <tab title="Curl">
    ```bash
    curl -X POST "https://api.sciphi.ai/v3/retrieval/search" \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer YOUR_API_KEY" \
      -d '{
        "query": "What was Uber'\''s profit in 2020?",
        "search_settings": {
          "use_hybrid_search": true,
          "hybrid_settings": {
            "full_text_weight": 1.0,
            "semantic_weight": 5.0,
            "full_text_limit": 200,
            "rrf_k": 50
          },
          "filters": {"metadata.title": {"$in": ["uber_2021.pdf"]}},
          "limit": 10,
          "chunk_settings": {
            "index_measure": "l2_distance"
          }
        }
      }'
    ```
  </tab>
</tabs>

### Advanced Filtering

Apply filters to narrow search results based on document properties or metadata. Supported operators include `$eq`, `$neq`, `$gt`, `$gte`, `$lt`, `$lte`, `$like`, `$ilike`, `$in`, `$nin`. You can combine filters using `$and` and `$or`.

<tabs>
  <tab title="Python">
    ```python
    filtered_results = client.retrieval.search(
        query="What are the effects of climate change?",
        search_settings={
            "filters": {
                "$and":[
                    {"document_type": {"$eq": "pdf"}}, # Assuming 'document_type' is stored
                    {"metadata.year": {"$gt": 2020}} # Access nested metadata fields
                ]
            },
            "limit": 10
        }
    )
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    const filteredResults = await client.retrieval.search({
      query: "What are the effects of climate change?",
      searchSettings: {
        filters: {
          $and: [
            {document_type: {$eq: "pdf"}},
            {"metadata.year": {$gt: 2020}}
          ]
        },
        limit: 10
      }
    });
    ```
  </tab>
</tabs>

### Distance Measures for Vector Search

Distance metrics for vector search, which can be configured through the `chunk_settings.index_measure` parameter. Choosing the right distance measure can significantly impact search quality depending on your embeddings and use case:

* **`cosine_distance`** (Default): Measures the cosine of the angle between vectors, ignoring magnitude. Best for comparing documents regardless of their length.
* **`l2_distance`** (Euclidean): Measures the straight-line distance between vectors. Useful when both direction and magnitude matter.
* **`max_inner_product`**: Optimized for finding vectors with similar direction. Good for recommendation systems.
* **`l1_distance`** (Manhattan): Measures the sum of absolute differences. Less sensitive to outliers than L2.
* **`hamming_distance`**: Counts the positions at which vectors differ. Best for binary embeddings.
* **`jaccard_distance`**: Measures dissimilarity between sample sets. Useful for sparse embeddings.

<tabs>
  <tab title="Python">
    ```python
    results = client.retrieval.search(
      query="What are the key features of quantum computing?",
      search_settings={
        "chunk_settings": {
          "index_measure": "l2_distance"  # Use Euclidean distance instead of default
        }
      }
    )
    ```
  </tab>
</tabs>

For most text embedding models (e.g., OpenAI's models), cosine\_distance is recommended. For specialized embeddings or specific use cases, experiment with different measures to find the optimal setting for your data.

## Knowledge Graph Enhanced Retrieval

Beyond searching through text chunks, R2R can leverage knowledge graphs to enrich the retrieval process. This offers several benefits:

* **Contextual Understanding:** Knowledge graphs store information as entities (like people, organizations, concepts) and relationships (like "works for", "is related to", "is a type of"). Searching the graph allows R2R to find connections and context that might be missed by purely text-based search.
* **Relationship-Based Queries:** Answer questions that rely on understanding connections, such as "What projects is Person X involved in?" or "How does Concept A relate to Concept B?".
* **Discovering Structure:** Graph search can reveal higher-level structures, such as communities of related entities or key connecting concepts within your data.
* **Complementary Results:** Graph results (entities, relationships, community summaries) complement text chunks by providing structured information and broader context.

When knowledge graph search is active within R2R, the `AggregateSearchResult` returned by the Search or RAG endpoints may include relevant items in the `graph_search_results` list, enhancing the context available for understanding or generation.

## Retrieval-Augmented Generation (RAG) (`/retrieval/rag`)

R2R's RAG engine combines the search capabilities above (including text, vector, hybrid, and potentially graph results) with Large Language Models (LLMs) to generate contextually relevant responses grounded in your ingested documents and optional web search results.

### RAG Configuration (`rag_generation_config`)

Control the LLM's generation process:

* `model`: Specify the LLM to use (e.g., `"openai/gpt-4o-mini"`, `"anthropic/claude-3-haiku-20240307"`). Defaults are set in R2R config.
* `stream`: Boolean (default `false`). Set to `true` for streaming responses.
* `temperature`, `max_tokens`, `top_p`, etc.: Standard LLM generation parameters.

### Basic RAG

Generate a response using retrieved context. Uses the same `search_mode` and `search_settings` as the search endpoint to find relevant information.

<tabs>
  <tab title="Python">
    ```python
    # Basic RAG call using default search and generation settings
    rag_response = client.retrieval.rag(query="What is DeepSeek R1?")
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    // Basic RAG call using default settings
    const ragResponse = await client.retrieval.rag({ query: "What is DeepSeek R1?" });
    ```
  </tab>

  <tab title="Curl">
    ```bash
    curl -X POST "https://api.sciphi.ai/v3/retrieval/rag" \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer YOUR_API_KEY" \
      -d '{
        "query": "What is DeepSeek R1?"
      }'
    ```
  </tab>
</tabs>

**Response Structure (`WrappedRAGResponse`):**

The non-streaming RAG endpoint returns a `WrappedRAGResponse` containing an `RAGResponse` object with fields like:

* `results.generated_answer`: The final synthesized answer from the LLM.
* `results.search_results`: The `AggregateSearchResult` used to generate the answer (containing chunks, possibly graph results, and web results).
* `results.citations`: A list of `Citation` objects linking parts of the answer to specific sources (`ChunkSearchResult`, `GraphSearchResult`, `WebSearchResult`, etc.) found in `search_results`. Each citation includes an `id` (short identifier used in the text like `[1]`) and a `payload` containing the source object.
* `results.metadata`: LLM provider metadata about the generation call.

```json
// Simplified Example Structure
{
  "results": {
    "generated_answer": "DeepSeek-R1 is a model that... [1]. It excels in tasks... [2].",
    "search_results": {
      "chunk_search_results": [ { "id": "chunk-abc...", "text": "...", "score": 0.8 }, /* ... */ ],
      "graph_search_results": [ { /* Graph Entity/Relationship */ } ],
      "web_search_results": [ { "url": "...", "title": "...", "snippet": "..." }, /* ... */ ]
    },
    "citations": [
      {
        "id": "cit.1", // Corresponds to [1] in text
        "object": "citation",
        "payload": { /* ChunkSearchResult for chunk-abc... */ }
      },
      {
        "id": "cit.2", // Corresponds to [2] in text
        "object": "citation",
        "payload": { /* WebSearchResult for relevant web page */ }
      }
      // ... more citations potentially linking to graph results too
    ],
    "metadata": { "model": "openai/gpt-4o-mini", ... }
  }
}

```

### RAG with Web Search Integration

Enhance RAG responses with up-to-date information from the web by setting `include_web_search=True`.

<tabs>
  <tab title="Python">
    ```python
    web_rag_response = client.retrieval.rag(
        query="What are the latest developments with DeepSeek R1?",
        include_web_search=True
    )
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    const webRagResponse = await client.retrieval.rag({
      query: "What are the latest developments with DeepSeek R1?",
      includeWebSearch: true // Use camelCase for JS SDK
    });
    ```
  </tab>

  <tab title="Curl">
    ```bash
    curl -X POST "https://api.sciphi.ai/v3/retrieval/rag" \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer YOUR_API_KEY" \
      -d '{
        "query": "What are the latest developments with DeepSeek R1?",
        "include_web_search": true
      }'
    ```
  </tab>
</tabs>

When enabled, R2R performs a web search using the query, and the results are added to the context provided to the LLM alongside results from your documents or knowledge graph.

### RAG with Hybrid Search

Combine hybrid search with RAG by configuring `search_settings`.

<tabs>
  <tab title="Python">
    ```python
    hybrid_rag_response = client.retrieval.rag(
        query="Who is Jon Snow?",
        search_settings={"use_hybrid_search": True}
    )
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    const hybridRagResponse = await client.retrieval.rag({
      query: "Who is Jon Snow?",
      searchSettings: {
        useHybridSearch: true
      },
    });
    ```
  </tab>

  <tab title="Curl">
    ```bash
    # Correctly place use_hybrid_search in search_settings
    curl -X POST "https://api.sciphi.ai/v3/retrieval/rag" \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer YOUR_API_KEY" \
      -d '{
        "query": "Who is Jon Snow?",
        "search_settings": {
          "use_hybrid_search": true,
          "limit": 10
        }
      }'
    ```
  </tab>
</tabs>

### Streaming RAG

Receive RAG responses as a stream of Server-Sent Events (SSE) by setting `stream: True` in `rag_generation_config`. This is ideal for real-time applications.

**Event Types:**

1. `search_results`: Contains the initial `AggregateSearchResult` (sent once at the beginning).
   * `data`: The full `AggregateSearchResult` object (chunks, potentially graph results, web results).
2. `message`: Streams partial tokens of the response as they are generated.
   * `data.delta.content`: The text chunk being streamed.
3. `citation`: Indicates when a citation source is identified. Sent *once* per unique source when it's first referenced.
   * `data.id`: The short citation ID (e.g., `"cit.1"`).
   * `data.payload`: The full source object (`ChunkSearchResult`, `GraphSearchResult`, `WebSearchResult`, etc.).
   * `data.is_new`: True if this is the first time this citation ID is sent.
   * `data.span`: The start/end character indices in the *current* accumulated text where the citation marker (e.g., `[1]`) appears.
4. `final_answer`: Sent once at the end, containing the complete generated answer and structured citations.
   * `data.generated_answer`: The full final text.
   * `data.citations`: List of all citations, including their `id`, `payload`, and all `spans` where they appeared in the final text.

<tabs>
  <tab title="Python">
    ```python
    from r2r import (
        CitationEvent,
        FinalAnswerEvent,
        MessageEvent,
        SearchResultsEvent,
        R2RClient,
        # Assuming ThinkingEvent is imported if needed, though not standard in basic RAG
    )

    # Set stream=True in rag_generation_config
    result_stream = client.retrieval.rag(
        query="What is DeepSeek R1?",
        search_settings={"limit": 25},
        rag_generation_config={"stream": True, "model": "openai/gpt-4o-mini"},
        include_web_search=True,
    )

    for event in result_stream:
        if isinstance(event, SearchResultsEvent):
            print(f"Search results received (Chunks: {len(event.data.data.chunk_search_results)}, Graph: {len(event.data.data.graph_search_results)}, Web: {len(event.data.data.web_search_results)})")
        elif isinstance(event, MessageEvent):
            # Access the actual text delta
            if event.data.delta and event.data.delta.content and event.data.delta.content[0].type == 'text' and event.data.delta.content[0].payload.value:
                 print(event.data.delta.content[0].payload.value, end="", flush=True)
        elif isinstance(event, CitationEvent):
            # Payload is only sent when is_new is True
            if event.data.is_new:
                print(f"\n&lt;&lt;&lt; New Citation Source Detected: ID={event.data.id} &gt;&gt;&gt;")

        elif isinstance(event, FinalAnswerEvent):
            print("\n\n--- Final Answer ---")
            print(event.data.generated_answer)
            print("\n--- Citations Summary ---")
            for cit in event.data.citations:
                 print(f"  ID: {cit.id}, Spans: {cit.span}")
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    // Set stream: true in ragGenerationConfig
    const resultStream = await client.retrieval.rag({
      query: "What is DeepSeek R1?",
      searchSettings: { limit: 25 },
      ragGenerationConfig: { stream: true, model: "openai/gpt-4o-mini" },
      includeWebSearch: true,
    });

    // Check if we got an async iterator (streaming)
    if (Symbol.asyncIterator in resultStream) {
      console.log("Starting stream processing...");
      // Loop over each event from the server
      for await (const event of resultStream) {
          switch (event.event) {
          case "search_results":
              console.log(`\nSearch results received (Chunks: ${event.data.chunk_search_results?.length || 0}, Graph: ${event.data.graph_search_results?.length || 0}, Web: ${event.data.web_search_results?.length || 0})`);
              break;
          case "message":
              // Access the actual text delta
              if (event.data?.delta?.content?.[0]?.text?.value) {
                process.stdout.write(event.data.delta.content[0].text.value);
              }
              break;
          case "citation":
              // Payload only sent when is_new is true
              if (event.data?.is_new) {
                process.stdout.write(`\n&lt;&lt;&lt; New Citation Source Detected: ID=${event.data.id} &gt;&gt;&gt;`);
                // console.log(`   Payload: ${JSON.stringify(event.data.payload)}`); // Can be verbose
              } else {
                 // Citation already seen, no need to log payload again
              }
              break;
          case "final_answer":
              process.stdout.write("\n\n--- Final Answer ---\n");
              console.log(event.data.generated_answer);
              console.log("\n--- Citations Summary ---");
              event.data.citations?.forEach(cit =&gt; {
                console.log(`  ID: ${cit.id}, Spans: ${JSON.stringify(cit.spans)}`);
                // console.log(`  Payload: ${JSON.stringify(cit.payload)}`); // Can be verbose
              });
              break;
          default:
              console.log("\nUnknown or unhandled event:", event.event);
          }
      }
      console.log("\nStream finished.");
    } else {
      // Handle non-streaming response if necessary (though we requested stream)
      console.log("Received non-streaming response:", resultStream);
    }
    ```
  </tab>
</tabs>

### Customizing RAG

Besides `search_settings`, you can customize RAG generation using `rag_generation_config`.

Example of customizing the model with web search:

<tabs>
  <tab title="Python">
    ```python
    # Requires ANTHROPIC_API_KEY env var if using Anthropic models
    response = client.retrieval.rag(
      query="Who was Aristotle and what are his recent influences?",
      rag_generation_config={
          "model":"anthropic/claude-3-haiku-20240307",
          "stream": False, # Get a single response object
          "temperature": 0.5
      },
      include_web_search=True
    )
    print(response.results.generated_answer)
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    // Requires ANTHROPIC_API_KEY env var if using Anthropic models
    const response = await client.retrieval.rag({
      query: "Who was Aristotle and what are his recent influences?",
      ragGenerationConfig: {
        model: 'anthropic/claude-3-haiku-20240307',
        temperature: 0.5,
        stream: false // Get a single response object
      },
      includeWebSearch: true
    });
    console.log(response.results.generated_answer);
    ```
  </tab>

  <tab title="Curl">
    ```bash
    # Requires ANTHROPIC_API_KEY env var if using Anthropic models
    curl -X POST "https://api.sciphi.ai/v3/retrieval/rag" \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer YOUR_API_KEY" \
        -d '{
            "query": "Who was Aristotle and what are his recent influences?",
            "rag_generation_config": {
                "model": "anthropic/claude-3-haiku-20240307",
                "temperature": 0.5,
                "stream": false
            },
            "include_web_search": true
        }'
    ```
  </tab>
</tabs>

## Conclusion

R2R's search and RAG capabilities provide flexible tools for finding and contextualizing information. Whether you need simple semantic search, advanced hybrid retrieval with filtering, or customizable RAG generation incorporating document chunks, knowledge graph insights, and web results via streaming or single responses, the system can be configured to meet your specific needs.

For more advanced use cases:

* Explore advanced RAG strategies like HyDE and RAG-Fusion in [Advanced RAG](/documentation/advanced-rag).
* Learn about the conversational [Agentic RAG](/documentation/retrieval/agentic-rag) system for multi-turn interactions.
* Dive deeper into specific configuration options in the [API &amp; SDK Reference](/api-and-sdks/retrieval/retrieval).


# Agentic RAG

R2R's **Agentic RAG** orchestrates multi-step reasoning with Retrieval-Augmented Generation (RAG). By pairing large language models with advanced retrieval and tool integrations, the agent can fetch relevant data from the internet, your documents and knowledge graphs, reason over it, and produce robust, context-aware answers.

<note>
  Agentic RAG (also called Deep Research) is an extension of R2R's basic retrieval functionality. If you are new to R2R, we suggest starting with the [Quickstart](/documentation/quickstart) and [Search &amp; RAG](/documentation/search-and-rag) docs first.
</note>

## Key Features

<cardgroup cols="{2}">
  <card title="Multi-Step Reasoning" icon="diagram-project">
    The agent can chain multiple actions, like searching documents or referencing conversation history, before generating its final response.
  </card>

  <card title="Retrieval Augmentation" icon="binoculars">
    Integrates with R2R's vector, full-text, or hybrid search to gather the most relevant context for each query.
  </card>
</cardgroup>

<cardgroup cols="{2}">
  <card title="Conversation Context" icon="comment">
    Maintain dialogue across multiple turns by including <code>conversation\_id</code> in each request.
  </card>

  <card title="Tool Usage" icon="wrench">
    Dynamically invoke tools at runtime to gather and analyze information from various sources.
  </card>
</cardgroup>

## Available Modes

The Agentic RAG system offers two primary operating modes:

### RAG Mode (Default)

Standard retrieval-augmented generation for answering questions based on your knowledge base:

* Semantic and hybrid search capabilities
* Document-level and chunk-level content retrieval
* Optional web search integrations, leveraging Serper and Firecrawl
* Source citation and evidence-based responses

### Research Mode

Advanced capabilities for deep analysis, reasoning, and computation:

* All RAG mode capabilities
* A dedicated reasoning system for complex problem-solving
* Critique capabilities to identify potential biases or logical fallacies
* Python execution for computational analysis
* Multi-step reasoning for deeper exploration of topics

## Available Tools

### RAG Tools

The agent can use the following tools in RAG mode:

| Tool Name                  | Description                                                                       | Dependencies                                                                                    |
| -------------------------- | --------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| `search_file_knowledge`    | Semantic/hybrid search on your ingested documents using R2R's search capabilities | None                                                                                            |
| `search_file_descriptions` | Search over file-level metadata (titles, doc-level descriptions)                  | None                                                                                            |
| `get_file_content`         | Fetch entire documents or chunk structures for deeper analysis                    | None                                                                                            |
| `web_search`               | Query external search APIs for up-to-date information                             | Requires `SERPER_API_KEY` environment variable ([serper.dev](https://serper.dev/))              |
| `web_scrape`               | Scrape and extract content from specific web pages                                | Requires `FIRECRAWL_API_KEY` environment variable ([firecrawl.dev](https://www.firecrawl.dev/)) |

### Research Tools

The agent can use the following tools in Research mode:

| Tool Name         | Description                                                                        | Dependencies |
| ----------------- | ---------------------------------------------------------------------------------- | ------------ |
| `rag`             | Leverage the underlying RAG agent to perform information retrieval and synthesis   | None         |
| `reasoning`       | Call a dedicated model for complex analytical thinking                             | None         |
| `critique`        | Analyze conversation history to identify flaws, biases, and alternative approaches | None         |
| `python_executor` | Execute Python code for complex calculations and analysis                          | None         |

## Basic Usage

Below are examples of how to use the agent for both single-turn queries and multi-turn conversations.

<tabs>
  <tab title="Python">
    ```python
    from r2r import R2RClient
    from r2r import (
        ThinkingEvent,
        ToolCallEvent,
        ToolResultEvent,
        CitationEvent,
        MessageEvent,
        FinalAnswerEvent,
    )

    # when using auth, do client.users.login(...)

    # Basic RAG mode with streaming
    response = client.retrieval.agent(
        message={
            "role": "user",
            "content": "What does DeepSeek R1 imply for the future of AI?"
        },
        rag_generation_config={
            "model": "anthropic/claude-3-7-sonnet-20250219",
            "extended_thinking": True,
            "thinking_budget": 4096,
            "temperature": 1,
            "top_p": None,
            "max_tokens_to_sample": 16000,
            "stream": True
        },
        rag_tools=["search_file_knowledge", "get_file_content"],
        mode="rag"
    )

    # Improved streaming event handling
    current_event_type = None
    for event in response:
        # Check if the event type has changed
        event_type = type(event)
        if event_type != current_event_type:
            current_event_type = event_type
            print() # Add newline before new event type

            # Print emoji based on the new event type
            if isinstance(event, ThinkingEvent):
                print(f"\nüß† Thinking: ", end="", flush=True)
            elif isinstance(event, ToolCallEvent):
                print(f"\nüîß Tool call: ", end="", flush=True)
            elif isinstance(event, ToolResultEvent):
                print(f"\nüìä Tool result: ", end="", flush=True)
            elif isinstance(event, CitationEvent):
                print(f"\nüìë Citation: ", end="", flush=True)
            elif isinstance(event, MessageEvent):
                print(f"\nüí¨ Message: ", end="", flush=True)
            elif isinstance(event, FinalAnswerEvent):
                print(f"\n‚úÖ Final answer: ", end="", flush=True)

        # Print the content without the emoji
        if isinstance(event, ThinkingEvent):
            print(f"{event.data.delta.content[0].payload.value}", end="", flush=True)
        elif isinstance(event, ToolCallEvent):
            print(f"{event.data.name}({event.data.arguments})")
        elif isinstance(event, ToolResultEvent):
            print(f"{event.data.content[:60]}...")
        elif isinstance(event, CitationEvent):
            print(f"{event.data}")
        elif isinstance(event, MessageEvent):
            print(f"{event.data.delta.content[0].payload.value}", end="", flush=True)
        elif isinstance(event, FinalAnswerEvent):
            print(f"{event.data.generated_answer[:100]}...")
            print(f"   Citations: {len(event.data.citations)} sources referenced")
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    const { r2rClient } = require("r2r-js");

    const client = new r2rClient();
    // when using auth, do client.users.login(...)

    async function main() {
        // Basic RAG mode with streaming
        const streamingResponse = await client.retrieval.agent({
            message: {
                role: "user",
                content: "What does DeepSeek R1 imply for the future of AI?"
            },
            ragTools: ["search_file_knowledge", "get_file_content"],
            ragGenerationConfig: {
                model: "anthropic/claude-3-7-sonnet-20250219",
                extendedThinking: true,
                thinkingBudget: 4096,
                temperature: 1,
                maxTokens: 16000,
                stream: true
            }
        });

        // Improved streaming event handling
        if (Symbol.asyncIterator in streamingResponse) {
            let currentEventType = null;

            for await (const event of streamingResponse) {
                // Check if event type has changed
                const eventType = event.event;
                if (eventType !== currentEventType) {
                    currentEventType = eventType;
                    console.log(); // Add newline before new event type

                    // Print emoji based on the new event type
                    switch(eventType) {
                        case "thinking":
                            process.stdout.write(`üß† Thinking: `);
                            break;
                        case "tool_call":
                            process.stdout.write(`üîß Tool call: `);
                            break;
                        case "tool_result":
                            process.stdout.write(`üìä Tool result: `);
                            break;
                        case "citation":
                            process.stdout.write(`üìë Citation: `);
                            break;
                        case "message":
                            process.stdout.write(`üí¨ Message: `);
                            break;
                        case "final_answer":
                            process.stdout.write(`‚úÖ Final answer: `);
                            break;
                    }
                }

                // Print content based on event type
                switch(eventType) {
                    case "thinking":
                        process.stdout.write(`${event.data.delta.content[0].payload.value}`);
                        break;
                    case "tool_call":
                        console.log(`${event.data.name}(${JSON.stringify(event.data.arguments)})`);
                        break;
                    case "tool_result":
                        console.log(`${event.data.content.substring(0, 60)}...`);
                        break;
                    case "citation":
                        console.log(`${event.data}`);
                        break;
                    case "message":
                        process.stdout.write(`${event.data.delta.content[0].payload.value}`);
                        break;
                    case "final_answer":
                        console.log(`${event.data.generated_answer.substring(0, 100)}...`);
                        console.log(`   Citations: ${event.data.citations.length} sources referenced`);
                        break;
                }
            }
        }
    }

    main();
    ```
  </tab>

  <tab title="Curl">
    ```bash
    curl -X POST "https://api.sciphi.ai/v3/retrieval/agent" \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer YOUR_API_KEY" \
      -d '{
        "message": {
            "role": "user",
            "content": "What does DeepSeek R1 imply for the future of AI?"
        },
        "rag_tools": ["search_file_knowledge", "get_file_content"],
        "rag_generation_config": {
            "model": "anthropic/claude-3-7-sonnet-20250219",
            "extended_thinking": true,
            "thinking_budget": 4096,
            "temperature": 1,
            "max_tokens_to_sample": 16000,
            "stream": true
        },
        "mode": "rag"
      }'
    ```
  </tab>
</tabs>

## Using Research Mode

Research mode provides more advanced reasoning capabilities for complex questions:

<tabs>
  <tab title="Python">
    ```python
    # Research mode with all available tools
    response = client.retrieval.agent(
        message={
            "role": "user", 
            "content": "Analyze the philosophical implications of DeepSeek R1 for the future of AI reasoning"
        },
        research_generation_config={
            "model": "anthropic/claude-3-opus-20240229",
            "extended_thinking": True,
            "thinking_budget": 8192,
            "temperature": 0.2,
            "max_tokens_to_sample": 32000,
            "stream": True
        },
        research_tools=["rag", "reasoning", "critique", "python_executor"],
        mode="research"
    )

    # Process streaming events as shown in the previous example
    # ...

    # Research mode with computational focus
    # This example solves a mathematical problem using the python_executor tool
    compute_response = client.retrieval.agent(
        message={
            "role": "user", 
            "content": "Calculate the factorial of 15 multiplied by 32. Show your work."
        },
        research_generation_config={
            "model": "anthropic/claude-3-opus-20240229",
            "max_tokens_to_sample": 1000,
            "stream": False
        },
        research_tools=["python_executor"],
        mode="research"
    )

    print(f"Final answer: {compute_response.results.messages[-1].content}")
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    // Research mode with all available tools
    const researchStream = await client.retrieval.agent({
        message: {
            role: "user", 
            content: "Analyze the philosophical implications of DeepSeek R1 for the future of AI reasoning"
        },
        researchGenerationConfig: {
            model: "anthropic/claude-3-opus-20240229",
            extendedThinking: true,
            thinkingBudget: 8192,
            temperature: 0.2,
            maxTokens: 32000,
            stream: true
        },
        researchTools: ["rag", "reasoning", "critique", "python_executor"],
        mode: "research"
    });

    // Process streaming events as shown in the previous example
    // ...

    // Research mode with computational focus
    const computeResponse = await client.retrieval.agent({
        message: {
            role: "user", 
            content: "Calculate the factorial of 15 multiplied by 32. Show your work."
        },
        researchGenerationConfig: {
            model: "anthropic/claude-3-opus-20240229",
            maxTokens: 1000,
            stream: false
        },
        researchTools: ["python_executor"],
        mode: "research"
    });

    console.log(`Final answer: ${computeResponse.results.messages[computeResponse.results.messages.length - 1].content}`);
    ```
  </tab>
</tabs>

## Customizing the Agent

### Tool Selection

You can customize which tools the agent has access to:

```python
# RAG mode with web capabilities
response = client.retrieval.agent(
    message={"role": "user", "content": "What are the latest developments in AI safety?"},
    rag_tools=["search_file_knowledge", "get_file_content", "web_search", "web_scrape"],
    mode="rag"
)

# Research mode with limited tools
response = client.retrieval.agent(
    message={"role": "user", "content": "Analyze the complexity of this algorithm"},
    research_tools=["reasoning", "python_executor"],  # Only reasoning and code execution
    mode="research"
)
```

### Search Settings Propagation

Any search settings passed to the agent will propagate to downstream searches. This includes:

* Filters to restrict document sources
* Limits on the number of results
* Hybrid search configuration
* Collection restrictions

```python
# Using search settings with the agent
response = client.retrieval.agent(
    message={"role": "user", "content": "Summarize our Q1 financial results"},
    search_settings={
        "use_semantic_search": True,
        "filters": {"collection_ids": {"$overlap": ["e43864f5-..."]}},
        "limit": 25
    },
    rag_tools=["search_file_knowledge", "get_file_content"],
    mode="rag"
)
```

### Model Selection and Parameters

You can customize the agent's behavior by selecting different models and adjusting generation parameters:

```python
# Using a specific model with custom parameters
response = client.retrieval.agent(
    message={"role": "user", "content": "Write a concise summary of DeepSeek R1's capabilities"},
    rag_generation_config={
        "model": "anthropic/claude-3-haiku-20240307",  # Faster model for simpler tasks
        "temperature": 0.3,                           # Lower temperature for more deterministic output
        "max_tokens_to_sample": 500,                  # Limit response length
        "stream": False                               # Non-streaming for simpler use cases
    },
    mode="rag"
)
```

## Multi-Turn Conversations

You can maintain context across multiple turns using `conversation_id`. The agent will remember previous interactions and build upon them in subsequent responses.

<tabs>
  <tab title="Python">
    ```python
    # Create a new conversation
    conversation = client.conversations.create()
    conversation_id = conversation.results.id

    # First turn
    first_response = client.retrieval.agent(
        message={"role": "user", "content": "What does DeepSeek R1 imply for the future of AI?"},
        rag_generation_config={
            "model": "anthropic/claude-3-7-sonnet-20250219",
            "temperature": 0.7,
            "max_tokens_to_sample": 1000,
            "stream": False
        },
        conversation_id=conversation_id,
        mode="rag"
    )
    print(f"First response: {first_response.results.messages[-1].content[:100]}...")

    # Follow-up query in the same conversation
    follow_up_response = client.retrieval.agent(
        message={"role": "user", "content": "How does it compare to other reasoning models?"},
        rag_generation_config={
            "model": "anthropic/claude-3-7-sonnet-20250219",
            "temperature": 0.7,
            "max_tokens_to_sample": 1000,
            "stream": False
        },
        conversation_id=conversation_id,
        mode="rag"
    )
    print(f"Follow-up response: {follow_up_response.results.messages[-1].content[:100]}...")

    # The agent maintains context, so it knows "it" refers to DeepSeek R1
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    // Create a new conversation
    const conversation = await client.conversations.create();
    const conversationId = conversation.results.id;

    // First turn
    const firstResponse = await client.retrieval.agent({
        message: {
            role: "user", 
            content: "What does DeepSeek R1 imply for the future of AI?"
        },
        ragGenerationConfig: {
            model: "anthropic/claude-3-7-sonnet-20250219",
            temperature: 0.7,
            maxTokens: 1000,
            stream: false
        },
        conversationId: conversationId,
        mode: "rag"
    });
    console.log(`First response: ${firstResponse.results.messages[firstResponse.results.messages.length - 1].content.substring(0, 100)}...`);

    // Follow-up query in the same conversation
    const followUpResponse = await client.retrieval.agent({
        message: {
            role: "user", 
            content: "How does it compare to other reasoning models?"
        },
        ragGenerationConfig: {
            model: "anthropic/claude-3-7-sonnet-20250219",
            temperature: 0.7,
            maxTokens: 1000,
            stream: false
        },
        conversationId: conversationId,
        mode: "rag"
    });
    console.log(`Follow-up response: ${followUpResponse.results.messages[followUpResponse.results.messages.length - 1].content.substring(0, 100)}...`);

    // The agent maintains context, so it knows "it" refers to DeepSeek R1
    ```
  </tab>
</tabs>

## Performance Considerations

Based on our integration testing, here are some considerations to optimize your agent usage:

### Response Time Management

Response times vary based on the complexity of the query, the number of tools used, and the length of the requested output:

```python
# For time-sensitive applications, consider:
# 1. Using a smaller max_tokens value
# 2. Selecting faster models like claude-3-haiku
# 3. Avoiding unnecessary tools

fast_response = client.retrieval.agent(
    message={"role": "user", "content": "Give me a quick overview of DeepSeek R1"},
    rag_generation_config={
        "model": "anthropic/claude-3-haiku-20240307",  # Faster model
        "max_tokens_to_sample": 200,                   # Limited output
        "stream": True                                 # Stream for perceived responsiveness
    },
    rag_tools=["search_file_knowledge"],              # Minimal tools
    mode="rag"
)
```

### Handling Large Context

The agent can process large document contexts efficiently, but performance can be improved by using appropriate filters:

```python
# When working with large document collections, use filters to narrow results
filtered_response = client.retrieval.agent(
    message={"role": "user", "content": "Summarize key points from our AI ethics documentation"},
    search_settings={
        "filters": {
            "$and": [
                {"document_type": {"$eq": "pdf"}},
                {"metadata.category": {"$eq": "ethics"}},
                {"metadata.year": {"$gt": 2023}}
            ]
        },
        "limit": 10  # Limit number of chunks returned
    },
    rag_generation_config={
        "max_tokens_to_sample": 500,
        "stream": True
    },
    mode="rag"
)
```

## How Tools Work (Under the Hood)

R2R's Agentic RAG leverages a powerful toolset to conduct comprehensive research:

### RAG Mode Tools

* **search\_file\_knowledge**: Looks up relevant text chunks and knowledge graph data from your ingested documents using semantic and hybrid search capabilities.
* **search\_file\_descriptions**: Searches over file-level metadata (titles, doc-level descriptions) rather than chunk content.
* **get\_file\_content**: Fetches entire documents or their chunk structures for deeper analysis when the agent needs more comprehensive context.
* **web\_search**: Queries external search APIs (like Serper or Google) for live, up-to-date information from the internet. Requires a `SERPER_API_KEY` environment variable.
* **web\_scrape**: Uses Firecrawl to extract content from specific web pages for in-depth analysis. Requires a `FIRECRAWL_API_KEY` environment variable.

### Research Mode Tools

* **rag**: A specialized research tool that utilizes the underlying RAG agent to perform comprehensive information retrieval and synthesis across your data sources.
* **python\_executor**: Executes Python code for complex calculations, statistical operations, and algorithmic implementations, giving the agent computational capabilities.
* **reasoning**: Allows the research agent to call a dedicated model as an external module for complex analytical thinking.
* **critique**: Analyzes conversation history to identify potential flaws, biases, and alternative approaches to improve research rigor.

The Agent is built on a sophisticated architecture that combines these tools with streaming capabilities and flexible response formats. It can decide which tools to use based on the query requirements and can dynamically invoke them during the research process.

## Conclusion

Agentic RAG provides a powerful approach to retrieval-augmented generation. By combining **advanced search**, **multi-step reasoning**, **conversation context**, and **dynamic tool usage**, the agent helps you build sophisticated Q\&amp;A or research solutions on your R2R-ingested data.

**Next Steps**

* **Ingest** your content using [Documents](/documentation/documents).
* Explore advanced retrieval in [Hybrid Search](/documentation/hybrid-search).
* Enhance understanding with [Knowledge Graphs](/documentation/graphs).
* Manage multi-turn chat with [Conversations](/documentation/conversations).
* Scale up your solution with the [API &amp; SDKs](/api-and-sdks).


# Hybrid Search

&gt; Combines chunk and full text search in one query for more relevant results

## Introduction

R2R's hybrid search blends keyword-based full-text search with semantic vector search, delivering results that are both contextually relevant and precise. By unifying these approaches, hybrid search excels at handling complex queries where both exact terms and overall meaning matter.

## How R2R Hybrid Search Works

<steps>
  ### Full-Text Search

  Leverages Postgres's `ts_rank_cd` and `websearch_to_tsquery` to find documents containing your keywords.

  ### Semantic Search

  Uses vector embeddings to locate documents contextually related to your query, even if they don't share exact keywords.

  ### Reciprocal Rank Fusion (RRF)

  Merges results from both full-text and semantic searches using a formula like:

  $\text{COALESCE}\left(\frac{1.0}{\text{rrf\_k} + \text{full\_text.rank\_ix}}, 0.0\right) \cdot \text{full\_text\_weight} + \text{COALESCE}\left(\frac{1.0}{\text{rrf\_k} + \text{semantic.rank\_ix}}, 0.0\right) \cdot \text{semantic\_weight}$

  This ensures that documents relevant both semantically and by keyword ranking float to the top.

  ### Result Ranking

  Orders the final set of results based on the combined RRF score, providing balanced, meaningful search outcomes.
</steps>

## Key Features

<tabs>
  <tab title="Full-Text Search">
    * Uses Postgres indexing and querying for quick, exact term matches.
    * Great for retrieving documents where specific terminology is critical.
  </tab>

  <tab title="Semantic Search">
    * Embeds queries and documents into vector representations.
    * Finds documents related to the query's meaning, not just its wording.
  </tab>

  <tab title="Hybrid Integration">
    * By enabling both `use_fulltext_search` and `use_semantic_search`, or choosing the `advanced` mode, you get the best of both worlds.
    * RRF blends these results, ensuring that documents align with the query's intent and exact terms where needed.
  </tab>
</tabs>

## Understanding Search Modes

R2R supports multiple search modes that can simplify or customize the configuration for you:

* **`basic`**: Primarily semantic search. Suitable for straightforward scenarios where semantic understanding is key, but you don't need the additional context of keyword matching.
* **`advanced`**: Combines semantic and full-text search by default, effectively enabling hybrid search with well-tuned default parameters. Ideal if you want the benefits of hybrid search without manual configuration.
* **`custom`**: Allows you full control over the search settings, including toggling semantic and full-text search independently. Choose this if you want to fine-tune weights, limits, and other search behaviors.

When using `advanced` mode, R2R automatically configures hybrid search for you. For `custom` mode, you can directly set `use_hybrid_search=True` or enable both `use_semantic_search` and `use_fulltext_search` to achieve a hybrid search setup.

## Configuration

**Choosing a Search Mode:**

* `basic`: Semantic-only.
  ```python
  search_mode = "basic"
  # Semantic search only, no full-text matching
  ```

* `advanced`: Hybrid by default.
  ```python
  search_mode = "advanced"
  # Hybrid search is automatically enabled with well-tuned defaults
  ```

* `custom`: Manually configure hybrid search.
  ```python
  search_mode = "custom"
  # Enable both semantic and full-text search and set weights as needed:
  search_settings = {
    "use_semantic_search": True,
    "use_fulltext_search": True,
    "use_hybrid_search": True,
    "hybrid_settings": {
      "full_text_weight": 1.0,
      "semantic_weight": 5.0,
      "full_text_limit": 200,
      "rrf_k": 50
    }
  }
  ```

For more details on runtime configuration and combining `search_mode` with custom `search_settings`, [refer to the Search API documentation](/api-and-sdks/retrieval/search-app).

## Best Practices

1. **Optimize Database and Embeddings**:\
   Ensure Postgres indexing and vector store configurations are optimal for performance.

2. **Adjust Weights and Limits**:\
   Tweak `full_text_weight`, `semantic_weight`, and `rrf_k` values when using `custom` mode. If you're using `advanced` mode, the defaults are already tuned for general use cases.

3. **Regular Updates**:\
   Keep embeddings and indexes up-to-date to maintain search quality.

4. **Choose Appropriate Embeddings**:\
   Select an embedding model that fits your content domain for the best semantic results.

## Conclusion

R2R's hybrid search delivers robust, context-aware retrieval by merging semantic and keyword-driven approaches. Whether you pick `basic` mode for simplicity, `advanced` mode for out-of-the-box hybrid search, or `custom` mode for granular control, R2R ensures you can tailor the search experience to your unique needs.


# Advanced RAG

&gt; Learn how to build and use advanced RAG techniques with R2R

R2R supports advanced Retrieval-Augmented Generation (RAG) techniques that can be easily configured at runtime. This flexibility allows you to experiment with different state of the art strategies and optimize retrieval for specific use cases. **This cookbook will cover toggling between vanilla RAG, [HyDE](https://arxiv.org/abs/2212.10496) and [RAG-Fusion](https://arxiv.org/abs/2402.03367).**.

<note>
  Advanced RAG techniques are still a beta feature in R2R. They are not currently supported in agentic workflows and there may be limitations in observability and analytics when implementing them.

  Are we missing an important RAG technique? If so, then please let us know at [founders@sciphi.ai](mailto:founders@sciphi.ai).
</note>

## Supported Advanced RAG Techniques

R2R currently supports two advanced RAG techniques:

1. **HyDE (Hypothetical Document Embeddings)**: Enhances retrieval by generating and embedding hypothetical documents based on the query.
2. **RAG-Fusion**: Improves retrieval quality by combining results from multiple search iterations.

## Using Advanced RAG Techniques

You can specify which advanced RAG technique to use by setting the `search_strategy` parameter in your vector search settings. Below is a comprehensive overview of techniques supported by R2R.

### HyDE

#### What is HyDE?

HyDE is an innovative approach that supercharges dense retrieval, especially in zero-shot scenarios. Here's how it works:

1. **Query Expansion**: HyDE uses a Language Model to generate hypothetical answers or documents based on the user's query.
2. **Enhanced Embedding**: These hypothetical documents are embedded, creating a richer semantic search space.
3. **Similarity Search**: The embeddings are used to find the most relevant actual documents in your database.
4. **Informed Generation**: The retrieved documents and original query are used to generate the final response.

#### Implementation Diagram

The diagram which follows below illustrates the HyDE flow which fits neatly into the schema of our diagram above (note, the GraphRAG workflow is omitted for brevity):

```mermaid

graph TD
    A[User Query] --&gt; B[QueryTransformPipe]
    B --&gt;|Generate Hypothetical Documents| C[MultiSearchPipe]
    C --&gt; D[VectorSearchPipe]
    D --&gt; E[RAG Generation]
    A --&gt; E
    F[Document DB] --&gt; D

    subgraph HyDE Process
    B --&gt; G[Hypothetical Doc 1]
    B --&gt; H[Hypothetical Doc 2]
    B --&gt; I[Hypothetical Doc n]
    G --&gt; J[Embed]
    H --&gt; J
    I --&gt; J
    J --&gt; C
    end

    subgraph Vector Search
    D --&gt; K[Similarity Search]
    K --&gt; L[Rank Results]
    L --&gt; E
    end

    C --&gt; |Multiple Searches| D
    K --&gt; |Retrieved Documents| L
```

#### Using HyDE in R2R

```python
client.retrieval.rag(
    "What are the main themes in the DeepSeek paper?",
    search_settings={
        "search_strategy": "hyde",
        "limit": 10
    }
)
```

```plaintext
RAGResponse(
    generated_answer='DeepSeek-R1 is a model that demonstrates impressive performance across various tasks, leveraging reinforcement learning (RL) and supervised fine-tuning (SFT) to enhance its capabilities. It excels in writing tasks, open-domain question answering, and benchmarks like IF-Eval, AlpacaEval2.0, and ArenaHard [1], [2]. DeepSeek-R1 outperforms its predecessor, DeepSeek-V3, in several areas, showcasing its strengths in reasoning and generalization across diverse domains [1]. It also achieves competitive results on factual benchmarks like SimpleQA, although it performs worse on the Chinese SimpleQA benchmark due to safety RL constraints [2]. Additionally, DeepSeek-R1 is involved in distillation processes to transfer its reasoning capabilities to smaller models, which perform exceptionally well on benchmarks [4], [6]. The model is optimized for English and Chinese, with plans to address language mixing issues in future updates [8].', 
    search_results=AggregateSearchResult(
      chunk_search_results=[ChunkSearchResult(score=0.643, text=Document Title: DeepSeek_R1.pdf ...)]
    ),
    citations=[Citation(index=1, rawIndex=1, startIndex=305, endIndex=308, snippetStartIndex=288, snippetEndIndex=315, sourceType='chunk', id='e760bb76-1c6e-52eb-910d-0ce5b567011b', document_id='e43864f5-a36f-548e-aacd-6f8d48b30c7f', owner_id='2acb499e-8428-543b-bd85-0d9098718220', collection_ids=['122fdf6a-e116-546b-a8f6-e4cb2e2c0a09'], score=0.6433466439465674, text='Document Title: DeepSeek_R1.pdf\n\nText: could achieve an accuracy of over 70%.\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\nmodels ability to follow format instructions. These improvements can be linked to the inclusion\nof instruction-following...]
    metadata={'id': 'chatcmpl-B0BaZ0vwIa58deI0k8NIuH6pBhngw', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}}], 'created': 1739384247, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion', 'service_tier': 'default', 'system_fingerprint': 'fp_4691090a87', ...}
)
```

### RAG-Fusion

#### What is RAG-Fusion?

RAG-Fusion is an advanced technique that combines Retrieval-Augmented Generation (RAG) with Reciprocal Rank Fusion (RRF) to improve the quality and relevance of retrieved information. Here's how it works:

1. **Query Expansion**: The original query is used to generate multiple related queries, providing different perspectives on the user's question.
2. **Multiple Retrievals**: Each generated query is used to retrieve relevant documents from the database.
3. **Reciprocal Rank Fusion**: The retrieved documents are re-ranked using the RRF algorithm, which combines the rankings from multiple retrieval attempts.
4. **Enhanced RAG**: The re-ranked documents, along with the original and generated queries, are used to generate the final response.

This approach helps to capture a broader context and potentially more relevant information compared to traditional RAG.

#### Implementation Diagram

Here's a diagram illustrating the RAG-Fusion workflow (again, we omit the graph process for brevity):

```mermaid
graph TD
    A[User Query] --&gt; B[QueryTransformPipe]
    B --&gt;|Generate Multiple Queries| C[MultiSearchPipe]
    C --&gt; D[VectorSearchPipe]
    D --&gt; E[RRF Reranking]
    E --&gt; F[RAG Generation]
    A --&gt; F
    G[Document DB] --&gt; D

    subgraph RAG-Fusion Process
    B --&gt; H[Generated Query 1]
    B --&gt; I[Generated Query 2]
    B --&gt; J[Generated Query n]
    H --&gt; C
    I --&gt; C
    J --&gt; C
    end

    subgraph Vector Search
    D --&gt; K[Search Results 1]
    D --&gt; L[Search Results 2]
    D --&gt; M[Search Results n]
    K --&gt; E
    L --&gt; E
    M --&gt; E
    end

    E --&gt; |Re-ranked Documents| F
```

#### Using RAG-Fusion in R2R

```python
rag_fusion_response = client.retrieval.rag(
    "What are the main themes in DeepSeeks paper?",
    search_settings={
        "search_strategy": "rag_fusion",
        "limit": 20
    }
)

```

### Combining with Other Settings

You can readily combine these advanced techniques with other search and RAG settings:

```python
custom_rag_response = client.retrieval.rag(
    "What are the main themes in the DeepSeek paper?",
    search_settings={
        "search_strategy": "hyde",
        "limit": 15,
        "use_hybrid_search": True
    },
    rag_generation_config={
        "model": "anthropic/claude-3-opus-20240229",
        "temperature": 0.7
    }
)
```

## Conclusion

By leveraging these advanced RAG techniques and customizing their underlying prompts, you can significantly enhance the quality and relevance of your retrieval and generation processes. Experiment with different strategies, settings, and prompt variations to find the optimal configuration for your specific use case. The flexibility of R2R allows you to iteratively improve your system's performance and adapt to changing requirements.


# Deduplication

&gt; Building and managing knowledge graphs through collections

In many cases, the chunks that go into a document contain duplicate elements. This can create significant noise within a graph, and produce less-than-optimal search results. One way to reconcile this is through entity deduplication, which condenses duplicate elements into a single, high quality element.

## Overview

Entity deduplication is the process of identifying and merging duplicate entities within a knowledge graph. R2R currently supports document-level deduplication, with graph-level deduplication planned for future releases.

### Document-Level Deduplication

Document-level deduplication focuses on consolidating duplicate entities within a single document. This process:

1. Identifies duplicate entities using configurable matching techniques
2. Merges matched entities into a single high-quality entity
3. Regenerates entity descriptions and embeddings using LLM
4. Updates related relationships to point to the merged entity

Following the process of creating a graph outlined in our [graph cookbook](/cookbooks/graphs), we can ingest a document. This process produces a number of entities and relationships, however, we see many duplicates!

When extracting elements from *The Gift of the Magi* by O. Henry, we find that there 129 total entities, however only 20 of the entities are unique.

<accordion icon="gear" title="Extracted Entities Before Deduplication">
  | Entity Name                | Count |
  | -------------------------- | ----- |
  | Magi                       | 15    |
  | Della                      | 15    |
  | Jim                        | 15    |
  | Platinum Fob Chain         | 15    |
  | Combs                      | 15    |
  | O. Henry                   | 11    |
  | The Gift of the Magi       | 10    |
  | Christmas                  | 8     |
  | Watch                      | 8     |
  | Christmas Eve              | 7     |
  | Christ Child               | 1     |
  | Gold Watch                 | 1     |
  | Mr. James Dillingham Young | 1     |
  | Shabby Little Couch        | 1     |
  | New York City              | 1     |
  | Flat                       | 1     |
  | Furnished Flat             | 1     |
  | Dillingham Young           | 1     |
  | Hair                       | 1     |
  | 1.87 Dollars               | 1     |
</accordion>

<tabs>
  <tab title="Python">
    ```python
    from r2r import R2RClient

    # Set up the client
    client = R2RClient("http://localhost:7272")

    client.documents.deduplicate("20e29a97-c53c-506d-b89c-1f5346befc58")
    ```
  </tab>
</tabs>

After running the deduplication process, we are left with 20 entities. Those that were duplicates have been merged, and their description has been updated to ensure that no description context is lost through the merging process.

### Deduplication Techniques

R2R supports (or plans to support) several deduplication techniques, each with its own advantages:

| Technique                  | Description                                                                         | Status    | Best For                                       |
| -------------------------- | ----------------------------------------------------------------------------------- | --------- | ---------------------------------------------- |
| Exact Name Matching        | Identifies duplicates based on exact string matches of entity names                 | Available | Clear duplicates with identical names          |
| N-Character Block Matching | Matches entities based on character block similarity, allowing for minor variations | Planned   | Names with slight variations or typos          |
| Semantic Similarity        | Uses embedding similarity to identify conceptually similar entities                 | Planned   | Entities with different names but same meaning |
| Fuzzy Name Matching        | Employs Levenshtein distance to catch minor spelling variations                     | Planned   | Handling typos and minor name variations       |

### Merging Strategy

When duplicates are identified, R2R employs a sophisticated merging strategy:

1. **Name Retention**: Keeps the most common form of the entity name
2. **Description Consolidation**: Combines descriptions from all duplicates and uses LLM to generate a comprehensive, non-redundant description
3. **Category Resolution**: Preserves the most specific category if categories differ
4. **Metadata Merging**: Combines metadata from all duplicates, resolving conflicts through configurable rules
5. **Relationship Redirection**: Updates all relationships to point to the merged entity

## Future Developments

### Runtime Configurable Techniques

Runtime configurable deduplication techniques will allow for more advanced strategies. This includes n-character block matching, semantic similarity matching, and fuzzy name matching.

### Graph-Level Deduplication

A major feature planned for R2R's deduplication capabilities is graph-level deduplication. This will:

* Identify and merge duplicates across multiple documents within a graph
* Maintain provenance information for merged entities
* Provide configurable merging rules at the graph level
* Support cross-document relationship consolidation

<warning>
  Entity deduplication is a critical step in maintaining graph quality. While automatic deduplication is powerful, it's recommended to review results, especially in domains where entity disambiguation is crucial.
</warning>


# Contextual Enrichment

&gt; Enhancing chunk quality through contextual understanding

<warning>
  Contextual enrichment is currently restricted to:

  * Self-hosted instances
  * Enterprise tier cloud accounts

  Contact our sales team for Enterprise pricing and features.
</warning>

When processing documents into chunks, individual segments can sometimes lack necessary context from surrounding content. Chunk enrichment addresses this by incorporating contextual information from neighboring chunks to create more meaningful and comprehensive text segments.

## Overview

Chunk enrichment is the process of enhancing individual document chunks by considering their surrounding context.

### How Enrichment Works

The enrichment process runs after initial document chunking and:

* Retrieves a configurable number of preceding and succeeding chunks
* Sends the chunks, along with document summary if available, to an LLM
* Generates an enriched version that maintains the original meaning while incorporating relevant context
* Creates new embeddings for the enriched chunks
* Replaces the original chunks in the vector database

### Example Enrichment

Consider this example from a technical document about spacecraft:

<accordion icon="code" title="Chunk Enrichment Example">
  | Stage            | Content                                                                                                                                                                                                                                      |
  | ---------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | Original Chunk   | "The heat shield underwent significant stress during this phase, reaching temperatures of 1500¬∞C."                                                                                                                                           |
  | Preceding Chunk  | "As the spacecraft began its descent through the Martian atmosphere, the entry sequence was initiated."                                                                                                                                      |
  | Succeeding Chunk | "These extreme temperatures were within expected parameters, thanks to the carbon-based ablative material."                                                                                                                                  |
  | Enriched Result  | "During the spacecraft's descent through the Martian atmosphere, the heat shield underwent significant stress during the entry phase, reaching temperatures of 1500¬∞C. These temperatures were successfully managed by the shield's design." |
</accordion>

The enriched version incorporates crucial context about the Martian descent while maintaining the core information about temperature and stress levels. This improved chunk will likely perform better in searches related to Mars missions, atmospheric entry, or heat shield performance.

### Configuration Settings

Chunk enrichment can be enabled through a custom configuration file. To learn more about managing your R2R configuration settings, read our [self hosting documentation](/self-hosting/configuration/overview).

```toml my_r2r.toml
[ingestion]
   [ingestion.chunk_enrichment_settings]
    enable_chunk_enrichment = true
    n_chunks = 2 # number of preceding/succeeding chunks to use
```

<error>
  Chunk enrichment can modify the original text content. While this generally improves search quality, it's crucial to note that this process mutates the underlying chunks.
</error>

### Enrichment Process Details

The enrichment process handles chunks in batches for efficiency:

1. **Context Collection**: Gathers preceding and succeeding chunks based on `n_chunks` setting
2. **LLM Enhancement**: Processes chunks through the configured LLM to incorporate context
3. **Fallback Handling**: Maintains original chunk text if enrichment fails
4. **Batch Processing**: Processes chunks in groups of 128 for optimal performance
5. **Vector Updates**: Replaces original chunks with enriched versions in the vector database


# Cloud Limits

&gt; Default Quotas, Rate Limits, &amp; File Size Constraints

SciPhi Cloud imposes limits to govern the maximum number of documents, chunks, and collections per user, as well as rate limits for requests and file-upload sizes. The same defaults apply when you deploy R2R locally (though you can override them in your local `r2r.toml`).

***

## Overview of Limits

### Default Limits

Below are the **default** per-user limits for **all** R2R Cloud deployments:

| Resource / Action              | Default Limit            | Notes                                                       |
| ------------------------------ | ------------------------ | ----------------------------------------------------------- |
| **Documents**                  | 100                      | Maximum number of documents you can upload                  |
| **Chunks**                     | 10,000                   | Maximum total chunks derived from uploaded files            |
| **Collections**                | 5                        | Maximum number of collections per user                      |
| **File Upload** (global)       | 2 MB (2,000,000 bytes)   | Overall max file upload size if no extension-specific limit |
| **File Upload** (by extension) | See table below          | Certain file types can have their own max size              |
| **Rate Limit** (global)        | 60 requests/min per user | -                                                           |

**Note**: Additionally, our Nginx ingress layer applies a **60 requests/minute per IP** limit.\
Thus, a single IP cannot exceed 60 requests/minute, regardless of user account.

***

### File Upload Size by Extension

R2R Cloud enforces specific maximum file sizes for different file types. If no extension-specific limit is found, we fall back to the global `2 MB` limit.

| Extension                    | Max Size (bytes) | Approx. MB | Notes                                |
| ---------------------------- | ---------------- | ---------- | ------------------------------------ |
| **txt**                      | 2,000,000        | \~2.0      | Plain text files                     |
| **md**                       | 2,000,000        | \~2.0      | Markdown                             |
| **csv**                      | 5,000,000        | \~5.0      | CSV spreadsheets                     |
| **doc**                      | 10,000,000       | \~10.0     | MS Word (legacy)                     |
| **docx**                     | 10,000,000       | \~10.0     | MS Word (modern)                     |
| **ppt**                      | 20,000,000       | \~20.0     | MS PowerPoint (legacy)               |
| **pptx**                     | 20,000,000       | \~20.0     | MS PowerPoint (modern)               |
| **xls**                      | 10,000,000       | \~10.0     | MS Excel (legacy)                    |
| **xlsx**                     | 10,000,000       | \~10.0     | MS Excel (modern)                    |
| **pdf**                      | 30,000,000       | \~30.0     | PDF can expand significantly in text |
| **epub**                     | 10,000,000       | \~10.0     | E-book format                        |
| **jpeg** / **jpg** / **png** | 5,000,000        | \~5.0      | Images                               |

&gt; You can customize or override these limits in your own `r2r.toml` if deploying locally.

***

### Route-Specific Rate Limits

In the `cloud.toml` example, certain endpoints have additional per-route limits. For instance:

| Route                       | Requests/Min | Monthly Limit | Notes                          |
| --------------------------- | ------------ | ------------- | ------------------------------ |
| `/v3/retrieval/search`      | 10           | 3,000         | Searching your knowledge base  |
| `/v3/retrieval/rag`         | 5            | 200           | Retrieval-Augmented Generation |
| `/v3/documents/create`      | 10           | 200           | Document ingestion             |
| `/v3/retrieval/agentic-rag` | 5            | 100           | Interactive agent calls        |
| `/v3/retrieval/completions` | 0            | 0             | Disabled in this sample        |
| `/v3/retrieval/embeddings`  | 0            | 0             | Disabled in this sample        |

These **per-route** limits are enforced in addition to the **global** 60 req/min user limit.\
*If a per-route limit is lower than the global limit, that route‚Äôs usage will be throttled sooner.*

***

## Starter Tier Overrides

When upgrading to the **Starter Tier**, these default limits increase:

| Resource / Action | New Limit |
| ----------------- | --------- |
| **Documents**     | 1,000     |
| **Chunks**        | 100,000   |
| **Collections**   | 50        |

All other limits (like file size and request rates) remain the same unless otherwise specified by your plan.

***

## Local Deployment

If you deploy R2R on your own infrastructure, **the same default limits** apply out of the box. However, you can easily override them in your local `r2r.toml` or equivalent configuration. For example:

```toml
[app]
default_max_documents_per_user = 200
default_max_chunks_per_user = 50_000
default_max_collections_per_user = 20
default_max_upload_size = 10_000_000  # 10 MB
```

And for route-based limits:

```toml
[database.route_limits]
"/v3/retrieval/search" = { route_per_min = 50, monthly_limit = 10_000 }
```

***

## Additional Notes

* **User-Level Overrides**: Admins can grant custom overrides to specific users. For example, a single user‚Äôs ‚Äúmax\_documents‚Äù might be raised to 5,000.
* **Monthly Limits**: Resets on the **1st** of each month.
* **Request Logging**: R2R logs usage for each route to track compliance with monthly or per-minute limits.
* **Nginx Rate Limit**: Cloud deployments also apply a **60 requests/minute limit per IP** at the ingress layer.

***

## Conclusion

R2R Cloud‚Äôs default usage limits keep the platform performant and equitable. When you need more capacity‚Äîsuch as uploading larger documents or making more requests‚Äîupgrading your tier or adjusting local `r2r.toml` (for self-hosted deployments) is straightforward.

By keeping usage within your plan‚Äôs limits and splitting large files where necessary, you‚Äôll ensure a smooth experience with R2R‚Äôs ingestion, retrieval, and advanced AI features.


# R2R API &amp; SDKs

&gt; Powerful document ingestion, search, and RAG capabilities at your fingertips

## Welcome to the R2R API &amp; SDK Reference

R2R is a powerful library that offers both methods and a REST API for document ingestion, Retrieval-Augmented Generation (RAG), evaluation, and additional features like observability, analytics, and document management. This API documentation will guide you through the various endpoints and functionalities R2R provides.

<note>
  This API documentation is designed to help developers integrate R2R's capabilities into their applications efficiently. Whether you're building a search engine, a question-answering system, or a document management solution, the R2R API has you covered.
</note>

## Key Features

R2R API offers a wide range of features, including:

* Document Ingestion and Management
* AI-Powered Search (Vector, Hybrid, and Knowledge Graph)
* Retrieval-Augmented Generation (RAG)
* User Auth &amp; Management
* Observability and Analytics

<card title="R2R GitHub Repository" icon="fa-brands fa-github" href="https://github.com/SciPhi-AI/R2R">
  View the R2R source code and contribute
</card>

## Getting Started

To get started with the R2R API, you'll need to:

1. Install R2R in your environment
2. Run the server with `python -m r2r.serve`, or customize your FastAPI for production settings.

For detailed installation and setup instructions, please refer to our [Installation Guide](/self-hosting/installation/overview).


# Retrieval

R2R's Retrieval system provides advanced search and generation capabilities powered by vector search, knowledge graphs, and large language models. The system offers multiple ways to interact with your data:

* Direct semantic search across documents and chunks
* Retrieval-Augmented Generation (RAG) for AI-powered answers
* Conversational RAG agents for complex queries
* Raw LLM completions for flexible text generation

## Core Features

### Vector Search

* Semantic similarity matching using document/chunk embeddings
* Hybrid search combining vector and keyword approaches
* Complex filtering with Postgres-style operators
* Configurable search limits and thresholds

### Knowledge Graph Search

* Entity and relationship-based retrieval
* Multi-hop traversal for connected information
* Local and global search strategies
* Community-aware knowledge structure

### RAG Generation

* Context-aware responses using retrieved content
* Customizable generation parameters
* Source attribution and citations
* Streaming support for real-time responses
* Web search integration for up-to-date information

### Deep Research Agent

* Multi-turn conversational capabilities
* Complex query decomposition
* Context maintenance across interactions
* Branch management for conversation trees
* Web search integration for external knowledge

## Available Endpoints

| Method | Endpoint                | Description                                                                                    |
| ------ | ----------------------- | ---------------------------------------------------------------------------------------------- |
| POST   | `/retrieval/search`     | Perform semantic search with hybrid vector and knowledge graph capabilities                    |
| POST   | `/retrieval/rag`        | Generate contextual responses using retrieved information with optional web search integration |
| POST   | `/retrieval/agent`      | Engage with a RAG-powered conversational agent with web search capabilities                    |
| POST   | `/retrieval/completion` | Generate text completions using language models                                                |
| POST   | `/retrieval/embedding`  | Generate embeddings for documents or text                                                      |

## Search Settings

### Vector Search Settings

```json
{
  "use_semantic_search": true,
  "filters": {"document_id": {"$eq": "3e157b3a-8469-51db-90d9-52e7d896b49b"}},
  "limit": 20,
  "use_hybrid_search": true
}
```

### Generation Configuration

```json
{
  "stream": false,
  "temperature": 0.7,
  "max_tokens": 150,
  "model": "gpt-4o-mini"
}
```

## Key Concepts

### Search

The search endpoint provides direct access to R2R's retrieval capabilities, allowing you to find relevant content using semantic similarity and knowledge graph relationships. Results can be filtered using complex queries and sorted by relevance.

### RAG

RAG combines retrieval with language model generation to produce informative responses grounded in your content. The system retrieves relevant context from your documents and can optionally integrate web search results to provide up-to-date information, generating accurate, sourced answers to queries.

### Agent

The agent provides a conversational interface for complex information retrieval. It can maintain context across multiple interactions, break down complex queries, and provide detailed responses with citations to source material. It can also leverage web search to incorporate external knowledge when needed.

### Completion

Direct access to language model generation capabilities, useful for tasks that don't require retrieval from your content. Supports both single-turn and multi-turn conversations.

## Filter Operations

Supported operators for content filtering:

* `eq`: Equals
* `neq`: Not equals
* `gt`: Greater than
* `gte`: Greater than or equal
* `lt`: Less than
* `lte`: Less than or equal
* `like`: Pattern matching
* `ilike`: Case-insensitive pattern matching
* `in`: In list
* `nin`: Not in list

Example:

```json
{
  "filters": {
    "metadata.category": {"$eq": "research"},
    "created_at": {"$gte": "2024-01-01"},
    "collection_ids": {"$in": ["uuid1", "uuid2"]}
  }
}
```

## Common Use Cases

1. **Research and Analysis**
   * Literature review
   * Document summarization
   * Relationship discovery
   * Cross-reference verification
   * Integration with web search for comprehensive research

2. **Question Answering**
   * Technical support
   * Educational assistance
   * Policy compliance
   * Data exploration
   * Real-time information access via web search

3. **Content Generation**
   * Report writing
   * Documentation creation
   * Content summarization
   * Knowledge synthesis
   * Fact-checking with web search

4. **Conversational Applications**
   * Interactive chatbots
   * Virtual assistants
   * Educational tutors
   * Research aids with web search capabilities


# Streaming Retrieval API

&gt; Using real-time streaming for RAG and agent interactions

# Streaming Retrieval API

R2R provides powerful streaming capabilities for its retrieval services, including RAG responses and agent interactions. These streaming features allow for real-time updates as information is retrieved and processed, enhancing user experience for applications that benefit from immediate feedback.

## Streaming Events

When using streaming in R2R, various event types are emitted during the retrieval and generation process:

| Event Type           | Description                                                 |
| -------------------- | ----------------------------------------------------------- |
| `SearchResultsEvent` | Contains initial search results from documents              |
| `MessageEvent`       | Streams partial tokens of the response as they're generated |
| `CitationEvent`      | Indicates when a citation is added to the response          |
| `ThinkingEvent`      | Contains the model's step-by-step reasoning (for agents)    |
| `ToolCallEvent`      | Indicates when the model calls a tool (for agents)          |
| `ToolResultEvent`    | Contains results from tool calls (for agents)               |
| `FinalAnswerEvent`   | Contains the complete generated answer with citations       |

## Streaming RAG

### Basic Streaming RAG

To use streaming with basic RAG functionality:

<tabs>
  <tab title="Python">
    ```python
    from r2r import (
        CitationEvent,
        FinalAnswerEvent,
        MessageEvent,
        SearchResultsEvent,
        R2RClient,
    )

    client = R2RClient("http://localhost:7272")

    result_stream = client.retrieval.rag(
        query="What is DeepSeek R1?",
        search_settings={"limit": 25},
        rag_generation_config={"stream": True},
    )

    for event in result_stream:
        if isinstance(event, SearchResultsEvent):
            print("Search results:", event.data)
        elif isinstance(event, MessageEvent):
            print("Partial message:", event.data.delta)
        elif isinstance(event, CitationEvent):
            print("New citation detected:", event.data)
        elif isinstance(event, FinalAnswerEvent):
            print("Final answer:", event.data.generated_answer)
            print("Citations:", event.data.citations)
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    const { r2rClient } = require("r2r-js");

    // Create client with your API key or local server URL
    const client = new r2rClient("http://localhost:7272");

    async function main() {
      // Initiate a streaming RAG request
      const resultStream = await client.retrieval.rag({
        query: "What is DeepSeek R1?",
        searchSettings: { limit: 25 },
        ragGenerationConfig: { stream: true },
      });
      
      // Process the streaming events
      if (Symbol.asyncIterator in resultStream) {
        for await (const rawEvent of resultStream) {
          // Decode the Uint8Array to string
          const textDecoder = new TextDecoder();
          const eventText = textDecoder.decode(rawEvent);
          
          // Parse the SSE format
          const eventLines = eventText.split('\n');
          let eventType = '';
          let eventData = '';
          
          for (const line of eventLines) {
            if (line.startsWith('event:')) {
              eventType = line.substring(6).trim();
            } else if (line.startsWith('data:')) {
              eventData = line.substring(5).trim();
            }
          }
          
          // Skip empty events or done events
          if (!eventType || !eventData || eventData === '[DONE]') {
            if (eventType === 'done') {
              console.log("Stream completed");
            }
            continue;
          }
          
          // Special handling for large events that might be split across multiple packets
          if (eventType === 'search_results') {
            console.log("Search results received!");
            continue;
          }
          
          // Process different event types
          try {
            const parsedData = JSON.parse(eventData);
            
            switch (eventType) {
              case "message":
                if (parsedData.delta &amp;&amp; parsedData.delta.content) {
                  // Extract message content
                  const content = parsedData.delta.content[0]?.payload?.value;
                  console.log("Message delta:", content);
                }
                break;
              case "citation":
                console.log("New citation event:", parsedData);
                break;
              case "final_answer":
                console.log("Final answer:", parsedData.generated_answer);
                console.log("Citations:", parsedData.citations);
                break;
              default:
                console.log(`Event type: ${eventType}`);
            }
          } catch (error) {
            // If JSON parsing fails, log the error and continue
            console.log(`Error parsing ${eventType} event: ${error.message}`);
            
            // For debugging, you could log the first part of the data
            if (eventData &amp;&amp; eventData.length &gt; 100) {
              console.log("Data begins with:", eventData.substring(0, 100) + "...");
            } else {
              console.log("Data:", eventData);
            }
          }
        }
      }
    }

    main().catch(console.error);
    ```
  </tab>

  <tab title="Curl">
    ```bash
    curl -X POST https://api.sciphi.ai/v3/retrieval/rag \
      -H "Content-Type: application/json" \
      -H "Accept: text/event-stream" \
      -d '{
        "query": "What is DeepSeek R1?",
        "search_settings": {"limit": 25},
        "rag_generation_config": {"stream": true}
      }' \
      -H "Authorization: Bearer your_token_here"
    ```
  </tab>
</tabs>

### Streaming RAG with Web Search

To include web search in your streaming RAG:

<tabs>
  <tab title="Python">
    ```python
    result_stream = client.retrieval.rag(
        query="What are the latest developments with DeepSeek R1?", 
        rag_generation_config={"stream": True},
        include_web_search=True
    )

    for event in result_stream:
        # Process events as shown in previous example
        pass
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    const resultStream = await client.retrieval.rag({
      query: "What are the latest developments with DeepSeek R1?",
      ragGenerationConfig: { stream: true },
      includeWebSearch: true
    });

    // Process events as shown in previous example
    ```
  </tab>
</tabs>

## Streaming Agent

R2R provides a powerful streaming agent mode that supports complex interactions with both document-based knowledge and web resources.

### Basic Streaming Agent

<tabs>
  <tab title="Python">
    ```python
    from r2r import (
        ThinkingEvent,
        ToolCallEvent,
        ToolResultEvent,
        CitationEvent,
        MessageEvent,
        FinalAnswerEvent,
    )

    agent_stream = client.retrieval.agent(
        query="What does DeepSeek R1 imply for the future of AI?",
        generation_config={"stream": True},
        mode="research"
    )

    for event in agent_stream:
        if isinstance(event, ThinkingEvent):
            print(f"üß† Thinking: {event.data.delta.content[0].payload.value}")
        elif isinstance(event, ToolCallEvent):
            print(f"üîß Tool call: {event.data.name}({event.data.arguments})")
        elif isinstance(event, ToolResultEvent):
            print(f"üìä Tool result: {event.data.content[:60]}...")
        elif isinstance(event, CitationEvent):
            print(f"üìë Citation: {event.data}")
        elif isinstance(event, MessageEvent):
            print(f"üí¨ Message: {event.data.delta.content[0].payload.value}")
        elif isinstance(event, FinalAnswerEvent):
            print(f"‚úÖ Final answer: {event.data.generated_answer[:100]}...")
            print(f"   Citations: {len(event.data.citations)} sources referenced")
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    const agentStream = await client.retrieval.agent({
      query: "What does DeepSeek R1 imply for the future of AI?",
      generationConfig: { stream: true },
      mode: "research"
    });

    if (Symbol.asyncIterator in agentStream) {
      for await (const event of agentStream) {
        switch(event.event) {
          case "thinking":
            console.log(`üß† Thinking: ${event.data.delta.content[0].payload.value}`);
            break;
          case "tool_call":
            console.log(`üîß Tool call: ${event.data.name}(${JSON.stringify(event.data.arguments)})`);
            break;
          case "tool_result":
            console.log(`üìä Tool result: ${event.data.content.substring(0, 60)}...`);
            break;
          case "citation":
            console.log(`üìë Citation: ${JSON.stringify(event.data)}`);
            break;
          case "message":
            console.log(`üí¨ Message: ${event.data.delta.content[0].payload.value}`);
            break;
          case "final_answer":
            console.log(`‚úÖ Final answer: ${event.data.generated_answer.substring(0, 100)}...`);
            console.log(`   Citations: ${event.data.citations.length} sources referenced`);
            break;
        }
      }
    }
    ```
  </tab>
</tabs>

### Advanced Research Agent with Tools

R2R's agent mode can leverage multiple tools to perform in-depth research:

<tabs>
  <tab title="Python">
    ```python
    agent_stream = client.retrieval.agent(
        query="Analyze DeepSeek R1's performance compared to other models",
        generation_config={
            "model": "anthropic/claude-3-7-sonnet-20250219",
            "extended_thinking": True,
            "thinking_budget": 4096,
            "temperature": 1,
            "max_tokens_to_sample": 16000,
            "stream": True
        },
        mode="research",
        rag_tools=["web_search", "web_scrape"]
    )

    # Process events as shown in previous example
    ```
  </tab>

  <tab title="JavaScript">
    ```javascript
    const agentStream = await client.retrieval.agent({
      query: "Analyze DeepSeek R1's performance compared to other models",
      generationConfig: {
        model: "anthropic/claude-3-7-sonnet-20250219",
        extendedThinking: true,
        thinkingBudget: 4096,
        temperature: 1,
        maxTokensToSample: 16000,
        stream: true
      },
      mode: "research",
      ragTools: ["web_search", "web_scrape"]
    });

    // Process events as shown in previous example
    ```
  </tab>
</tabs>

## Streaming Citations

R2R's streaming citations provide detailed attribution information that links specific parts of the response to source documents:

```json
{
  "event": "citation",
  "data": {
    "id": "abc123",
    "object": "citation",
    "raw_index": 1,
    "index": 1,
    "start_index": 305,
    "end_index": 308,
    "source_type": "chunk",
    "source_id": "e760bb76-1c6e-52eb-910d-0ce5b567011b",
    "document_id": "e43864f5-a36f-548e-aacd-6f8d48b30c7f",
    "source_title": "DeepSeek_R1.pdf"
  }
}
```

Each citation includes:

* `id`: Unique identifier for the citation
* `index`: The display index (e.g., \[1], \[2])
* `start_index` and `end_index`: Character positions in the response
* `source_type`: The type of source (chunk, graph, web)
* `source_id`: ID of the specific chunk/node
* `document_id`: ID of the parent document
* `source_title`: Title of the source document

## Implementing Streaming UI

To create a responsive UI with streaming RAG, consider these patterns:

### Frontend Implementation

<tabs>
  <tab title="React">
    ```javascript
    import { useState, useEffect } from 'react';

    function RAGComponent() {
      const [messages, setMessages] = useState([]);
      const [currentMessage, setCurrentMessage] = useState('');
      const [citations, setCitations] = useState([]);
      const [isLoading, setIsLoading] = useState(false);

      const handleSubmit = async (query) =&gt; {
        setIsLoading(true);
        setCurrentMessage('');
        setCitations([]);
        
        try {
          const response = await fetch('/api/rag', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              query,
              stream: true
            })
          });
          
          const reader = response.body.getReader();
          const decoder = new TextDecoder();
          
          while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            
            const chunk = decoder.decode(value);
            const events = chunk.split('\n\n').filter(Boolean);
            
            for (const eventText of events) {
              if (!eventText.startsWith('data: ')) continue;
              
              const eventData = JSON.parse(eventText.slice(6));
              
              switch (eventData.event) {
                case 'message':
                  setCurrentMessage(prev =&gt; prev + eventData.data.delta.content[0].payload.value);
                  break;
                case 'citation':
                  setCitations(prev =&gt; [...prev, eventData.data]);
                  break;
                case 'final_answer':
                  setMessages(prev =&gt; [...prev, {
                    role: 'assistant',
                    content: eventData.data.generated_answer,
                    citations: eventData.data.citations
                  }]);
                  break;
              }
            }
          }
        } catch (error) {
          console.error('Error with streaming RAG:', error);
        } finally {
          setIsLoading(false);
        }
      };

      return (
        <div classname="rag-container">
          {/* UI implementation */}
          {isLoading &amp;&amp; <div classname="typing-indicator">{currentMessage}</div>}
          {/* Display messages and citations */}
        </div>
      );
    }
    ```
  </tab>
</tabs>

## Best Practices

### Optimizing Streaming RAG

1. **Handle Event Types Properly**
   * Process each event type according to its purpose
   * Update UI incrementally as events arrive
   * Cache search results to improve perceived performance

2. **Error Handling**
   * Implement robust error handling for stream interruptions
   * Provide fallback mechanisms for connection issues
   * Consider retry logic for temporary failures

3. **UI Considerations**
   * Display typing indicators during generation
   * Highlight citations as they appear
   * Show search results separately from generated content

4. **Performance**
   * Monitor stream processing performance
   * Optimize rendering for large responses
   * Consider batching UI updates for efficiency

## Example Implementation

Here's a complete example of RAG with hybrid search, web integration, and streaming:

<tabs>
  <tab title="Python">
    ```python
    from r2r import R2RClient, CitationEvent, MessageEvent, SearchResultsEvent, FinalAnswerEvent

    client = R2RClient("http://localhost:7272")

    # Configure the streaming RAG request
    stream = client.retrieval.rag(
        query="What are the key capabilities of DeepSeek R1 for reasoning tasks?",
        search_settings={
            "use_hybrid_search": True,
            "hybrid_settings": {
                "full_text_weight": 1.0,
                "semantic_weight": 3.0
            },
            "limit": 30
        },
        rag_generation_config={
            "model": "anthropic/claude-3-5-sonnet-20241022",
            "temperature": 0.7,
            "stream": True
        },
        include_web_search=True
    )

    # Process the streaming events
    search_results = None
    message_buffer = ""
    citations = []

    for event in stream:
        if isinstance(event, SearchResultsEvent):
            search_results = event.data
            print(f"Retrieved {len(event.data.chunk_search_results)} chunks")
            
            if event.data.web_search_results:
                print(f"Retrieved {len(event.data.web_search_results)} web results")
        
        elif isinstance(event, MessageEvent):
            delta = event.data.delta.content[0].payload.value
            message_buffer += delta
            print(delta, end="", flush=True)
        
        elif isinstance(event, CitationEvent):
            citations.append(event.data)
            print(f"\n[Citation {event.data.index}]", end="", flush=True)
        
        elif isinstance(event, FinalAnswerEvent):
            print("\n\nFinal answer complete with", len(event.data.citations), "citations")
    ```
  </tab>
</tabs>

## Advanced Configuration

### Customizing Streaming Behavior

<tabs>
  <tab title="Python">
    ```python
    # Custom timeout and chunk size for streaming
    client = R2RClient(
        base_url="http://localhost:7272",
        timeout=120.0,
        stream_chunk_size=4096
    )

    # Configure retrieval and generation parameters
    stream = client.retrieval.rag(
        query="Complex query requiring detailed analysis",
        search_settings={
            "limit": 50,
            "use_hybrid_search": True
        },
        rag_generation_config={
            "model": "anthropic/claude-3-7-sonnet-20250219",
            "stream": True,
            "temperature": 0.2,
            "max_tokens_to_sample": 4000
        }
    )
    ```
  </tab>
</tabs>

## Limitations and Considerations

* Stream connections require stable network connectivity
* Processing streams requires more client-side logic than non-streaming requests
* Citation indices may not be finalized until the entire response is generated
* Some LLM providers may have different streaming behaviors or limitations

For more information on RAG and retrieval capabilities, see the [Search and RAG](/documentation/search-and-rag) and [Retrieval API Reference](/api-and-sdks/retrieval/retrieval) documentation.


# Search R2R

```http
POST https://api.sciphi.ai/v3/retrieval/search
Content-Type: application/json
```

Perform a search query against vector and/or graph-based
databases.

**Search Modes:**
- `basic`: Defaults to semantic search. Simple and easy to use.
- `advanced`: Combines semantic search with full-text search for more comprehensive results.
- `custom`: Complete control over how search is performed. Provide a full `SearchSettings` object.

**Filters:**
Apply filters directly inside `search_settings.filters`. For example:
```json
{
"filters": {"document_id": {"$eq": "e43864f5-a36f-548e-aacd-6f8d48b30c7f"}}
}
```
Supported operators: `$eq`, `$neq`, `$gt`, `$gte`, `$lt`, `$lte`, `$like`, `$ilike`, `$in`, `$nin`.

**Hybrid Search:**
Enable hybrid search by setting `use_hybrid_search: true` in search_settings. This combines semantic search with
keyword-based search for improved results. Configure with `hybrid_settings`:
```json
{
"use_hybrid_search": true,
"hybrid_settings": {
    "full_text_weight": 1.0,
    "semantic_weight": 5.0,
    "full_text_limit": 200,
    "rrf_k": 50
}
}
```

**Graph-Enhanced Search:**
Knowledge graph integration is enabled by default. Control with `graph_search_settings`:
```json
{
"graph_search_settings": {
    "use_graph_search": true,
    "kg_search_type": "local"
}
}
```

**Advanced Filtering:**
Use complex filters to narrow down results by metadata fields or document properties:
```json
{
"filters": {
    "$and":[
        {"document_type": {"$eq": "pdf"}},
        {"metadata.year": {"$gt": 2020}}
    ]
}
}
```

**Results:**
The response includes vector search results and optional graph search results.
Each result contains the matched text, document ID, and relevance score.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# if using auth, do client.login(...)

# Basic search
response = client.retrieval.search(
    query="What is DeepSeek R1?",
)

# Advanced mode with specific filters
response = client.retrieval.search(
    query="What is DeepSeek R1?",
    search_mode="advanced",
    search_settings={
        "filters": {"document_id": {"$eq": "e43864f5-a36f-548e-aacd-6f8d48b30c7f"}},
        "limit": 5
    }
)

# Using hybrid search
response = client.retrieval.search(
    query="What was Uber's profit in 2020?",
    search_settings={
        "use_hybrid_search": True,
        "hybrid_settings": {
            "full_text_weight": 1.0,
            "semantic_weight": 5.0,
            "full_text_limit": 200,
            "rrf_k": 50
        },
        "filters": {"title": {"$in": ["DeepSeek_R1.pdf"]}},
    }
)

# Advanced filtering
results = client.retrieval.search(
    query="What are the effects of climate change?",
    search_settings={
        "filters": {
            "$and":[
                {"document_type": {"$eq": "pdf"}},
                {"metadata.year": {"$gt": 2020}}
            ]
        },
        "limit": 10
    }
)

# Knowledge graph enhanced search
results = client.retrieval.search(
    query="What was DeepSeek R1",
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();
// if using auth, do client.login(...)

// Basic search
const response = await client.retrieval.search({
    query: "What is DeepSeek R1?",
});

// With specific filters
const filteredResponse = await client.retrieval.search({
    query: "What is DeepSeek R1?",
    searchSettings: {
        filters: {"document_id": {"$eq": "e43864f5-a36f-548e-aacd-6f8d48b30c7f"}},
        limit: 5
    }
});

// Using hybrid search
const hybridResponse = await client.retrieval.search({
    query: "What was Uber's profit in 2020?",
    searchSettings: {
        indexMeasure: "l2_distance",
        useHybridSearch: true,
        hybridSettings: {
            fulltextWeight: 1.0,
            semanticWeight: 5.0,
            fulltextLimit: 200,
        },
        filters: {"title": {"$in": ["DeepSeek_R1.pdf"]}},
    }
});

// Advanced filtering
const advancedResults = await client.retrieval.search({
    query: "What are the effects of climate change?",
    searchSettings: {
        filters: {
            $and: [
                {document_type: {$eq: "pdf"}},
                {"metadata.year": {$gt: 2020}}
            ]
        },
        limit: 10
    }
});

// Knowledge graph enhanced search
const kgResults = await client.retrieval.search({
    query: "who was aristotle?"
});

```

```shell Shell

# Basic search
curl -X POST "https://api.sciphi.ai/v3/retrieval/search" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "query": "What is DeepSeek R1?"
}'

# With hybrid search and filters
curl -X POST "https://api.sciphi.ai/v3/retrieval/search" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "query": "What was Uber'''s profit in 2020?",
    "search_settings": {
        "use_hybrid_search": true,
        "hybrid_settings": {
        "full_text_weight": 1.0,
        "semantic_weight": 5.0,
        "full_text_limit": 200,
        "rrf_k": 50
        },
        "filters": {"title": {"$in": ["DeepSeek_R1.pdf"]}},
        "limit": 10,
        "chunk_settings": {
        "index_measure": "l2_distance",
        "probes": 25,
        "ef_search": 100
        }
    }
    }'


```

```shell
curl -X POST https://api.sciphi.ai/v3/retrieval/search \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "query": "query"
}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/retrieval/search \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "query": "string"
}'
```

# RAG Query

```http
POST https://api.sciphi.ai/v3/retrieval/rag
Content-Type: application/json
```

Execute a RAG (Retrieval-Augmented Generation) query.

This endpoint combines search results with language model generation to produce accurate,
contextually-relevant responses based on your document corpus.

**Features:**
- Combines vector search, optional knowledge graph integration, and LLM generation
- Automatically cites sources with unique citation identifiers
- Supports both streaming and non-streaming responses
- Compatible with various LLM providers (OpenAI, Anthropic, etc.)
- Web search integration for up-to-date information

**Search Configuration:**
All search parameters from the search endpoint apply here, including filters, hybrid search, and graph-enhanced search.

**Generation Configuration:**
Fine-tune the language model's behavior with `rag_generation_config`:
```json
{
    "model": "openai/gpt-4o-mini",  // Model to use
    "temperature": 0.7,              // Control randomness (0-1)
    "max_tokens": 1500,              // Maximum output length
    "stream": true                   // Enable token streaming
}
```

**Model Support:**
- OpenAI models (default)
- Anthropic Claude models (requires ANTHROPIC_API_KEY)
- Local models via Ollama
- Any provider supported by LiteLLM

**Streaming Responses:**
When `stream: true` is set, the endpoint returns Server-Sent Events with the following types:
- `search_results`: Initial search results from your documents
- `message`: Partial tokens as they're generated
- `citation`: Citation metadata when sources are referenced
- `final_answer`: Complete answer with structured citations

**Example Response:**
```json
{
"generated_answer": "DeepSeek-R1 is a model that demonstrates impressive performance...[1]",
"search_results": { ... },
"citations": [
    {
        "id": "cit.123456",
        "object": "citation",
        "payload": { ... }
    }
]
}
```

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

# Basic RAG request
response = client.retrieval.rag(
    query="What is DeepSeek R1?",
)

# Advanced RAG with custom search settings
response = client.retrieval.rag(
    query="What is DeepSeek R1?",
    search_settings={
        "use_semantic_search": True,
        "filters": {"document_id": {"$eq": "e43864f5-a36f-548e-aacd-6f8d48b30c7f"}},
        "limit": 10,
    },
    rag_generation_config={
        "stream": False,
        "temperature": 0.7,
        "max_tokens": 1500
    }
)

# Hybrid search in RAG
results = client.retrieval.rag(
    "Who is Jon Snow?",
    search_settings={"use_hybrid_search": True}
)

# Custom model selection
response = client.retrieval.rag(
    "Who was Aristotle?",
    rag_generation_config={"model":"anthropic/claude-3-haiku-20240307", "stream": True}
)
for chunk in response:
    print(chunk)

# Streaming RAG
from r2r import (
    CitationEvent,
    FinalAnswerEvent,
    MessageEvent,
    SearchResultsEvent,
    R2RClient,
)

result_stream = client.retrieval.rag(
    query="What is DeepSeek R1?",
    search_settings={"limit": 25},
    rag_generation_config={"stream": True},
)

# Process different event types
for event in result_stream:
    if isinstance(event, SearchResultsEvent):
        print("Search results:", event.data)
    elif isinstance(event, MessageEvent):
        print("Partial message:", event.data.delta)
    elif isinstance(event, CitationEvent):
        print("New citation detected:", event.data.id)
    elif isinstance(event, FinalAnswerEvent):
        print("Final answer:", event.data.generated_answer)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();
// when using auth, do client.login(...)

// Basic RAG request
const response = await client.retrieval.rag({
    query: "What is DeepSeek R1?",
});

// RAG with custom settings
const advancedResponse = await client.retrieval.rag({
    query: "What is DeepSeek R1?",
    searchSettings: {
        useSemanticSearch: true,
        filters: {"document_id": {"$eq": "e43864f5-a36f-548e-aacd-6f8d48b30c7f"}},
        limit: 10,
    },
    ragGenerationConfig: {
        stream: false,
        temperature: 0.7,
        maxTokens: 1500
    }
});

// Hybrid search in RAG
const hybridResults = await client.retrieval.rag({
    query: "Who is Jon Snow?",
    searchSettings: {
        useHybridSearch: true
    },
});

// Custom model
const customModelResponse = await client.retrieval.rag({
    query: "Who was Aristotle?",
    ragGenerationConfig: {
        model: 'anthropic/claude-3-haiku-20240307',
        temperature: 0.7,
    }
});

// Streaming RAG
const resultStream = await client.retrieval.rag({
    query: "What is DeepSeek R1?",
    searchSettings: { limit: 25 },
    ragGenerationConfig: { stream: true },
});

// Process streaming events
if (Symbol.asyncIterator in resultStream) {
    for await (const event of resultStream) {
        switch (event.event) {
            case "search_results":
                console.log("Search results:", event.data);
                break;
            case "message":
                console.log("Partial message delta:", event.data.delta);
                break;
            case "citation":
                console.log("New citation event:", event.data.id);
                break;
            case "final_answer":
                console.log("Final answer:", event.data.generated_answer);
                break;
            default:
                console.log("Unknown or unhandled event:", event);
        }
    }
}

```

```shell Shell

# Basic RAG request
curl -X POST "https://api.sciphi.ai/v3/retrieval/rag" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "query": "What is DeepSeek R1?"
}'

# RAG with custom settings
curl -X POST "https://api.sciphi.ai/v3/retrieval/rag" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "query": "What is DeepSeek R1?",
    "search_settings": {
        "use_semantic_search": true,
        "filters": {"document_id": {"$eq": "e43864f5-a36f-548e-aacd-6f8d48b30c7f"}},
        "limit": 10
    },
    "rag_generation_config": {
        "stream": false,
        "temperature": 0.7,
        "max_tokens": 1500
    }
}'

# Hybrid search in RAG
curl -X POST "https://api.sciphi.ai/v3/retrieval/rag" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "query": "Who is Jon Snow?",
    "search_settings": {
        "use_hybrid_search": true,
        "filters": {},
        "limit": 10
    }
}'

# Custom model
curl -X POST "https://api.sciphi.ai/v3/retrieval/rag" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "query": "Who is Jon Snow?",
    "rag_generation_config": {
        "model": "anthropic/claude-3-haiku-20240307",
        "temperature": 0.7
    }
}'

```

```shell
curl -X POST https://api.sciphi.ai/v3/retrieval/rag \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "query": "query"
}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/retrieval/rag \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "query": "string"
}'
```

# RAG-powered Conversational Agent

```http
POST https://api.sciphi.ai/v3/retrieval/agent
Content-Type: application/json
```

Engage with an intelligent agent for information retrieval, analysis, and research.

This endpoint offers two operating modes:
- **RAG mode**: Standard retrieval-augmented generation for answering questions based on knowledge base
- **Research mode**: Advanced capabilities for deep analysis, reasoning, and computation

### RAG Mode (Default)

The RAG mode provides fast, knowledge-based responses using:
- Semantic and hybrid search capabilities
- Document-level and chunk-level content retrieval
- Optional web search integration
- Source citation and evidence-based responses

### Research Mode

The Research mode builds on RAG capabilities and adds:
- A dedicated reasoning system for complex problem-solving
- Critique capabilities to identify potential biases or logical fallacies
- Python execution for computational analysis
- Multi-step reasoning for deeper exploration of topics

### Available Tools

**RAG Tools:**
- `search_file_knowledge`: Semantic/hybrid search on your ingested documents
- `search_file_descriptions`: Search over file-level metadata
- `content`: Fetch entire documents or chunk structures
- `web_search`: Query external search APIs for up-to-date information
- `web_scrape`: Scrape and extract content from specific web pages

**Research Tools:**
- `rag`: Leverage the underlying RAG agent for information retrieval
- `reasoning`: Call a dedicated model for complex analytical thinking
- `critique`: Analyze conversation history to identify flaws and biases
- `python_executor`: Execute Python code for complex calculations and analysis

### Streaming Output

When streaming is enabled, the agent produces different event types:
- `thinking`: Shows the model's step-by-step reasoning (when extended_thinking=true)
- `tool_call`: Shows when the agent invokes a tool
- `tool_result`: Shows the result of a tool call
- `citation`: Indicates when a citation is added to the response
- `message`: Streams partial tokens of the response
- `final_answer`: Contains the complete generated answer and structured citations

### Conversations

Maintain context across multiple turns by including `conversation_id` in each request.
After your first call, store the returned `conversation_id` and include it in subsequent calls.
If no conversation name has already been set for the conversation, the system will automatically assign one.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import (
    R2RClient,
    ThinkingEvent,
    ToolCallEvent,
    ToolResultEvent,
    CitationEvent,
    FinalAnswerEvent,
    MessageEvent,
)

client = R2RClient()
# when using auth, do client.login(...)

# Basic synchronous request
response = client.retrieval.agent(
    message={
        "role": "user",
        "content": "Do a deep analysis of the philosophical implications of DeepSeek R1"
    },
    rag_tools=["web_search", "web_scrape", "search_file_descriptions", "search_file_knowledge", "get_file_content"],
)

# Advanced analysis with streaming and extended thinking
streaming_response = client.retrieval.agent(
    message={
        "role": "user",
        "content": "Do a deep analysis of the philosophical implications of DeepSeek R1"
    },
    search_settings={"limit": 20},
    rag_tools=["web_search", "web_scrape", "search_file_descriptions", "search_file_knowledge", "get_file_content"],
    rag_generation_config={
        "model": "anthropic/claude-3-7-sonnet-20250219",
        "extended_thinking": True,
        "thinking_budget": 4096,
        "temperature": 1,
        "top_p": None,
        "max_tokens": 16000,
        "stream": True
    }
)

# Process streaming events with emoji only on type change
current_event_type = None
for event in streaming_response:
    # Check if the event type has changed
    event_type = type(event)
    if event_type != current_event_type:
        current_event_type = event_type
        print() # Add newline before new event type

        # Print emoji based on the new event type
        if isinstance(event, ThinkingEvent):
            print(f"
üß† Thinking: ", end="", flush=True)
        elif isinstance(event, ToolCallEvent):
            print(f"
üîß Tool call: ", end="", flush=True)
        elif isinstance(event, ToolResultEvent):
            print(f"
üìä Tool result: ", end="", flush=True)
        elif isinstance(event, CitationEvent):
            print(f"
üìë Citation: ", end="", flush=True)
        elif isinstance(event, MessageEvent):
            print(f"
üí¨ Message: ", end="", flush=True)
        elif isinstance(event, FinalAnswerEvent):
            print(f"
‚úÖ Final answer: ", end="", flush=True)

    # Print the content without the emoji
    if isinstance(event, ThinkingEvent):
        print(f"{event.data.delta.content[0].payload.value}", end="", flush=True)
    elif isinstance(event, ToolCallEvent):
        print(f"{event.data.name}({event.data.arguments})")
    elif isinstance(event, ToolResultEvent):
        print(f"{event.data.content[:60]}...")
    elif isinstance(event, CitationEvent):
        print(f"{event.data.id}")
    elif isinstance(event, MessageEvent):
        print(f"{event.data.delta.content[0].payload.value}", end="", flush=True)
    elif isinstance(event, FinalAnswerEvent):
        print(f"{event.data.generated_answer[:100]}...")
        print(f"   Citations: {len(event.data.citations)} sources referenced")

# Conversation with multiple turns (synchronous)
conversation = client.conversations.create()

# First message in conversation
results_1 = client.retrieval.agent(
    message={"role": "user", "content": "What does DeepSeek R1 imply for the future of AI?"},
    rag_generation_config={
        "model": "anthropic/claude-3-7-sonnet-20250219",
        "extended_thinking": True,
        "thinking_budget": 4096,
        "temperature": 1,
        "top_p": None,
        "max_tokens": 16000,
        "stream": False
    },
    conversation_id=conversation.results.id
)

# Follow-up query in the same conversation
results_2 = client.retrieval.agent(
    message={"role": "user", "content": "How does it compare to other reasoning models?"},
    rag_generation_config={
        "model": "anthropic/claude-3-7-sonnet-20250219",
        "extended_thinking": True,
        "thinking_budget": 4096,
        "temperature": 1,
        "top_p": None,
        "max_tokens": 16000,
        "stream": False
    },
    conversation_id=conversation.results.id
)

# Access the final results
print(f"First response: {results_1.generated_answer[:100]}...")
print(f"Follow-up response: {results_2.generated_answer[:100]}...")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();
// when using auth, do client.login(...)

async function main() {
    // Basic synchronous request
    const ragResponse = await client.retrieval.agent({
        message: {
            role: "user",
            content: "Do a deep analysis of the philosophical implications of DeepSeek R1"
        },
        ragTools: ["web_search", "web_scrape", "search_file_descriptions", "search_file_knowledge", "get_file_content"]
    });

    // Advanced analysis with streaming and extended thinking
    const streamingResponse = await client.retrieval.agent({
        message: {
            role: "user",
            content: "Do a deep analysis of the philosophical implications of DeepSeek R1"
        },
        searchSettings: {limit: 20},
        ragTools: ["web_search", "web_scrape", "search_file_descriptions", "search_file_knowledge", "get_file_content"],
        ragGenerationConfig: {
            model: "anthropic/claude-3-7-sonnet-20250219",
            extendedThinking: true,
            thinkingBudget: 4096,
            temperature: 1,
            maxTokens: 16000,
            stream: true
        }
    });

    // Process streaming events with emoji only on type change
    if (Symbol.asyncIterator in streamingResponse) {
        let currentEventType = null;

        for await (const event of streamingResponse) {
            // Check if event type has changed
            const eventType = event.event;
            if (eventType !== currentEventType) {
                currentEventType = eventType;
                console.log(); // Add newline before new event type

                // Print emoji based on the new event type
                switch(eventType) {
                    case "thinking":
                        process.stdout.write(`üß† Thinking: `);
                        break;
                    case "tool_call":
                        process.stdout.write(`üîß Tool call: `);
                        break;
                    case "tool_result":
                        process.stdout.write(`üìä Tool result: `);
                        break;
                    case "citation":
                        process.stdout.write(`üìë Citation: `);
                        break;
                    case "message":
                        process.stdout.write(`üí¨ Message: `);
                        break;
                    case "final_answer":
                        process.stdout.write(`‚úÖ Final answer: `);
                        break;
                }
            }

            // Print content based on event type
            switch(eventType) {
                case "thinking":
                    process.stdout.write(`${event.data.delta.content[0].payload.value}`);
                    break;
                case "tool_call":
                    console.log(`${event.data.name}(${JSON.stringify(event.data.arguments)})`);
                    break;
                case "tool_result":
                    console.log(`${event.data.content.substring(0, 60)}...`);
                    break;
                case "citation":
                    console.log(`${event.data.id}`);
                    break;
                case "message":
                    process.stdout.write(`${event.data.delta.content[0].payload.value}`);
                    break;
                case "final_answer":
                    console.log(`${event.data.generated_answer.substring(0, 100)}...`);
                    console.log(`   Citations: ${event.data.citations.length} sources referenced`);
                    break;
            }
        }
    }

    // Conversation with multiple turns (synchronous)
    const conversation = await client.conversations.create();

    // First message in conversation
    const results1 = await client.retrieval.agent({
        message: {"role": "user", "content": "What does DeepSeek R1 imply for the future of AI?"},
        ragGenerationConfig: {
            model: "anthropic/claude-3-7-sonnet-20250219",
            extendedThinking: true,
            thinkingBudget: 4096,
            temperature: 1,
            maxTokens: 16000,
            stream: false
        },
        conversationId: conversation.results.id
    });

    // Follow-up query in the same conversation
    const results2 = await client.retrieval.agent({
        message: {"role": "user", "content": "How does it compare to other reasoning models?"},
        ragGenerationConfig: {
            model: "anthropic/claude-3-7-sonnet-20250219",
            extendedThinking: true,
            thinkingBudget: 4096,
            temperature: 1,
            maxTokens: 16000,
            stream: false
        },
        conversationId: conversation.results.id
    });

    // Log the results
    console.log(`First response: ${results1.generated_answer.substring(0, 100)}...`);
    console.log(`Follow-up response: ${results2.generated_answer.substring(0, 100)}...`);
}

main();

```

```shell Shell

# Basic request
curl -X POST "https://api.sciphi.ai/v3/retrieval/agent" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "message": {
        "role": "user",
        "content": "What were the key contributions of Aristotle to logic?"
    },
    "search_settings": {
        "use_semantic_search": true,
        "filters": {"document_id": {"$eq": "e43864f5-a36f-548e-aacd-6f8d48b30c7f"}}
    },
    "rag_tools": ["search_file_knowledge", "get_file_content", "web_search"]
}'

# Advanced analysis with extended thinking
curl -X POST "https://api.sciphi.ai/v3/retrieval/agent" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "message": {
        "role": "user",
        "content": "Do a deep analysis of the philosophical implications of DeepSeek R1"
    },
    "search_settings": {"limit": 20},
    "research_tools": ["rag", "reasoning", "critique", "python_executor"],
    "rag_generation_config": {
        "model": "anthropic/claude-3-7-sonnet-20250219",
        "extended_thinking": true,
        "thinking_budget": 4096,
        "temperature": 1,
        "top_p": null,
        "max_tokens": 16000,
        "stream": False
    }
}'

# Conversation continuation
curl -X POST "https://api.sciphi.ai/v3/retrieval/agent" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "message": {
        "role": "user",
        "content": "How does it compare to other reasoning models?"
    },
    "conversation_id": "YOUR_CONVERSATION_ID"
}'

```

```shell
curl -X POST https://api.sciphi.ai/v3/retrieval/agent \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/retrieval/agent \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Generate Message Completions

```http
POST https://api.sciphi.ai/v3/retrieval/completion
Content-Type: application/json
```

Generate completions for a list of messages.

This endpoint uses the language model to generate completions for
the provided messages. The generation process can be customized
using the generation_config parameter.

The messages list should contain alternating user and assistant
messages, with an optional system message at the start. Each
message should have a 'role' and 'content'.

## Request Headers

- X-API-Key (required)

## Query Parameters

- response_model (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.completion(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"},
        {"role": "assistant", "content": "The capital of France is Paris."},
        {"role": "user", "content": "What about Italy?"}
    ],
    generation_config={
        "model": "openai/gpt-4o-mini",
        "temperature": 0.7,
        "max_tokens": 150,
        "stream": False
    }
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();
// when using auth, do client.login(...)

async function main() {
    const response = await client.completion({
        messages: [
            { role: "system", content: "You are a helpful assistant." },
            { role: "user", content: "What is the capital of France?" },
            { role: "assistant", content: "The capital of France is Paris." },
            { role: "user", content: "What about Italy?" }
        ],
        generationConfig: {
            model: "openai/gpt-4o-mini",
            temperature: 0.7,
            maxTokens: 150,
            stream: false
        }
    });
}

main();

```

```shell Shell

curl -X POST "https://api.sciphi.ai/v3/retrieval/completion" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"},
        {"role": "assistant", "content": "The capital of France is Paris."},
        {"role": "user", "content": "What about Italy?"}
    ],
    "generation_config": {
        "model": "openai/gpt-4o-mini",
        "temperature": 0.7,
        "max_tokens": 150,
        "stream": false
    }
    }'

```

```shell
curl -X POST https://api.sciphi.ai/v3/retrieval/completion \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json"
```

```shell
curl -X POST "https://api.sciphi.ai/v3/retrieval/completion?response_model=string" \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "messages": [
    {
      "role": "system"
    }
  ]
}'
```

# Generate Embeddings

```http
POST https://api.sciphi.ai/v3/retrieval/embedding
Content-Type: application/json
```

Generate embeddings for the provided text using the specified
model.

This endpoint uses the language model to generate embeddings for
the provided text. The model parameter specifies the model to use
for generating embeddings.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.retrieval.embedding(
    text="What is DeepSeek R1?",
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();
// when using auth, do client.login(...)

async function main() {
    const response = await client.retrieval.embedding({
        text: "What is DeepSeek R1?",
    });
}

main();

```

```shell Shell

curl -X POST "https://api.sciphi.ai/v3/retrieval/embedding" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "text": "What is DeepSeek R1?",
    }'

```

```shell
curl -X POST https://api.sciphi.ai/v3/retrieval/embedding \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d "string"
```

```shell
curl -X POST https://api.sciphi.ai/v3/retrieval/embedding \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d "string"
```

# Documents

A `Document` in R2R is the system's digital representation of any piece of content you ingest, like a PDF report, webpage text, image, or audio file. It acts as the central container for downstream `Chunks`, `Entities`, and more.

Key processes associated with a `Document` include:

* **Ingestion:** Content is accepted from various formats (`.pdf`, `.docx`, `.txt`, `.png`, `.mp3`, etc.) via file upload, raw text, or pre-defined chunks.
* **Chunking:** The document's content is broken down into smaller, searchable `Chunks`.
* **Metadata &amp; Collections:** Documents are associated with descriptive `metadata` (e.g., title, source) and organized into `Collections` for access control.
* **Enrichment (Optional):** The system can extract `Entities` and `Relationships` for knowledge graphs or generate embeddings for semantic search.
* **Status Tracking:** Ingestion and enrichment processes are monitored.

Essentially, the `Document` object is R2R's foundational unit for turning your raw information into searchable, analyzable knowledge for RAG and agentic workflows.

## Core Document Endpoints

This section provides a high-level overview. See the detailed endpoint documentation below for request/response schemas and examples.

| Method   | Endpoint                               | Description                                                          |
| :------- | :------------------------------------- | :------------------------------------------------------------------- |
| `POST`   | `/documents`                           | Ingest new information (file, text, or chunks) as a document.        |
| `GET`    | `/documents`                           | List existing documents with pagination and filtering.               |
| `GET`    | `/documents/{id}`                      | Retrieve details (metadata, status) about a specific document.       |
| `GET`    | `/documents/{id}/download`             | Download the original source file of a document.                     |
| `GET`    | `/documents/{id}/chunks`               | List the text `Chunks` derived from a document's content.            |
| `PATCH`  | `/documents/{id}/metadata`             | Add or update `metadata` for a document.                             |
| `PUT`    | `/documents/{id}/metadata`             | Replace all `metadata` for a document.                               |
| `DELETE` | `/documents/{id}`                      | Delete a specific document and its associated data.                  |
| `DELETE` | `/documents/by-filter`                 | Delete multiple documents matching filter criteria.                  |
| `POST`   | `/documents/search`                    | Search across generated document *summaries*.                        |
| `GET`    | `/documents/download_zip`              | Download multiple original document files as a zip archive.          |
| `POST`   | `/documents/export`                    | Export document metadata to CSV (superuser).                         |
| `POST`   | `/documents/{id}/extract`              | Start knowledge graph entity/relationship extraction for a document. |
| `GET`    | `/documents/{id}/entities`             | List `Entities` identified within a document.                        |
| `POST`   | `/documents/{id}/entities/export`      | Export a document's entities to CSV (superuser).                     |
| `GET`    | `/documents/{id}/relationships`        | List `Relationships` identified within a document.                   |
| `POST`   | `/documents/{id}/relationships/export` | Export a document's relationships to CSV (superuser).                |
| `POST`   | `/documents/{id}/deduplicate`          | Start entity deduplication process for a document's entities.        |
| `GET`    | `/documents/{id}/collections`          | List `Collections` that contain a specific document (superuser).     |

***


# List documents

```http
GET https://api.sciphi.ai/v3/documents
```

Returns a paginated list of documents the authenticated user has
access to.

Results can be filtered by providing specific document IDs. Regular
users will only see documents they own or have access to through
collections. Superusers can see all documents.

The documents are returned in order of last modification, with most
recent first.

## Request Headers

- X-API-Key (required)

## Query Parameters

- ids (optional): A list of document IDs to retrieve. If not provided, all documents will be returned.
- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.
- include_summary_embeddings (optional): Specifies whether or not to include embeddings of each document summary.
- owner_only (optional): If true, only returns documents owned by the user, not all accessible documents.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.list(
    limit=10,
    offset=0
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.list({
        limit: 10,
        offset: 0,
    });
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/documents"  \
-H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -G https://api.sciphi.ai/v3/documents \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d ids=string \
     -d offset=0
```

# Create a new document

```http
POST https://api.sciphi.ai/v3/documents
Content-Type: multipart/form-data
```

Creates a new Document object from an input file, text content, or chunks. The chosen `ingestion_mode` determines
how the ingestion process is configured:

**Ingestion Modes:**
- `hi-res`: Comprehensive parsing and enrichment, including summaries and possibly more thorough parsing.
- `fast`: Speed-focused ingestion that skips certain enrichment steps like summaries.
- `custom`: Provide a full `ingestion_config` to customize the entire ingestion process.

Either a file or text content must be provided, but not both. Documents are shared through `Collections` which allow for tightly specified cross-user interactions.

The ingestion process runs asynchronously and its progress can be tracked using the returned
task_id.

## Request Headers

- X-API-Key (required)

## Response Body

- 202: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.create(
    file_path="pg_essay_1.html",
    metadata={"metadata_1":"some random metadata"},
    id=None
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.create({
        file: { path: "examples/data/marmeladov.txt", name: "marmeladov.txt" },
        metadata: { title: "marmeladov.txt" },
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/documents" \
-H "Content-Type: multipart/form-data" \
-H "Authorization: Bearer YOUR_API_KEY" \
-F "file=@pg_essay_1.html;type=text/html" \
-F 'metadata={}' \
-F 'id=null'

```

```shell
curl -X POST https://api.sciphi.ai/v3/documents \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: multipart/form-data"
```

# Replace metadata of a document

```http
PUT https://api.sciphi.ai/v3/documents/{id}/metadata
Content-Type: application/json
```

Replaces metadata in a document. This endpoint allows overwriting existing metadata fields.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The ID of the document to append metadata to.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.replace_metadata(
    id="9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
    metadata=[{"key": "new_key", "value": "new_value"}]
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.replaceMetadata({
        id: "9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
        metadata: [{ key: "new_key", value: "new_value" }],
    });
}

main();

```

```shell
curl -X PUT https://api.sciphi.ai/v3/documents/id/metadata \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '[
  {
    "key": "value"
  }
]'
```

```shell
curl -X PUT https://api.sciphi.ai/v3/documents/:id/metadata \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '[
  {
    "string": {}
  }
]'
```

# Append metadata to a document

```http
PATCH https://api.sciphi.ai/v3/documents/{id}/metadata
Content-Type: application/json
```

Appends metadata to a document. This endpoint allows adding new metadata fields or updating existing ones.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The ID of the document to append metadata to.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.append_metadata(
    id="9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
    metadata=[{"key": "new_key", "value": "new_value"}]
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.appendMetadata({
        id: "9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
        metadata: [{ key: "new_key", value: "new_value" }],
    });
}

main();

```

```shell
curl -X PATCH https://api.sciphi.ai/v3/documents/id/metadata \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '[
  {
    "key": "value"
  }
]'
```

```shell
curl -X PATCH https://api.sciphi.ai/v3/documents/:id/metadata \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '[
  {
    "string": {}
  }
]'
```

# Export documents to CSV

```http
POST https://api.sciphi.ai/v3/documents/export
Content-Type: application/json
```

Export documents as a downloadable CSV file.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.documents.export(
    output_path="export.csv",
    columns=["id", "title", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.documents.export({
        outputPath: "export.csv",
        columns: ["id", "title", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/documents/export"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "title", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/documents/export \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Export multiple documents as zip

```http
GET https://api.sciphi.ai/v3/documents/download_zip
```

Export multiple documents as a zip file. Documents can be
filtered by IDs and/or date range.

The endpoint allows downloading:
- Specific documents by providing their IDs
- Documents within a date range
- All accessible documents if no filters are provided

Files are streamed as a zip archive to handle potentially large downloads efficiently.

## Request Headers

- X-API-Key (required)

## Query Parameters

- document_ids (optional): List of document IDs to include in the export. If not provided, all accessible documents will be included.
- start_date (optional): Filter documents created on or after this date.
- end_date (optional): Filter documents created before this date.

## Response Body


- 422: Validation Error

## Examples

```python Python

client.documents.download_zip(
    document_ids=["uuid1", "uuid2"],
    start_date="2024-01-01",
    end_date="2024-12-31"
)

```

```shell cURL

curl -X GET "https://api.example.com/v3/documents/download_zip?document_ids=uuid1,uuid2&amp;start_date=2024-01-01&amp;end_date=2024-12-31" \
-H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -G https://api.sciphi.ai/v3/documents/download_zip \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d document_ids=string \
     --data-urlencode start_date=2023-01-01T00:00:00Z
```

# Retrieve a document

```http
GET https://api.sciphi.ai/v3/documents/{id}
```

Retrieves detailed information about a specific document by its
ID.

This endpoint returns the document's metadata, status, and system information. It does not
return the document's content - use the `/documents/{id}/download` endpoint for that.

Users can only retrieve documents they own or have access to through collections.
Superusers can retrieve any document.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The ID of the document to retrieve.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.retrieve(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.retrieve({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    });
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/documents/b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"  \
-H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl https://api.sciphi.ai/v3/documents/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Delete a document

```http
DELETE https://api.sciphi.ai/v3/documents/{id}
```

Delete a specific document. All chunks corresponding to the
document are deleted, and all other references to the document are
removed.

NOTE - Deletions do not yet impact the knowledge graph or other derived data. This feature is planned for a future release.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): Document ID

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.delete(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.delete({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    });
}

main();

```

```shell cURL

curl -X DELETE "https://api.example.com/v3/documents/b4ac4dd6-5f27-596e-a55b-7cf242ca30aa" \
-H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/documents/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# List document chunks

```http
GET https://api.sciphi.ai/v3/documents/{id}/chunks
```

Retrieves the text chunks that were generated from a document
during ingestion. Chunks represent semantic sections of the
document and are used for retrieval and analysis.

Users can only access chunks from documents they own or have access
to through collections. Vector embeddings are only included if
specifically requested.

Results are returned in chunk sequence order, representing their
position in the original document.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The ID of the document to retrieve chunks for.

## Query Parameters

- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.
- include_vectors (optional): Whether to include vector embeddings in the response.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.list_chunks(
    id="32b6a70f-a995-5c51-85d2-834f06283a1e"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.listChunks({
        id: "32b6a70f-a995-5c51-85d2-834f06283a1e",
    });
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/documents/b4ac4dd6-5f27-596e-a55b-7cf242ca30aa/chunks"  \
-H "Authorization: Bearer YOUR_API_KEY"                            
```

```shell
curl -G https://api.sciphi.ai/v3/documents/:id/chunks \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d offset=0 \
     -d limit=0
```

# Download document content

```http
GET https://api.sciphi.ai/v3/documents/{id}/download
```

Downloads the original file content of a document.

For uploaded files, returns the original file with its proper MIME
type. For text-only documents, returns the content as plain text.

Users can only download documents they own or have access to
through collections.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): Document ID

## Response Body


- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.download(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.download({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    });
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/documents/b4ac4dd6-5f27-596e-a55b-7cf242ca30aa/download"  \
-H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl https://api.sciphi.ai/v3/documents/:id/download \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Delete documents by filter

```http
DELETE https://api.sciphi.ai/v3/documents/by-filter
Content-Type: application/json
```

Delete documents based on provided filters.

Allowed operators
include: `eq`, `neq`, `gt`, `gte`, `lt`, `lte`, `like`,
`ilike`, `in`, and `nin`. Deletion requests are limited to a
user's own documents.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient
client = R2RClient()
# when using auth, do client.login(...)
response = client.documents.delete_by_filter(
    filters={"document_type": {"$eq": "txt"}}
)

```

```shell cURL

curl -X DELETE "https://api.example.com/v3/documents/by-filter?filters=%7B%22document_type%22%3A%7B%22%24eq%22%3A%22text%22%7D%2C%22created_at%22%3A%7B%22%24lt%22%3A%222023-01-01T00%3A00%3A00Z%22%7D%7D" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/documents/by-filter \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "string": {}
}'
```

# List document collections

```http
GET https://api.sciphi.ai/v3/documents/{id}/collections
```

Retrieves all collections that contain the specified document.
This endpoint is restricted to superusers only and provides a
system-wide view of document organization.

Collections are used to organize documents and manage access control. A document can belong
to multiple collections, and users can access documents through collection membership.

The results are paginated and ordered by collection creation date, with the most recently
created collections appearing first.

NOTE - This endpoint is only available to superusers, it will be extended to regular users in a future release.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): Document ID

## Query Parameters

- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.list_collections(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa", offset=0, limit=10
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.listCollections({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    });
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/documents/b4ac4dd6-5f27-596e-a55b-7cf242ca30aa/collections"  \
-H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -G https://api.sciphi.ai/v3/documents/:id/collections \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d offset=0 \
     -d limit=0
```

# Extract entities and relationships

```http
POST https://api.sciphi.ai/v3/documents/{id}/extract
Content-Type: application/json
```

Extracts entities and relationships from a document.

The entities and relationships extraction process involves:

    1. Parsing documents into semantic chunks

    2. Extracting entities and relationships using LLMs

    3. Storing the created entities and relationships in the knowledge graph

    4. Preserving the document's metadata and content, and associating the elements with collections the document belongs to

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The ID of the document to extract entities and relationships from.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.documents.extract(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
)

```

```shell
curl -X POST https://api.sciphi.ai/v3/documents/id/extract \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/documents/:id/extract \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Deduplicate entities

```http
POST https://api.sciphi.ai/v3/documents/{id}/deduplicate
Content-Type: application/json
```

Deduplicates entities from a document.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The ID of the document to extract entities and relationships from.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()

response = client.documents.deduplicate(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.documents.deduplicate({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/documents/b4ac4dd6-5f27-596e-a55b-7cf242ca30aa/deduplicate"  \
-H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X POST https://api.sciphi.ai/v3/documents/:id/deduplicate \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Export document entities to CSV

```http
POST https://api.sciphi.ai/v3/documents/{id}/entities/export
Content-Type: application/json
```

Export documents as a downloadable CSV file.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The ID of the document to export entities from.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.documents.export_entities(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    output_path="export.csv",
    columns=["id", "title", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.documents.exportEntities({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
        outputPath: "export.csv",
        columns: ["id", "title", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/documents/export_entities"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "title", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/documents/:id/entities/export \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Export document relationships to CSV

```http
POST https://api.sciphi.ai/v3/documents/{id}/relationships/export
Content-Type: application/json
```

Export documents as a downloadable CSV file.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The ID of the document to export entities from.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.documents.export_entities(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    output_path="export.csv",
    columns=["id", "title", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.documents.exportEntities({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
        outputPath: "export.csv",
        columns: ["id", "title", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/documents/export_entities"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "title", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/documents/:id/relationships/export \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Search document summaries

```http
POST https://api.sciphi.ai/v3/documents/search
Content-Type: application/json
```

Perform a search query on the automatically generated document
summaries in the system.

This endpoint allows for complex filtering of search results using PostgreSQL-based queries.
Filters can be applied to various fields such as document_id, and internal metadata values.


Allowed operators include `eq`, `neq`, `gt`, `gte`, `lt`, `lte`, `like`, `ilike`, `in`, and `nin`.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```shell
curl -X POST https://api.sciphi.ai/v3/documents/search \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "query": "query"
}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/documents/search \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "query": "string"
}'
```

# Graphs

A `Graph` in R2R is a knowledge graph that is associated with a specific `Collection`. Each Graph contains:

* **Entities**: Extracted information nodes from documents (e.g., people, places, concepts)
* **Relationships**: Connections between entities defining how they relate
* **Communities**: LLM generated descriptions of groupings of related entities found from Leiden clustering.
* **Document Mappings**: Tracking which documents have contributed to the graph

Key features of Graphs in R2R:

1. **Git-like Model**
   * Each Collection has an associated Graph that can diverge independently
   * The `pull` operation syncs document knowledge into the graph
   * Changes can be experimental without affecting the base Collection and underlying documents

2. **Knowledge Organization**
   * Automatic entity and relationship extraction from documents
   * Community detection for hierarchical knowledge organization
   * Support for manual creation and editing of entities, relationships and communities
   * Rich metadata and property management

3. **Access Control**
   * Graph operations are tied to Collection permissions
   * Superuser privileges required for certain operations like community building
   * Document-level access checks when pulling content

### Core Operations

| Method | Endpoint                                    | Description                  |
| ------ | ------------------------------------------- | ---------------------------- |
| GET    | `/graphs/{collection_id}`                   | Get graph details            |
| POST   | `/graphs/{collection_id}/pull`              | Sync documents with graph    |
| POST   | `/graphs/{collection_id}/communities/build` | Build graph communities      |
| POST   | `/graphs/{collection_id}/reset`             | Reset graph to initial state |

### Entity Management

| Method | Endpoint                                       | Description   |
| ------ | ---------------------------------------------- | ------------- |
| GET    | `/graphs/{collection_id}/entities`             | List entities |
| POST   | `/graphs/{collection_id}/entities`             | Create entity |
| GET    | `/graphs/{collection_id}/entities/{entity_id}` | Get entity    |
| POST   | `/graphs/{collection_id}/entities/{entity_id}` | Update entity |
| DELETE | `/graphs/{collection_id}/entities/{entity_id}` | Delete entity |

Similar CRUD endpoints exist by mapping `/entities` above to for relationships (`/relationships`) and communities (`/communities`).


# Entities

### Entity management endpoints

Entities are basic building blocks of a graph. They can be automatically extracted from a document and then added to a graph.

To automatically extract entities and relationships from a document, you can use the following endpoint:

| Method | Endpoint                                     | Description                                        |
| ------ | -------------------------------------------- | -------------------------------------------------- |
| POST   | `/documents/{id}/entities_and_relationships` | Extract entities and relationships from a document |

To manage entities extracted from a document, you can use the following endpoints:

| Method | Endpoint                        | Description                      |
| ------ | ------------------------------- | -------------------------------- |
| GET    | `/documents/{id}/entities`      | Get entities for a document      |
| GET    | `/documents/{id}/entities/{id}` | Get an entity for a document     |
| POST   | `/documents/{id}/entities`      | Add entities to a document       |
| POST   | `/documents/{id}/entities/{id}` | Update an entity for a document  |
| DELETE | `/documents/{id}/entities/{id}` | Delete an entity from a document |

Once entities are added to a graph by a POST to "/graphs/\{id}/\{object\_type}" endpoint, you can manage them using the following endpoints:

| Method | Endpoint                     | Description                             |
| ------ | ---------------------------- | --------------------------------------- |
| GET    | `/graphs/{id}/entities`      | Get entities for a graph                |
| GET    | `/graphs/{id}/entities/{id}` | Get an entity for a graph               |
| POST   | `/graphs/{id}/entities`      | Add entities to a graph                 |
| POST   | `/graphs/{id}/entities/{id}` | Update a specific entity on the graph   |
| DELETE | `/graphs/{id}/entities/{id}` | Delete a specific entity from the graph |


# Get Entities

```http
GET https://api.sciphi.ai/v3/graphs/{collection_id}/entities
```

Lists all entities in the graph with pagination support.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to list entities from.

## Query Parameters

- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.list_entities(collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.listEntities({
        collection_id: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
    });
}

main();

```

```shell
curl https://api.sciphi.ai/v3/graphs/collection_id/entities \
     -H "Authorization: Bearer <token>"
```

```shell
curl -G https://api.sciphi.ai/v3/graphs/:collection_id/entities \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d offset=0 \
     -d limit=0
```

# Create Entity

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/entities
Content-Type: application/json
```

Creates a new entity in the graph.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to add the entity to.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/collection_id/entities \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "name": "name",
  "description": "description"
}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/entities \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "name": "string",
  "description": "string"
}'
```

# Export graph entities to CSV

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/entities/export
Content-Type: application/json
```

Export documents as a downloadable CSV file.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The ID of the collection to export entities from.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.graphs.export_entities(
    collection_id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    output_path="export.csv",
    columns=["id", "title", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.graphs.exportEntities({
        collectionId: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
        outputPath: "export.csv",
        columns: ["id", "title", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/graphs/export_entities"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "title", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/entities/export \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Get Entity

```http
GET https://api.sciphi.ai/v3/graphs/{collection_id}/entities/{entity_id}
```

Retrieves a specific entity by its ID.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph containing the entity.
- entity_id (required): The ID of the entity to retrieve.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.get_entity(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
    entity_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.get_entity({
        collectionId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
        entityId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
    });
}

main();

```

```shell
curl https://api.sciphi.ai/v3/graphs/collection_id/entities/entity_id \
     -H "Authorization: Bearer <token>"
```

```shell
curl https://api.sciphi.ai/v3/graphs/:collection_id/entities/:entity_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Update Entity

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/entities/{entity_id}
Content-Type: application/json
```

Updates an existing entity in the graph.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph containing the entity.
- entity_id (required): The ID of the entity to update.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/collection_id/entities/entity_id \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/entities/:entity_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Remove an entity

```http
DELETE https://api.sciphi.ai/v3/graphs/{collection_id}/entities/{entity_id}
```

Removes an entity from the graph.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to remove the entity from.
- entity_id (required): The ID of the entity to remove from the graph.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.remove_entity(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
    entity_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.removeEntity({
        collectionId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
        entityId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
    });
}

main();

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/graphs/collection_id/entities/entity_id \
     -H "Authorization: Bearer <token>"
```

```shell
curl -X DELETE https://api.sciphi.ai/v3/graphs/:collection_id/entities/:entity_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Relationships

### Relationship management endpoints

Relationships are basic building blocks of a graph. They can be automatically extracted from a document and then added to a graph.

To automatically extract relationships from a document, you can use the following endpoint. Note that you only need to run this once per document to extract both entities and relationships:

| Method | Endpoint                                     | Description                                        |
| ------ | -------------------------------------------- | -------------------------------------------------- |
| POST   | `/documents/{id}/entities_and_relationships` | Extract entities and relationships from a document |

To manage relationships extracted from a document, you can use the following endpoints:

| Method | Endpoint                             | Description                           |
| ------ | ------------------------------------ | ------------------------------------- |
| GET    | `/documents/{id}/relationships`      | Get relationships for a document      |
| GET    | `/documents/{id}/relationships/{id}` | Get a relationship for a document     |
| POST   | `/documents/{id}/relationships`      | Add relationships to a document       |
| POST   | `/documents/{id}/relationships/{id}` | Update a relationship for a document  |
| DELETE | `/documents/{id}/relationships/{id}` | Delete a relationship from a document |

Once entities are added to a graph using the "/graphs/\{id}/\{object\_type}/add" endpoint, you can manage them using the following endpoints:

| Method | Endpoint                          | Description                        |
| ------ | --------------------------------- | ---------------------------------- |
| GET    | `/graphs/{id}/relationships`      | Get relationships for a graph      |
| GET    | `/graphs/{id}/relationships/{id}` | Get a relationship for a graph     |
| POST   | `/graphs/{id}/relationships`      | Add relationships to a graph       |
| POST   | `/graphs/{id}/relationships/{id}` | Update a relationship for a graph  |
| DELETE | `/graphs/{id}/relationships/{id}` | Delete a relationship from a graph |


# Get Relationships

```http
GET https://api.sciphi.ai/v3/graphs/{collection_id}/relationships
```

Lists all relationships in the graph with pagination support.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to list relationships from.

## Query Parameters

- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.list_relationships(collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.listRelationships({
        collectionId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
    });
}

main();

```

```shell
curl https://api.sciphi.ai/v3/graphs/collection_id/relationships \
     -H "Authorization: Bearer <token>"
```

```shell
curl -G https://api.sciphi.ai/v3/graphs/:collection_id/relationships \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d offset=0 \
     -d limit=0
```

# Create Relationship

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/relationships
Content-Type: application/json
```

Creates a new relationship in the graph.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to add the relationship to.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/collection_id/relationships \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "subject": "subject",
  "subject_id": "subject_id",
  "predicate": "predicate",
  "object": "object",
  "object_id": "object_id",
  "description": "description"
}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/relationships \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "subject": "string",
  "subject_id": "string",
  "predicate": "string",
  "object": "string",
  "object_id": "string",
  "description": "string"
}'
```

# Export graph relationships to CSV

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/relationships/export
Content-Type: application/json
```

Export documents as a downloadable CSV file.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The ID of the document to export entities from.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.graphs.export_entities(
    collection_id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    output_path="export.csv",
    columns=["id", "title", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.graphs.exportEntities({
        collectionId: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
        outputPath: "export.csv",
        columns: ["id", "title", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/graphs/export_relationships"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "title", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/relationships/export \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Get Relationship

```http
GET https://api.sciphi.ai/v3/graphs/{collection_id}/relationships/{relationship_id}
```

Retrieves a specific relationship by its ID.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph containing the relationship.
- relationship_id (required): The ID of the relationship to retrieve.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.get_relationship(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
    relationship_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.getRelationship({
        collectionId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
        relationshipId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
    });
}

main();

```

```shell
curl https://api.sciphi.ai/v3/graphs/collection_id/relationships/relationship_id \
     -H "Authorization: Bearer <token>"
```

```shell
curl https://api.sciphi.ai/v3/graphs/:collection_id/relationships/:relationship_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Update Relationship

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/relationships/{relationship_id}
Content-Type: application/json
```

Updates an existing relationship in the graph.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph containing the relationship.
- relationship_id (required): The ID of the relationship to update.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/collection_id/relationships/relationship_id \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/relationships/:relationship_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Delete Relationship

```http
DELETE https://api.sciphi.ai/v3/graphs/{collection_id}/relationships/{relationship_id}
```

Removes a relationship from the graph.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to remove the relationship from.
- relationship_id (required): The ID of the relationship to remove from the graph.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.delete_relationship(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
    relationship_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.deleteRelationship({
        collectionId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
        relationshipId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
    });
}

main();

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/graphs/collection_id/relationships/relationship_id \
     -H "Authorization: Bearer <token>"
```

```shell
curl -X DELETE https://api.sciphi.ai/v3/graphs/:collection_id/relationships/:relationship_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Communities

### Community management endpoints

Communities are groups of entities that are connected to each other. They can be generated using a clustering and summarization algorithm implemented by R2R.

To automatically generate communities from entities and relationships present in the graph, you can use the following endpoint:

| Method | Endpoint                         | Description                                                               |
| ------ | -------------------------------- | ------------------------------------------------------------------------- |
| POST   | `/graphs/{id}/communities/build` | To build communities from entities and relationships present in the graph |

Once communities are generated, you can manage them using the following endpoints. You can also add your own communities to the graph.

| Method | Endpoint                          | Description                        |
| ------ | --------------------------------- | ---------------------------------- |
| GET    | `/graphs/{id}/relationships`      | Get relationships for a graph      |
| GET    | `/graphs/{id}/relationships/{id}` | Get a relationship for a graph     |
| POST   | `/graphs/{id}/relationships`      | Add relationships to a graph       |
| POST   | `/graphs/{id}/relationships/{id}` | Update a relationship for a graph  |
| DELETE | `/graphs/{id}/relationships/{id}` | Delete a relationship from a graph |


# Build Communities

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/communities/build
Content-Type: application/json
```

Creates communities in the graph by analyzing entity
relationships and similarities.

Communities are created through the following process:
1. Analyzes entity relationships and metadata to build a similarity graph
2. Applies advanced community detection algorithms (e.g. Leiden) to identify densely connected groups
3. Creates hierarchical community structure with multiple granularity levels
4. Generates natural language summaries and statistical insights for each community

The resulting communities can be used to:
- Understand high-level graph structure and organization
- Identify key entity groupings and their relationships
- Navigate and explore the graph at different levels of detail
- Generate insights about entity clusters and their characteristics

The community detection process is configurable through settings like:
    - Community detection algorithm parameters
    - Summary generation prompt

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The unique identifier of the collection

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/collection_id/communities/build \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/communities/build \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# List communities

```http
GET https://api.sciphi.ai/v3/graphs/{collection_id}/communities
```

Lists all communities in the graph with pagination support.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to get communities for.

## Query Parameters

- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.list_communities(collection_id="9fbe403b-c11c-5aae-8ade-ef22980c3ad1")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.listCommunities({
        collectionId: "9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
    });
}

main();

```

```shell
curl https://api.sciphi.ai/v3/graphs/collection_id/communities \
     -H "Authorization: Bearer <token>"
```

```shell
curl -G https://api.sciphi.ai/v3/graphs/:collection_id/communities \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d offset=0 \
     -d limit=0
```

# Create a new community

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/communities
Content-Type: application/json
```

Creates a new community in the graph.

While communities are typically built automatically via the /graphs/{id}/communities/build endpoint,
this endpoint allows you to manually create your own communities.

This can be useful when you want to:
- Define custom groupings of entities based on domain knowledge
- Add communities that weren't detected by the automatic process
- Create hierarchical organization structures
- Tag groups of entities with specific metadata

The created communities will be integrated with any existing automatically detected communities
in the graph's community structure.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to create the community in.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.create_community(
    collection_id="9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
    name="My Community",
    summary="A summary of the community",
    findings=["Finding 1", "Finding 2"],
    rating=5,
    rating_explanation="This is a rating explanation",
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.createCommunity({
        collectionId: "9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
        name: "My Community",
        summary: "A summary of the community",
        findings: ["Finding 1", "Finding 2"],
        rating: 5,
        ratingExplanation: "This is a rating explanation",
    });
}

main();

```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/collection_id/communities \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "name": "name",
  "summary": "summary"
}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/communities \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "name": "string",
  "summary": "string"
}'
```

# Retrieve a community

```http
GET https://api.sciphi.ai/v3/graphs/{collection_id}/communities/{community_id}
```

Retrieves a specific community by its ID.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The ID of the collection to get communities for.
- community_id (required): The ID of the community to get.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.get_community(collection_id="9fbe403b-c11c-5aae-8ade-ef22980c3ad1")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.getCommunity({
        collectionId: "9fbe403b-c11c-5aae-8ade-ef22980c3ad1",
    });
}

main();

```

```shell
curl https://api.sciphi.ai/v3/graphs/collection_id/communities/community_id \
     -H "Authorization: Bearer <token>"
```

```shell
curl https://api.sciphi.ai/v3/graphs/:collection_id/communities/:community_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Update community

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/communities/{community_id}
Content-Type: application/json
```

Updates an existing community in the graph.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required)
- community_id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.update_community(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
    community_update={
        "metadata": {
            "topic": "Technology",
            "description": "Tech companies and products"
        }
    }
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

async function main() {
    const response = await client.graphs.updateCommunity({
        collectionId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
        communityId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
        communityUpdate: {
            metadata: {
                topic: "Technology",
                description: "Tech companies and products"
            }
        }
    });
}

main();

```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/collection_id/communities/community_id \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/communities/:community_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Delete a community

```http
DELETE https://api.sciphi.ai/v3/graphs/{collection_id}/communities/{community_id}
```

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to delete the community from.
- community_id (required): The ID of the community to delete.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.delete_community(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
    community_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.deleteCommunity({
        collectionId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
        communityId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
    });
}

main();

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/graphs/collection_id/communities/community_id \
     -H "Authorization: Bearer <token>"
```

```shell
curl -X DELETE https://api.sciphi.ai/v3/graphs/:collection_id/communities/:community_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Export document communities to CSV

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/communities/export
Content-Type: application/json
```

Export documents as a downloadable CSV file.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The ID of the document to export entities from.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.graphs.export_communities(
    collection_id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
    output_path="export.csv",
    columns=["id", "title", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.graphs.exportCommunities({
        collectionId: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
        outputPath: "export.csv",
        columns: ["id", "title", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/graphs/export_communities"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "title", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/communities/export \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# List graphs

```http
GET https://api.sciphi.ai/v3/graphs
```

Returns a paginated list of graphs the authenticated user has
access to.

Results can be filtered by providing specific graph IDs. Regular
users will only see graphs they own or have access to. Superusers
can see all graphs.

The graphs are returned in order of last modification, with most
recent first.

## Request Headers

- X-API-Key (required)

## Query Parameters

- collection_ids (optional): A list of graph IDs to retrieve. If not provided, all graphs will be returned.
- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.list()

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.list({});
}

main();

```

```shell
curl https://api.sciphi.ai/v3/graphs \
     -H "Authorization: Bearer <token>"
```

```shell
curl -G https://api.sciphi.ai/v3/graphs \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d collection_ids=string \
     -d offset=0
```

# Retrieve graph details

```http
GET https://api.sciphi.ai/v3/graphs/{collection_id}
```

Retrieves detailed information about a specific graph by ID.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.get(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.retrieve({
        collectionId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
    });
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/graphs/d09dedb1-b2ab-48a5-b950-6e1f464d83e7" \
    -H "Authorization: Bearer YOUR_API_KEY" 
```

```shell
curl https://api.sciphi.ai/v3/graphs/:collection_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Update graph

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}
Content-Type: application/json
```

Update an existing graphs's configuration.

This endpoint allows updating the name and description of an
existing collection. The user must have appropriate permissions to
modify the collection.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The collection ID corresponding to the graph to update

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.update(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
    graph={
        "name": "New Name",
        "description": "New Description"
    }
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.update({
        collection_id: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
        name: "New Name",
        description: "New Description",
    });
}

main();

```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/collection_id \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Reset a graph back to the initial state.

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/reset
```

Deletes a graph and all its associated data.

This endpoint permanently removes the specified graph along with
all entities and relationships that belong to only this graph. The
original source entities and relationships extracted from
underlying documents are not deleted and are managed through the
document lifecycle.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.reset(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7",
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.graphs.reset({
        collectionId: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/graphs/d09dedb1-b2ab-48a5-b950-6e1f464d83e7/reset" \
    -H "Authorization: Bearer YOUR_API_KEY" 
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/reset \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Pull latest entities to the graph

```http
POST https://api.sciphi.ai/v3/graphs/{collection_id}/pull
Content-Type: application/json
```

Adds documents to a graph by copying their entities and
relationships.

This endpoint:
1. Copies document entities to the graphs_entities table
2. Copies document relationships to the graphs_relationships table
3. Associates the documents with the graph

When a document is added:
- Its entities and relationships are copied to graph-specific tables
- Existing entities/relationships are updated by merging their properties
- The document ID is recorded in the graph's document_ids array

Documents added to a graph will contribute their knowledge to:
- Graph analysis and querying
- Community detection
- Knowledge graph enrichment

The user must have access to both the graph and the documents being added.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_id (required): The ID of the graph to initialize.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

response = client.graphs.pull(
    collection_id="d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

async function main() {
    const response = await client.graphs.pull({
        collection_id: "d09dedb1-b2ab-48a5-b950-6e1f464d83e7"
    });
}

main();

```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/collection_id/pull \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json"
```

```shell
curl -X POST https://api.sciphi.ai/v3/graphs/:collection_id/pull \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json"
```

# Indices

An `Index` in R2R represents a vector index structure that optimizes similarity search operations across chunks. Indices are crucial for efficient retrieval in RAG applications, supporting various similarity measures and index types optimized for different use cases.

Indices in R2R provide:

* Fast similarity search capabilities
* Multiple index method options (HNSW, IVF-Flat)
* Configurable similarity measures
* Concurrent index building
* Performance optimization for vector operations

## Available Endpoints

| Method | Endpoint        | Description                              |
| ------ | --------------- | ---------------------------------------- |
| POST   | `/indices`      | Create a new vector index                |
| GET    | `/indices`      | List available indices with pagination   |
| GET    | `/indices/{id}` | Get details of a specific index          |
| PUT    | `/indices/{id}` | Update an existing index's configuration |
| DELETE | `/indices/{id}` | Delete an existing index                 |


# List Vector Indices

```http
GET https://api.sciphi.ai/v3/indices
```

List existing vector similarity search indices with pagination
support.

Returns details about each index including:
- Name and table name
- Indexing method and parameters
- Size and row count
- Creation timestamp and last updated
- Performance statistics (if available)

The response can be filtered using the filter_by parameter to narrow down results
based on table name, index method, or other attributes.

## Request Headers

- X-API-Key (required)

## Query Parameters

- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()

# List all indices
indices = client.indices.list(
    offset=0,
    limit=10
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.indicies.list({
        offset: 0,
        limit: 10,
        filters: { table_name: "vectors" }
}

main();

```

```shell Shell

curl -X GET "https://api.example.com/indices?offset=0&amp;limit=10" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -H "Content-Type: application/json"

# With filters
curl -X GET "https://api.example.com/indices?offset=0&amp;limit=10&amp;filters={"table_name":"vectors"}" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -H "Content-Type: application/json"

```

```shell
curl https://api.sciphi.ai/v3/indices \
     -H "Authorization: Bearer <token>"
```

```shell
curl -G https://api.sciphi.ai/v3/indices \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d offset=0 \
     -d limit=0
```

# Create Vector Index

```http
POST https://api.sciphi.ai/v3/indices
Content-Type: application/json
```

Create a new vector similarity search index in over the target
table. Allowed tables include 'vectors', 'entity',
'document_collections'. Vectors correspond to the chunks of text
that are indexed for similarity search, whereas entity and
document_collections are created during knowledge graph
construction.

This endpoint creates a database index optimized for efficient similarity search over vector embeddings.
It supports two main indexing methods:

1. HNSW (Hierarchical Navigable Small World):
   - Best for: High-dimensional vectors requiring fast approximate nearest neighbor search
   - Pros: Very fast search, good recall, memory-resident for speed
   - Cons: Slower index construction, more memory usage
   - Key parameters:
     * m: Number of connections per layer (higher = better recall but more memory)
     * ef_construction: Build-time search width (higher = better recall but slower build)
     * ef: Query-time search width (higher = better recall but slower search)

2. IVF-Flat (Inverted File with Flat Storage):
   - Best for: Balance between build speed, search speed, and recall
   - Pros: Faster index construction, less memory usage
   - Cons: Slightly slower search than HNSW
   - Key parameters:
     * lists: Number of clusters (usually sqrt(n) where n is number of vectors)
     * probe: Number of nearest clusters to search

Supported similarity measures:
- cosine_distance: Best for comparing semantic similarity
- l2_distance: Best for comparing absolute distances
- ip_distance: Best for comparing raw dot products

Notes:
- Index creation can be resource-intensive for large datasets
- Use run_with_orchestration=True for large indices to prevent timeouts
- The 'concurrently' option allows other operations while building
- Index names must be unique per table

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

# Create an HNSW index for efficient similarity search
result = client.indices.create(
    config={
        "table_name": "chunks",  # The table containing vector embeddings
        "index_method": "hnsw",   # Hierarchical Navigable Small World graph
        "index_measure": "cosine_distance",  # Similarity measure
        "index_arguments": {
            "m": 16,              # Number of connections per layer
            "ef_construction": 64,# Size of dynamic candidate list for construction
            "ef": 40,            # Size of dynamic candidate list for search
        },
        "index_name": "my_document_embeddings_idx",
        "index_column": "embedding",
        "concurrently": True     # Build index without blocking table writes
    },
    run_with_orchestration=True  # Run as orchestrated task for large indices
)

# Create an IVF-Flat index for balanced performance
result = client.indices.create(
    config={
        "table_name": "chunks",
        "index_method": "ivf_flat", # Inverted File with Flat storage
        "index_measure": "l2_distance",
        "index_arguments": {
            "lists": 100,         # Number of cluster centroids
            "probe": 10,          # Number of clusters to search
        },
        "index_name": "my_ivf_embeddings_idx",
        "index_column": "embedding",
        "concurrently": True
    }
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.indicies.create({
        config: {
            tableName: "vectors",
            indexMethod: "hnsw",
            indexMeasure: "cosine_distance",
            indexArguments: {
                m: 16,
                ef_construction: 64,
                ef: 40
            },
            indexName: "my_document_embeddings_idx",
            indexColumn: "embedding",
            concurrently: true
        },
        runWithOrchestration: true
    });
}

main();

```

```shell Shell

# Create HNSW Index
curl -X POST "https://api.example.com/indices" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "config": {
        "table_name": "vectors",
        "index_method": "hnsw",
        "index_measure": "cosine_distance",
        "index_arguments": {
        "m": 16,
        "ef_construction": 64,
        "ef": 40
        },
        "index_name": "my_document_embeddings_idx",
        "index_column": "embedding",
        "concurrently": true
    },
    "run_with_orchestration": true
    }'

# Create IVF-Flat Index
curl -X POST "https://api.example.com/indices" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d '{
    "config": {
        "table_name": "vectors",
        "index_method": "ivf_flat",
        "index_measure": "l2_distance",
        "index_arguments": {
        "lists": 100,
        "probe": 10
        },
        "index_name": "my_ivf_embeddings_idx",
        "index_column": "embedding",
        "concurrently": true
    }
    }'

```

```shell
curl -X POST https://api.sciphi.ai/v3/indices \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "config": {}
}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/indices \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "config": {}
}'
```

# Get Vector Index Details

```http
GET https://api.sciphi.ai/v3/indices/{table_name}/{index_name}
```

Get detailed information about a specific vector index.

Returns comprehensive information about the index including:
- Configuration details (method, measure, parameters)
- Current size and row count
- Build progress (if still under construction)
- Performance statistics:
    * Average query time
    * Memory usage
    * Cache hit rates
    * Recent query patterns
- Maintenance information:
    * Last vacuum
    * Fragmentation level
    * Recommended optimizations

## Request Headers

- X-API-Key (required)

## Path Parameters

- table_name (required): The table of vector embeddings to delete (e.g. `vectors`, `entity`, `document_collections`)
- index_name (required): The name of the index to delete

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()

# Get detailed information about a specific index
index = client.indices.retrieve("index_1")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.indicies.retrieve({
        indexName: "index_1",
        tableName: "vectors"
    });

    console.log(response);
}

main();

```

```shell Shell

curl -X GET "https://api.example.com/indices/vectors/index_1" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl https://api.sciphi.ai/v3/indices/chunks/index_name \
     -H "Authorization: Bearer <token>"
```

```shell
curl https://api.sciphi.ai/v3/indices/chunks/:index_name \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Delete Vector Index

```http
DELETE https://api.sciphi.ai/v3/indices/{table_name}/{index_name}
```

Delete an existing vector similarity search index.

This endpoint removes the specified index from the database. Important considerations:

- Deletion is permanent and cannot be undone
- Underlying vector data remains intact
- Queries will fall back to sequential scan
- Running queries during deletion may be slower
- Use run_with_orchestration=True for large indices to prevent timeouts
- Consider index dependencies before deletion

The operation returns immediately but cleanup may continue in background.

## Request Headers

- X-API-Key (required)

## Path Parameters

- table_name (required): The table of vector embeddings to delete (e.g. `vectors`, `entity`, `document_collections`)
- index_name (required): The name of the index to delete

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()

# Delete an index with orchestration for cleanup
result = client.indices.delete(
    index_name="index_1",
    table_name="vectors",
    run_with_orchestration=True
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.indicies.delete({
        indexName: "index_1"
        tableName: "vectors"
    });

    console.log(response);
}

main();

```

```shell Shell

curl -X DELETE "https://api.example.com/indices/index_1" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/indices/chunks/index_name \
     -H "Authorization: Bearer <token>"
```

```shell
curl -X DELETE https://api.sciphi.ai/v3/indices/chunks/:index_name \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Users

A `User` in R2R represents an authenticated entity that can interact with the system. Users are the foundation of R2R's access control system, enabling granular permissions management, activity tracking, and content organization through collections.

Users in R2R provide:

* Authentication and authorization
* Collection membership management
* Activity tracking and analytics
* Metadata customization
* Superuser capabilities for system administration

## Available Endpoints

| Method | Endpoint                                       | Description                                                |
| ------ | ---------------------------------------------- | ---------------------------------------------------------- |
| GET    | `/users`                                       | List users with pagination and filtering (superusers only) |
| GET    | `/users/{user_id}`                             | Get detailed user information                              |
| GET    | `/users/{user_id}/collections`                 | List user's collections                                    |
| POST   | `/users/{user_id}/collections/{collection_id}` | Add user to collection                                     |
| DELETE | `/users/{user_id}/collections/{collection_id}` | Remove user from collection                                |
| POST   | `/users/{user_id}`                             | Update user information                                    |


# List Users

```http
GET https://api.sciphi.ai/v3/users
```

List all users with pagination and filtering options.

Only accessible by superusers.

## Request Headers

- X-API-Key (required)

## Query Parameters

- ids (optional): List of user IDs to filter by
- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

# List users with filters
users = client.users.list(
    offset=0,
    limit=100,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.list();
}

main();

```

```shell Shell

curl -X GET "https://api.example.com/users?offset=0&amp;limit=100&amp;username=john&amp;email=john@example.com&amp;is_active=true&amp;is_superuser=false" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl https://api.sciphi.ai/v3/users \
     -H "Authorization: Bearer <token>"
```

```shell
curl -G https://api.sciphi.ai/v3/users \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d ids=string \
     -d offset=0
```

# Register

```http
POST https://api.sciphi.ai/v3/users
Content-Type: application/json
```

Register a new user with the given email and password.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
new_user = client.users.create(
    email="jane.doe@example.com",
    password="secure_password123"
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.create({
        email: "jane.doe@example.com",
        password: "secure_password123"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/users" \
    -H "Content-Type: application/json" \
    -d '{
        "email": "jane.doe@example.com",
        "password": "secure_password123"
    }'
```

```shell
curl -X POST https://api.sciphi.ai/v3/users \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "email": "string",
  "password": "string"
}'
```

# Export users to CSV

```http
POST https://api.sciphi.ai/v3/users/export
Content-Type: application/json
```

Export users as a CSV file.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.users.export(
    output_path="export.csv",
    columns=["id", "name", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.users.export({
        outputPath: "export.csv",
        columns: ["id", "name", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/users/export"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "name", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/users/export \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Verify user's email address

```http
POST https://api.sciphi.ai/v3/users/verify-email
Content-Type: application/json
```

Verify a user's email address.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
tokens = client.users.verify_email(
    email="jane.doe@example.com",
    verification_code="1lklwal!awdclm"
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.verifyEmail({
        email: jane.doe@example.com",
        verificationCode: "1lklwal!awdclm"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/users/login" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "email=jane.doe@example.com&amp;verification_code=1lklwal!awdclm"

```

```shell
curl -X POST https://api.sciphi.ai/v3/users/verify-email \
     -H "X-API-Key: string" \
     -H "Content-Type: application/json" \
     -d '{
  "email": "string",
  "verification_code": "string"
}'
```

# Send Verification Email

```http
POST https://api.sciphi.ai/v3/users/send-verification-email
Content-Type: application/json
```

Send a user's email a verification code.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
tokens = client.users.send_verification_email(
    email="jane.doe@example.com",
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.sendVerificationEmail({
        email: jane.doe@example.com",
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/users/send-verification-email" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "email=jane.doe@example.com"

```

```shell
curl -X POST https://api.sciphi.ai/v3/users/send-verification-email \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d "string"
```

# Authenticate user and get tokens

```http
POST https://api.sciphi.ai/v3/users/login
Content-Type: application/x-www-form-urlencoded
```

Authenticate a user and provide access tokens.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
tokens = client.users.login(
    email="jane.doe@example.com",
    password="secure_password123"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.login({
        email: jane.doe@example.com",
        password: "secure_password123"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/users/login" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=jane.doe@example.com&amp;password=secure_password123"

```

```shell
curl -X POST https://api.sciphi.ai/v3/users/login \
     -H "X-API-Key: string" \
     -H "Content-Type: application/x-www-form-urlencoded" \
     -d username=string \
     -d password=string
```

# Log out current user

```http
POST https://api.sciphi.ai/v3/users/logout
```

Log out the current user.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)
result = client.users.logout()

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.logout();
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/users/logout" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

# Refresh access token

```http
POST https://api.sciphi.ai/v3/users/refresh-token
Content-Type: application/json
```

Refresh the access token using a refresh token.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

new_tokens = client.users.refresh_token()
# New tokens are automatically stored in the client
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.refreshAccessToken();
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/users/refresh-token" \
    -H "Content-Type: application/json" \
    -d '{
        "refresh_token": "YOUR_REFRESH_TOKEN"
    }'
```

```shell
curl -X POST https://api.sciphi.ai/v3/users/refresh-token \
     -H "X-API-Key: string" \
     -H "Content-Type: application/json" \
     -d "string"
```

# Change user password

```http
POST https://api.sciphi.ai/v3/users/change-password
Content-Type: application/json
```

Change the authenticated user's password.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

result = client.users.change_password(
    current_password="old_password123",
    new_password="new_secure_password456"
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.changePassword({
        currentPassword: "old_password123",
        newPassword: "new_secure_password456"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/users/change-password" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "current_password": "old_password123",
        "new_password": "new_secure_password456"
    }'
```

```shell
curl -X POST https://api.sciphi.ai/v3/users/change-password \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "current_password": "string",
  "new_password": "string"
}'
```

# Request password reset

```http
POST https://api.sciphi.ai/v3/users/request-password-reset
Content-Type: application/json
```

Request a password reset for a user.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
result = client.users.request_password_reset(
    email="jane.doe@example.com"
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.requestPasswordReset({
        email: jane.doe@example.com",
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/users/request-password-reset" \
    -H "Content-Type: application/json" \
    -d '{
        "email": "jane.doe@example.com"
    }'
```

```shell
curl -X POST https://api.sciphi.ai/v3/users/request-password-reset \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d "string"
```

# Reset password with token

```http
POST https://api.sciphi.ai/v3/users/reset-password
Content-Type: application/json
```

Reset a user's password using a reset token.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
result = client.users.reset_password(
    reset_token="reset_token_received_via_email",
    new_password="new_secure_password789"
)
```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.resetPassword({
        resestToken: "reset_token_received_via_email",
        newPassword: "new_secure_password789"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/users/reset-password" \
    -H "Content-Type: application/json" \
    -d '{
        "reset_token": "reset_token_received_via_email",
        "new_password": "new_secure_password789"
    }'
```

```shell
curl -X POST https://api.sciphi.ai/v3/users/reset-password \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "reset_token": "string",
  "new_password": "string"
}'
```

# Get authenticated user details

```http
GET https://api.sciphi.ai/v3/users/me
```

Get detailed information about the currently authenticated
user.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

# Get user details
users = client.users.me()

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.me();
}

main();

```

```shell Shell

curl -X GET "https://api.example.com/users/me" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl https://api.sciphi.ai/v3/users/me \
     -H "Authorization: Bearer <token>"
```

# Get user details

```http
GET https://api.sciphi.ai/v3/users/{id}
```

Get detailed information about a specific user.

Users can only access their own information unless they are
superusers.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

# Get user details
users = client.users.retrieve(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.retrieve({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
    });
}

main();

```

```shell Shell

curl -X GET "https://api.example.com/users/550e8400-e29b-41d4-a716-446655440000" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl https://api.sciphi.ai/v3/users/550e8400-e29b-41d4-a716-446655440000 \
     -H "Authorization: Bearer <token>"
```

```shell
curl https://api.sciphi.ai/v3/users/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Update user information

```http
POST https://api.sciphi.ai/v3/users/{id}
Content-Type: application/json
```

Update user information.

Users can only update their own information unless they are
superusers. Superuser status can only be modified by existing
superusers.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): ID of the user to update

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

# Update user
updated_user = client.update_user(
    "550e8400-e29b-41d4-a716-446655440000",
    name="John Doe"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.update({
        id: "550e8400-e29b-41d4-a716-446655440000",
        name: "John Doe"
    });
}

main();

```

```shell Shell

curl -X POST "https://api.example.com/users/550e8400-e29b-41d4-a716-446655440000" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "id": "550e8400-e29b-41d4-a716-446655440000",
        "name": "John Doe",
    }'

```

```shell
curl -X POST https://api.sciphi.ai/v3/users/id \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/users/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Delete user

```http
DELETE https://api.sciphi.ai/v3/users/{id}
Content-Type: application/json
```

Delete a specific user.

Users can only delete their own account unless they are superusers.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

# Delete user
client.users.delete(id="550e8400-e29b-41d4-a716-446655440000", password="secure_password123")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.delete({
        id: "550e8400-e29b-41d4-a716-446655440000",
        password: "secure_password123"
    });
}

main();

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/users/550e8400-e29b-41d4-a716-446655440000 \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

```shell
curl -X DELETE https://api.sciphi.ai/v3/users/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# List user's collections

```http
GET https://api.sciphi.ai/v3/users/{id}/collections
```

Get all collections associated with a specific user.

Users can only access their own collections unless they are
superusers.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required)

## Query Parameters

- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

# Get user collections
collections = client.user.list_collections(
    "550e8400-e29b-41d4-a716-446655440000",
    offset=0,
    limit=100
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.listCollections({
        id: "550e8400-e29b-41d4-a716-446655440000",
        offset: 0,
        limit: 100
    });
}

main();

```

```shell Shell

curl -X GET "https://api.example.com/users/550e8400-e29b-41d4-a716-446655440000/collections?offset=0&amp;limit=100" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl https://api.sciphi.ai/v3/users/550e8400-e29b-41d4-a716-446655440000/collections \
     -H "Authorization: Bearer <token>"
```

```shell
curl -G https://api.sciphi.ai/v3/users/:id/collections \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d offset=0 \
     -d limit=0
```

# Add user to collection

```http
POST https://api.sciphi.ai/v3/users/{id}/collections/{collection_id}
```

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required)
- collection_id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

# Add user to collection
client.users.add_to_collection(
    id="550e8400-e29b-41d4-a716-446655440000",
    collection_id="750e8400-e29b-41d4-a716-446655440000"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.addToCollection({
        id: "550e8400-e29b-41d4-a716-446655440000",
        collectionId: "750e8400-e29b-41d4-a716-446655440000"
    });
}

main();

```

```shell Shell

curl -X POST "https://api.example.com/users/550e8400-e29b-41d4-a716-446655440000/collections/750e8400-e29b-41d4-a716-446655440000" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X POST https://api.sciphi.ai/v3/users/550e8400-e29b-41d4-a716-446655440000/collections/750e8400-e29b-41d4-a716-446655440000 \
     -H "Authorization: Bearer <token>"
```

```shell
curl -X POST https://api.sciphi.ai/v3/users/:id/collections/:collection_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Remove user from collection

```http
DELETE https://api.sciphi.ai/v3/users/{id}/collections/{collection_id}
```

Remove a user from a collection.

Requires either superuser status or access to the collection.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required)
- collection_id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

# Remove user from collection
client.users.remove_from_collection(
    id="550e8400-e29b-41d4-a716-446655440000",
    collection_id="750e8400-e29b-41d4-a716-446655440000"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.users.removeFromCollection({
        id: "550e8400-e29b-41d4-a716-446655440000",
        collectionId: "750e8400-e29b-41d4-a716-446655440000"
    });
}

main();

```

```shell Shell

curl -X DELETE "https://api.example.com/users/550e8400-e29b-41d4-a716-446655440000/collections/750e8400-e29b-41d4-a716-446655440000" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/users/550e8400-e29b-41d4-a716-446655440000/collections/750e8400-e29b-41d4-a716-446655440000 \
     -H "Authorization: Bearer <token>"
```

```shell
curl -X DELETE https://api.sciphi.ai/v3/users/:id/collections/:collection_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# List User API Keys

```http
GET https://api.sciphi.ai/v3/users/{id}/api-keys
```

List all API keys for the specified user.

Only superusers or the user themselves may list the API keys.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): ID of the user whose API keys to list

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

keys = client.users.list_api_keys(
    id="550e8400-e29b-41d4-a716-446655440000"
)

```

```shell cURL

curl -X GET "https://api.example.com/users/550e8400-e29b-41d4-a716-446655440000/api-keys" \
    -H "Authorization: Bearer YOUR_API_TOKEN"

```

```shell
curl https://api.sciphi.ai/v3/users/:id/api-keys \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Create User API Key

```http
POST https://api.sciphi.ai/v3/users/{id}/api-keys
Content-Type: application/json
```

Create a new API key for the specified user.

Only superusers or the user themselves may create an API key.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): ID of the user for whom to create an API key

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# client.login(...)

result = client.users.create_api_key(
    id="550e8400-e29b-41d4-a716-446655440000",
    name="My API Key",
    description="API key for accessing the app",
)
# result["api_key"] contains the newly created API key

```

```shell cURL

curl -X POST "https://api.example.com/users/550e8400-e29b-41d4-a716-446655440000/api-keys" \
    -H "Authorization: Bearer YOUR_API_TOKEN" \
    -d '{"name": "My API Key", "description": "API key for accessing the app"}'

```

```shell
curl -X POST https://api.sciphi.ai/v3/users/:id/api-keys \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Delete User API Key

```http
DELETE https://api.sciphi.ai/v3/users/{id}/api-keys/{key_id}
```

Delete a specific API key for the specified user.

Only superusers or the user themselves may delete the API key.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): ID of the user
- key_id (required): ID of the API key to delete

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient
from uuid import UUID

client = R2RClient()
# client.login(...)

response = client.users.delete_api_key(
    id="550e8400-e29b-41d4-a716-446655440000",
    key_id="d9c562d4-3aef-43e8-8f08-0cf7cd5e0a25"
)

```

```shell cURL

curl -X DELETE "https://api.example.com/users/550e8400-e29b-41d4-a716-446655440000/api-keys/d9c562d4-3aef-43e8-8f08-0cf7cd5e0a25" \
    -H "Authorization: Bearer YOUR_API_TOKEN"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/users/:id/api-keys/:key_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Fetch User Limits

```http
GET https://api.sciphi.ai/v3/users/{id}/limits
```

Return the system default limits, user-level overrides, and
final "effective" limit settings for the specified user.

Only superusers or the user themself may fetch these values.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): ID of the user to fetch limits for

## Response Body

- 200: Returns system default limits, user overrides, and final effective settings.
- 403: If the requesting user is neither the same user nor a superuser.
- 404: If the user ID does not exist.
- 422: Validation Error

## Examples

```python Python

                            from r2r import R2RClient

                            client = R2RClient()
                            # client.login(...)

                            user_limits = client.users.get_limits("550e8400-e29b-41d4-a716-446655440000")
                        
```

```javascript JavaScript

                            const { r2rClient } = require("r2r-js");

                            const client = new r2rClient();
                            // await client.users.login(...)

                            async function main() {
                                const userLimits = await client.users.getLimits({
                                    id: "550e8400-e29b-41d4-a716-446655440000"
                                });
                                console.log(userLimits);
                            }

                            main();
                        
```

```shell cURL

                            curl -X GET "https://api.example.com/v3/users/550e8400-e29b-41d4-a716-446655440000/limits" \
                                -H "Authorization: Bearer YOUR_API_KEY"
                        
```

```shell
curl https://api.sciphi.ai/v3/users/:id/limits \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

```shell
curl https://api.sciphi.ai/v3/users/:id/limits \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

```shell
curl https://api.sciphi.ai/v3/users/:id/limits \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Google Authorize

```http
GET https://api.sciphi.ai/v3/users/oauth/google/authorize
```

Redirect user to Google's OAuth 2.0 consent screen.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response

## Examples

```shell
curl https://api.sciphi.ai/v3/users/oauth/google/authorize
```

# Google Callback

```http
GET https://api.sciphi.ai/v3/users/oauth/google/callback
```

Google's callback that will receive the `code` and `state`.

We then exchange code for tokens, verify, and log the user in.

## Request Headers

- X-API-Key (required)

## Query Parameters

- code (required)
- state (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```shell
curl -G https://api.sciphi.ai/v3/users/oauth/google/callback \
     -d code=code \
     -d state=state
```

```shell
curl -G https://api.sciphi.ai/v3/users/oauth/google/callback \
     -H "X-API-Key: string" \
     -d code=string \
     -d state=string
```

# Github Authorize

```http
GET https://api.sciphi.ai/v3/users/oauth/github/authorize
```

Redirect user to GitHub's OAuth consent screen.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response

## Examples

```shell
curl https://api.sciphi.ai/v3/users/oauth/github/authorize
```

# Github Callback

```http
GET https://api.sciphi.ai/v3/users/oauth/github/callback
```

GitHub callback route to exchange code for an access_token, then
fetch user info from GitHub's API, then do the same 'oauth-based'
login or registration.

## Request Headers

- X-API-Key (required)

## Query Parameters

- code (required)
- state (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```shell
curl -G https://api.sciphi.ai/v3/users/oauth/github/callback \
     -d code=code \
     -d state=state
```

```shell
curl -G https://api.sciphi.ai/v3/users/oauth/github/callback \
     -H "X-API-Key: string" \
     -d code=string \
     -d state=string
```

# Collections

A `Collection` in R2R is a logical grouping mechanism that enables organization and access control for documents and their associated chunks. Collections serve as a fundamental unit for managing permissions, sharing content, and organizing related documents across users and teams.

Collections in R2R provide:

* Organizational structure for documents
* Access control and permissions management
* Group-based content sharing
* Document categorization and management
* User collaboration capabilities

## Available Endpoints

| Method | Endpoint                                    | Description                                                                         |
| ------ | ------------------------------------------- | ----------------------------------------------------------------------------------- |
| POST   | `/collections`                              | Create a new collection                                                             |
| GET    | `/collections`                              | List collections with pagination and filtering                                      |
| GET    | `/collections/{id}`                         | Get details of a specific collection                                                |
| POST   | `/collections/{id}`                         | Update an existing collection                                                       |
| DELETE | `/collections/{id}`                         | Delete an existing collection                                                       |
| GET    | `/collections/{id}/documents`               | List documents in a collection                                                      |
| POST   | `/collections/{id}/documents/{document_id}` | Add a document to a collection                                                      |
| POST   | `/collections/{id}/extract`                 | Extracts entities and relationships for all unextracted documents in the collection |
| DELETE | `/collections/{id}/documents/{document_id}` | Remove a document from a collection                                                 |
| GET    | `/collections/{id}/users`                   | List users with access to a collection                                              |
| POST   | `/collections/{id}/users/{user_id}`         | Add a user to a collection                                                          |
| DELETE | `/collections/{id}/users/{user_id}`         | Remove a user from a collection                                                     |


# List collections

```http
GET https://api.sciphi.ai/v3/collections
```

Returns a paginated list of collections the authenticated user
has access to.

Results can be filtered by providing specific collection IDs.
Regular users will only see collections they own or have access to.
Superusers can see all collections.

The collections are returned in order of last modification, with
most recent first.

## Request Headers

- X-API-Key (required)

## Query Parameters

- ids (optional): A list of collection IDs to retrieve. If not provided, all collections will be returned.
- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.
- owner_only (optional): If true, only returns collections owned by the user, not all accessible collections.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.list(
    offset=0,
    limit=10,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.list();
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/collections?offset=0&amp;limit=10&amp;name=Sample" \
     -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -G https://api.sciphi.ai/v3/collections \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d ids=string \
     -d offset=0
```

# Create a new collection

```http
POST https://api.sciphi.ai/v3/collections
Content-Type: application/json
```

Create a new collection and automatically add the creating user
to it.

This endpoint allows authenticated users to create a new collection
with a specified name and optional description. The user creating
the collection is automatically added as a member.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.create(
    name="My New Collection",
    description="This is a sample collection"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.create({
        name: "My New Collection",
        description: "This is a sample collection"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/collections" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_API_KEY" \
     -d '{"name": "My New Collection", "description": "This is a sample collection"}'

```

```shell
curl -X POST https://api.sciphi.ai/v3/collections \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "name": "string"
}'
```

# Export collections to CSV

```http
POST https://api.sciphi.ai/v3/collections/export
Content-Type: application/json
```

Export collections as a CSV file.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.collections.export(
    output_path="export.csv",
    columns=["id", "name", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.collections.export({
        outputPath: "export.csv",
        columns: ["id", "name", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/collections/export"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "name", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/collections/export \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Get collection details

```http
GET https://api.sciphi.ai/v3/collections/{id}
```

Get details of a specific collection.

This endpoint retrieves detailed information about a single
collection identified by its UUID. The user must have access to the
collection to view its details.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the collection

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.retrieve("123e4567-e89b-12d3-a456-426614174000")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.retrieve({id: "123e4567-e89b-12d3-a456-426614174000"});
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/collections/123e4567-e89b-12d3-a456-426614174000" \
     -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl https://api.sciphi.ai/v3/collections/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Update collection

```http
POST https://api.sciphi.ai/v3/collections/{id}
Content-Type: application/json
```

Update an existing collection's configuration.

This endpoint allows updating the name and description of an
existing collection. The user must have appropriate permissions to
modify the collection.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the collection to update

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.update(
    "123e4567-e89b-12d3-a456-426614174000",
    name="Updated Collection Name",
    description="Updated description"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.update({
        id: "123e4567-e89b-12d3-a456-426614174000",
        name: "Updated Collection Name",
        description: "Updated description"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/collections/123e4567-e89b-12d3-a456-426614174000" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_API_KEY" \
     -d '{"name": "Updated Collection Name", "description": "Updated description"}'

```

```shell
curl -X POST https://api.sciphi.ai/v3/collections/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Delete collection

```http
DELETE https://api.sciphi.ai/v3/collections/{id}
```

Delete an existing collection.

This endpoint allows deletion of a collection identified by its
UUID. The user must have appropriate permissions to delete the
collection. Deleting a collection removes all associations but does
not delete the documents within it.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the collection to delete

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.delete("123e4567-e89b-12d3-a456-426614174000")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.delete({id: "123e4567-e89b-12d3-a456-426614174000"});
}

main();

```

```shell cURL

curl -X DELETE "https://api.example.com/v3/collections/123e4567-e89b-12d3-a456-426614174000" \
     -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/collections/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Add document to collection

```http
POST https://api.sciphi.ai/v3/collections/{id}/documents/{document_id}
```

Add a document to a collection.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required)
- document_id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.add_document(
    "123e4567-e89b-12d3-a456-426614174000",
    "456e789a-b12c-34d5-e678-901234567890"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.addDocument({
        id: "123e4567-e89b-12d3-a456-426614174000"
        documentId: "456e789a-b12c-34d5-e678-901234567890"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/collections/123e4567-e89b-12d3-a456-426614174000/documents/456e789a-b12c-34d5-e678-901234567890" \
     -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X POST https://api.sciphi.ai/v3/collections/:id/documents/:document_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Remove document from collection

```http
DELETE https://api.sciphi.ai/v3/collections/{id}/documents/{document_id}
```

Remove a document from a collection.

This endpoint removes the association between a document and a
collection. It does not delete the document itself. The user must
have permissions to modify the collection.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the collection
- document_id (required): The unique identifier of the document to remove

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.remove_document(
    "123e4567-e89b-12d3-a456-426614174000",
    "456e789a-b12c-34d5-e678-901234567890"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.removeDocument({
        id: "123e4567-e89b-12d3-a456-426614174000"
        documentId: "456e789a-b12c-34d5-e678-901234567890"
    });
}

main();

```

```shell cURL

curl -X DELETE "https://api.example.com/v3/collections/123e4567-e89b-12d3-a456-426614174000/documents/456e789a-b12c-34d5-e678-901234567890" \
     -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/collections/:id/documents/:document_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# List documents in collection

```http
GET https://api.sciphi.ai/v3/collections/{id}/documents
```

Get all documents in a collection with pagination and sorting
options.

This endpoint retrieves a paginated list of documents associated
with a specific collection. It supports sorting options to
customize the order of returned documents.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the collection

## Query Parameters

- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.list_documents(
    "123e4567-e89b-12d3-a456-426614174000",
    offset=0,
    limit=10,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.listDocuments({id: "123e4567-e89b-12d3-a456-426614174000"});
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/collections/123e4567-e89b-12d3-a456-426614174000/documents?offset=0&amp;limit=10" \
     -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -G https://api.sciphi.ai/v3/collections/:id/documents \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d offset=0 \
     -d limit=0
```

# List users in collection

```http
GET https://api.sciphi.ai/v3/collections/{id}/users
```

Get all users in a collection with pagination and sorting
options.

This endpoint retrieves a paginated list of users who have access
to a specific collection. It supports sorting options to customize
the order of returned users.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the collection

## Query Parameters

- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.list_users(
    "123e4567-e89b-12d3-a456-426614174000",
    offset=0,
    limit=10,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.listUsers({
        id: "123e4567-e89b-12d3-a456-426614174000"
    });
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/collections/123e4567-e89b-12d3-a456-426614174000/users?offset=0&amp;limit=10" \
     -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -G https://api.sciphi.ai/v3/collections/:id/users \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d offset=0 \
     -d limit=0
```

# Add user to collection

```http
POST https://api.sciphi.ai/v3/collections/{id}/users/{user_id}
```

Add a user to a collection.

This endpoint grants a user access to a specific collection. The
authenticated user must have admin permissions for the collection
to add new users.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the collection
- user_id (required): The unique identifier of the user to add

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.add_user(
    "123e4567-e89b-12d3-a456-426614174000",
    "789a012b-c34d-5e6f-g789-012345678901"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.addUser({
        id: "123e4567-e89b-12d3-a456-426614174000"
        userId: "789a012b-c34d-5e6f-g789-012345678901"
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/collections/123e4567-e89b-12d3-a456-426614174000/users/789a012b-c34d-5e6f-g789-012345678901" \
     -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X POST https://api.sciphi.ai/v3/collections/:id/users/:user_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Remove user from collection

```http
DELETE https://api.sciphi.ai/v3/collections/{id}/users/{user_id}
```

Remove a user from a collection.

This endpoint revokes a user's access to a specific collection. The
authenticated user must have admin permissions for the collection
to remove users.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the collection
- user_id (required): The unique identifier of the user to remove

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.collections.remove_user(
    "123e4567-e89b-12d3-a456-426614174000",
    "789a012b-c34d-5e6f-g789-012345678901"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.collections.removeUser({
        id: "123e4567-e89b-12d3-a456-426614174000"
        userId: "789a012b-c34d-5e6f-g789-012345678901"
    });
}

main();

```

```shell cURL

curl -X DELETE "https://api.example.com/v3/collections/123e4567-e89b-12d3-a456-426614174000/users/789a012b-c34d-5e6f-g789-012345678901" \
     -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/collections/:id/users/:user_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Extract entities and relationships

```http
POST https://api.sciphi.ai/v3/collections/{id}/extract
Content-Type: application/json
```

Extracts entities and relationships from a document.

The entities and relationships extraction process involves:
1. Parsing documents into semantic chunks
2. Extracting entities and relationships using LLMs

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The ID of the document to extract entities and relationships from.

## Query Parameters

- run_with_orchestration (optional): Whether to run the entities and relationships extraction process with orchestration.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.documents.extract(
    id="9fbe403b-c11c-5aae-8ade-ef22980c3ad1"
)

```

```shell
curl -X POST https://api.sciphi.ai/v3/collections/id/extract \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json"
```

```shell
curl -X POST "https://api.sciphi.ai/v3/collections/:id/extract?run_with_orchestration=true" \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json"
```

# Get a collection by name

```http
GET https://api.sciphi.ai/v3/collections/name/{collection_name}
```

Retrieve a collection by its (owner_id, name) combination.

The authenticated user can only fetch collections they own, or, if
superuser, from anyone.

## Request Headers

- X-API-Key (required)

## Path Parameters

- collection_name (required): The name of the collection

## Query Parameters

- owner_id (optional): (Superuser only) Specify the owner_id to retrieve a collection by name

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```shell
curl https://api.sciphi.ai/v3/collections/name/collection_name \
     -H "Authorization: Bearer <token>"
```

```shell
curl -G https://api.sciphi.ai/v3/collections/name/:collection_name \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d owner_id=string
```

# Conversations

A `Conversation` in R2R represents a threaded exchange of messages that can branch into multiple paths. Conversations provide a structured way to maintain dialogue history, support branching discussions, and manage message flows.

Conversations in R2R provide:

* Threaded message management
* Branching conversation paths
* Message editing with history preservation
* Metadata attachment capabilities
* Conversational context maintenance

## Available Endpoints

| Method | Endpoint                                    | Description                        |
| ------ | ------------------------------------------- | ---------------------------------- |
| POST   | `/conversations`                            | Create a new conversation          |
| GET    | `/conversations`                            | List conversations with pagination |
| GET    | `/conversations/{id}`                       | Get conversation details           |
| DELETE | `/conversations/{id}`                       | Delete a conversation              |
| POST   | `/conversations/{id}/messages`              | Add a message to conversation      |
| PUT    | `/conversations/{id}/messages/{message_id}` | Update an existing message         |
| GET    | `/conversations/{id}/branches`              | List conversation branches         |


# List conversations

```http
GET https://api.sciphi.ai/v3/conversations
```

List conversations with pagination and sorting options.

This endpoint returns a paginated list of conversations for the
authenticated user.

## Request Headers

- X-API-Key (required)

## Query Parameters

- ids (optional): A list of conversation IDs to retrieve. If not provided, all conversations will be returned.
- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.conversations.list(
    offset=0,
    limit=10,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.conversations.list();
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/conversations?offset=0&amp;limit=10" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -G https://api.sciphi.ai/v3/conversations \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d ids=string \
     -d offset=0
```

# Create a new conversation

```http
POST https://api.sciphi.ai/v3/conversations
Content-Type: application/json
```

Create a new conversation.

This endpoint initializes a new conversation for the authenticated
user.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.conversations.create()

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.conversations.create();
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/conversations" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X POST https://api.sciphi.ai/v3/conversations \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Export conversations to CSV

```http
POST https://api.sciphi.ai/v3/conversations/export
Content-Type: application/json
```

Export conversations as a downloadable CSV file.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.conversations.export(
    output_path="export.csv",
    columns=["id", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.conversations.export({
        outputPath: "export.csv",
        columns: ["id", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/conversations/export"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/conversations/export \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Export messages to CSV

```http
POST https://api.sciphi.ai/v3/conversations/export_messages
Content-Type: application/json
```

Export conversations as a downloadable CSV file.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")
# when using auth, do client.login(...)

response = client.conversations.export_messages(
    output_path="export.csv",
    columns=["id", "created_at"],
    include_header=True,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

function main() {
    await client.conversations.exportMessages({
        outputPath: "export.csv",
        columns: ["id", "created_at"],
        includeHeader: true,
    });
}

main();

```

```shell cURL

curl -X POST "http://127.0.0.1:7272/v3/conversations/export_messages"                             -H "Authorization: Bearer YOUR_API_KEY"                             -H "Content-Type: application/json"                             -H "Accept: text/csv"                             -d '{ "columns": ["id", "created_at"], "include_header": true }'                             --output export.csv

```

```shell
curl -X POST https://api.sciphi.ai/v3/conversations/export_messages \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Get conversation details

```http
GET https://api.sciphi.ai/v3/conversations/{id}
```

Get details of a specific conversation.

This endpoint retrieves detailed information about a single
conversation identified by its UUID.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the conversation

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.conversations.get(
    "123e4567-e89b-12d3-a456-426614174000"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.conversations.retrieve({
        id: "123e4567-e89b-12d3-a456-426614174000",
    });
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/conversations/123e4567-e89b-12d3-a456-426614174000" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl https://api.sciphi.ai/v3/conversations/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Update conversation

```http
POST https://api.sciphi.ai/v3/conversations/{id}
Content-Type: application/json
```

Update an existing conversation.

This endpoint updates the name of an existing conversation
identified by its UUID.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the conversation to delete

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.conversations.update("123e4567-e89b-12d3-a456-426614174000", "new_name")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.conversations.update({
        id: "123e4567-e89b-12d3-a456-426614174000",
        name: "new_name",
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/conversations/123e4567-e89b-12d3-a456-426614174000"                                 -H "Authorization: Bearer YOUR_API_KEY"                                 -H "Content-Type: application/json"                                 -d '{"name": "new_name"}'

```

```shell
curl -X POST https://api.sciphi.ai/v3/conversations/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "name": "string"
}'
```

# Delete conversation

```http
DELETE https://api.sciphi.ai/v3/conversations/{id}
```

Delete an existing conversation.

This endpoint deletes a conversation identified by its UUID.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the conversation to delete

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.conversations.delete("123e4567-e89b-12d3-a456-426614174000")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.conversations.delete({
        id: "123e4567-e89b-12d3-a456-426614174000",
    });
}

main();

```

```shell cURL

curl -X DELETE "https://api.example.com/v3/conversations/123e4567-e89b-12d3-a456-426614174000" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/conversations/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Add message to conversation

```http
POST https://api.sciphi.ai/v3/conversations/{id}/messages
Content-Type: application/json
```

Add a new message to a conversation.

This endpoint adds a new message to an existing conversation.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the conversation

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.conversations.add_message(
    "123e4567-e89b-12d3-a456-426614174000",
    content="Hello, world!",
    role="user",
    parent_id="parent_message_id",
    metadata={"key": "value"}
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.conversations.addMessage({
        id: "123e4567-e89b-12d3-a456-426614174000",
        content: "Hello, world!",
        role: "user",
        parentId: "parent_message_id",
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/conversations/123e4567-e89b-12d3-a456-426614174000/messages" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{"content": "Hello, world!", "parent_id": "parent_message_id", "metadata": {"key": "value"}}'

```

```shell
curl -X POST https://api.sciphi.ai/v3/conversations/:id/messages \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "content": "string",
  "role": "string"
}'
```

# Update message in conversation

```http
POST https://api.sciphi.ai/v3/conversations/{id}/messages/{message_id}
Content-Type: application/json
```

Update an existing message in a conversation.

This endpoint updates the content of an existing message in a
conversation.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required): The unique identifier of the conversation
- message_id (required): The ID of the message to update

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.conversations.update_message(
    "123e4567-e89b-12d3-a456-426614174000",
    "message_id_to_update",
    content="Updated content"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.conversations.updateMessage({
        id: "123e4567-e89b-12d3-a456-426614174000",
        messageId: "message_id_to_update",
        content: "Updated content",
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/conversations/123e4567-e89b-12d3-a456-426614174000/messages/message_id_to_update" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{"content": "Updated content"}'

```

```shell
curl -X POST https://api.sciphi.ai/v3/conversations/:id/messages/:message_id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Prompts

A `Prompt` in R2R represents a templated instruction or query pattern that can be reused across the system. Prompts are managed by superusers and provide a consistent way to structure interactions with language models and other AI components.

Prompts in R2R provide:

* Templated instruction management
* Type-safe input handling
* Centralized prompt governance
* Dynamic prompt generation
* Version control for prompts

## Available Endpoints

| Method | Endpoint          | Description                                |
| ------ | ----------------- | ------------------------------------------ |
| POST   | `/prompts`        | Create a new prompt template               |
| GET    | `/prompts`        | List all available prompts                 |
| GET    | `/prompts/{name}` | Get a specific prompt with optional inputs |
| PUT    | `/prompts/{name}` | Update an existing prompt                  |
| DELETE | `/prompts/{name}` | Delete a prompt template                   |


# List all prompts

```http
GET https://api.sciphi.ai/v3/prompts
```

List all available prompts.

This endpoint retrieves a list of all prompts in the system. Only
superusers can access this endpoint.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.prompts.list()

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.prompts.list();
}

main();

```

```shell cURL

curl -X GET "https://api.example.com/v3/prompts" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

# Create a new prompt

```http
POST https://api.sciphi.ai/v3/prompts
Content-Type: application/json
```

Create a new prompt with the given configuration.

This endpoint allows superusers to create a new prompt with a
specified name, template, and input types.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.prompts.create(
    name="greeting_prompt",
    template="Hello, {name}!",
    input_types={"name": "string"}
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.prompts.create({
        name: "greeting_prompt",
        template: "Hello, {name}!",
        inputTypes: { name: "string" },
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/prompts" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{"name": "greeting_prompt", "template": "Hello, {name}!", "input_types": {"name": "string"}}'

```

```shell
curl -X POST https://api.sciphi.ai/v3/prompts \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "name": "string",
  "template": "string"
}'
```

# Get an existing prompt

```http
POST https://api.sciphi.ai/v3/prompts/{name}
Content-Type: application/json
```

Get a specific prompt by name, optionally with inputs and
override.

This endpoint retrieves a specific prompt and allows for optional
inputs and template override. Only superusers can access this
endpoint.

## Request Headers

- X-API-Key (required)

## Path Parameters

- name (required): Prompt name

## Query Parameters

- prompt_override (optional): Prompt override

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.prompts.get(
    "greeting_prompt",
    inputs={"name": "John"},
    prompt_override="Hi, {name}!"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.prompts.retrieve({
        name: "greeting_prompt",
        inputs: { name: "John" },
        promptOverride: "Hi, {name}!",
    });
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/prompts/greeting_prompt?inputs=%7B%22name%22%3A%22John%22%7D&amp;prompt_override=Hi%2C%20%7Bname%7D!" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X POST "https://api.sciphi.ai/v3/prompts/:name?prompt_override=string" \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json"
```

# Update an existing prompt

```http
PUT https://api.sciphi.ai/v3/prompts/{name}
Content-Type: application/json
```

Update an existing prompt's template and/or input types.

This endpoint allows superusers to update the template and input
types of an existing prompt.

## Request Headers

- X-API-Key (required)

## Path Parameters

- name (required): Prompt name

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.prompts.update(
    "greeting_prompt",
    template="Greetings, {name}!",
    input_types={"name": "string", "age": "integer"}
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.prompts.update({
        name: "greeting_prompt",
        template: "Greetings, {name}!",
        inputTypes: { name: "string", age: "integer" },
    });
}

main();

```

```shell cURL

curl -X PUT "https://api.example.com/v3/prompts/greeting_prompt" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{"template": "Greetings, {name}!", "input_types": {"name": "string", "age": "integer"}}'

```

```shell
curl -X PUT https://api.sciphi.ai/v3/prompts/:name \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{}'
```

# Delete a prompt

```http
DELETE https://api.sciphi.ai/v3/prompts/{name}
```

Delete a prompt by name.

This endpoint allows superusers to delete an existing prompt.

## Request Headers

- X-API-Key (required)

## Path Parameters

- name (required): Prompt name

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.prompts.delete("greeting_prompt")

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.prompts.delete({
        name: "greeting_prompt",
    });
}

main();

```

```shell cURL

curl -X DELETE "https://api.example.com/v3/prompts/greeting_prompt" \
    -H "Authorization: Bearer YOUR_API_KEY"

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/prompts/:name \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Chunks

A `Chunk` in R2R represents a processed segment of content derived from a parent Document. Chunks are the fundamental unit of retrieval and serve as the basis for semantic search, knowledge graph construction, and RAG operations. Each chunk contains text content, metadata, and optional vector embeddings.

Chunks are automatically generated during document ingestion and are optimized for:

* Semantic search and retrieval
* Knowledge graph relationship extraction
* Vector similarity comparisons
* Metadata filtering and organization

## Available Endpoints

| Method | Endpoint         | Description                                                  |
| ------ | ---------------- | ------------------------------------------------------------ |
| GET    | `/chunks`        | List chunks with pagination and filtering options            |
| POST   | `/chunks/search` | Perform semantic search across chunks with complex filtering |
| GET    | `/chunks/{id}`   | Retrieve a specific chunk by ID                              |
| POST   | `/chunks/{id}`   | Update an existing chunk's content or metadata               |
| DELETE | `/chunks/{id}`   | Delete a specific chunk                                      |


# Search Chunks

```http
POST https://api.sciphi.ai/v3/chunks/search
Content-Type: application/json
```

Perform a semantic search query over all stored chunks.

This endpoint allows for complex filtering of search results using PostgreSQL-based queries.
Filters can be applied to various fields such as document_id, and internal metadata values.

Allowed operators include `eq`, `neq`, `gt`, `gte`, `lt`, `lte`, `like`, `ilike`, `in`, and `nin`.

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
response = client.chunks.search(
    query="search query",
    search_settings={
        "limit": 10
    }
)

```

```shell
curl -X POST https://api.sciphi.ai/v3/chunks/search \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "query": "query"
}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/chunks/search \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "query": "string"
}'
```

# Retrieve Chunk

```http
GET https://api.sciphi.ai/v3/chunks/{id}
```

Get a specific chunk by its ID.

Returns the chunk's content, metadata, and associated
document/collection information. Users can only retrieve chunks
they own or have access to through collections.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
response = client.chunks.retrieve(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.chunks.retrieve({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
    });
}

main();

```

```shell
curl https://api.sciphi.ai/v3/chunks/id \
     -H "Authorization: Bearer <token>"
```

```shell
curl https://api.sciphi.ai/v3/chunks/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# Update Chunk

```http
POST https://api.sciphi.ai/v3/chunks/{id}
Content-Type: application/json
```

Update an existing chunk's content and/or metadata.

The chunk's vectors will be automatically recomputed based on the
new content. Users can only update chunks they own unless they are
superusers.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
response = client.chunks.update(
    {
        "id": "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
        "text": "Updated content",
        "metadata": {"key": "new value"}
    }
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.chunks.update({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa",
        text: "Updated content",
        metadata: {key: "new value"}
    });
}

main();

```

```shell
curl -X POST https://api.sciphi.ai/v3/chunks/id \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "id": "id",
  "text": "text"
}'
```

```shell
curl -X POST https://api.sciphi.ai/v3/chunks/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -H "Content-Type: application/json" \
     -d '{
  "id": "string",
  "text": "string"
}'
```

# Delete Chunk

```http
DELETE https://api.sciphi.ai/v3/chunks/{id}
```

Delete a specific chunk by ID.

This permanently removes the chunk and its associated vector
embeddings. The parent document remains unchanged. Users can only
delete chunks they own unless they are superusers.

## Request Headers

- X-API-Key (required)

## Path Parameters

- id (required)

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
response = client.chunks.delete(
    id="b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.chunks.delete({
        id: "b4ac4dd6-5f27-596e-a55b-7cf242ca30aa"
    });
}

main();

```

```shell
curl -X DELETE https://api.sciphi.ai/v3/chunks/id \
     -H "Authorization: Bearer <token>"
```

```shell
curl -X DELETE https://api.sciphi.ai/v3/chunks/:id \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>"
```

# List Chunks

```http
GET https://api.sciphi.ai/v3/chunks
```

List chunks with pagination support.

Returns a paginated list of chunks that the user has access to.
Results can be filtered and sorted based on various parameters.
Vector embeddings are only included if specifically requested.

Regular users can only list chunks they own or have access to
through collections. Superusers can list all chunks in the system.

## Request Headers

- X-API-Key (required)

## Query Parameters

- metadata_filter (optional): Filter by metadata
- include_vectors (optional): Include vector data in response
- offset (optional): Specifies the number of objects to skip. Defaults to 0.
- limit (optional): Specifies a limit on the number of objects to return, ranging between 1 and 100. Defaults to 100.

## Response Body

- 200: Successful Response
- 422: Validation Error

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
response = client.chunks.list(
    metadata_filter={"key": "value"},
    include_vectors=False,
    offset=0,
    limit=10,
)

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.chunks.list({
        metadataFilter: {key: "value"},
        includeVectors: false,
        offset: 0,
        limit: 10,
    });
}

main();

```

```shell
curl https://api.sciphi.ai/v3/chunks \
     -H "Authorization: Bearer <token>"
```

```shell
curl -G https://api.sciphi.ai/v3/chunks \
     -H "X-API-Key: string" \
     -H "Authorization: Bearer <token>" \
     -d metadata_filter=string \
     -d include_vectors=true
```

# Check system health

```http
GET https://api.sciphi.ai/v3/health
```

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.system.health()

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.system.health();
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/health"\
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_API_KEY" \

```

# R2R Settings

```http
GET https://api.sciphi.ai/v3/system/settings
```

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.system.settings()

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.system.settings();
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/system/settings" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_API_KEY" \

```

# Server status

```http
GET https://api.sciphi.ai/v3/system/status
```

## Request Headers

- X-API-Key (required)

## Response Body

- 200: Successful Response

## Examples

```python Python

from r2r import R2RClient

client = R2RClient()
# when using auth, do client.login(...)

result = client.system.status()

```

```javascript JavaScript

const { r2rClient } = require("r2r-js");

const client = new r2rClient();

function main() {
    const response = await client.system.status();
}

main();

```

```shell cURL

curl -X POST "https://api.example.com/v3/system/status" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_API_KEY" \

```

# R2R logs

```http
GET https://api.sciphi.ai/v3/logs/viewer
```

## Request Headers

- X-API-Key (required)

## Examples

```shell
curl https://api.sciphi.ai/v3/logs/viewer
```

# Ingestion

&gt; Learn how to ingest, update, and delete documents with R2R

## Introduction

R2R provides a powerful and flexible ingestion to process and manage various types of documents. It supports a wide range of file formats‚Äîtext, documents, PDFs, images, audio, and even video‚Äîand transforms them into searchable, analyzable content. The ingestion process includes parsing, chunking, embedding, and optionally extracting entities and relationships for knowledge graph construction.

This cookbook will guide you through:

* Ingesting files, raw text, or pre-processed chunks
* Choosing an ingestion mode (`fast`, `hi-res`, `ocr`, or `custom`)
* Updating and deleting documents and chunks

For more on configuring ingestion, see the [Ingestion Configuration Overview](/self-hosting/configuration/ingestion).

### Supported File Types

R2R supports ingestion of the following document types:

| Category          | File types                                |
| ----------------- | ----------------------------------------- |
| Image             | `.bmp`, `.heic`, `.jpeg`, `.png`, `.tiff` |
| MP3               | `.mp3`                                    |
| PDF               | `.pdf`                                    |
| CSV               | `.csv`                                    |
| E-mail            | `.eml`, `.msg`, `.p7s`                    |
| EPUB              | `.epub`                                   |
| Excel             | `.xls`, `.xlsx`                           |
| HTML              | `.html`                                   |
| Markdown          | `.md`                                     |
| Org Mode          | `.org`                                    |
| Open Office       | `.odt`                                    |
| Plain text        | `.txt`                                    |
| PowerPoint        | `.ppt`, `.pptx`                           |
| reStructured Text | `.rst`                                    |
| Rich Text         | `.rtf`                                    |
| TSV               | `.tsv`                                    |
| Word              | `.doc`, `.docx`                           |
| Code              | `.py`, `.js`, `.ts`, `.css`               |

## Ingestion Modes

R2R offers four primary ingestion modes to tailor the process to your requirements:

* **`fast`**:\
  A speed-oriented ingestion mode that prioritizes rapid processing with minimal enrichment. Summaries and some advanced parsing are skipped, making this ideal for quickly processing large volumes of documents.

* **`hi-res`**:\
  A comprehensive, high-quality ingestion mode that may leverage multimodal foundation models (visual language models) for parsing complex documents and PDFs, even integrating image-based content.
  * On a **lite** deployment, R2R uses its built-in (`r2r`) parser.
  * On a **full** deployment, it can use `unstructured_local` or `unstructured_api` for more robust parsing and advanced features.\
    Choose `hi-res` mode if you need the highest quality extraction, including image-to-text analysis and richer semantic segmentation.

* **`ocr`**:
  OCR mode utilizes optical character recognition models to convert PDFs to markdown. Currently, this mode requires use of Mistral OCR.

* **`custom`**:\
  For advanced users who require fine-grained control. In `custom` mode, you provide a full `ingestion_config` dict or object to specify every detail: parser options, chunking strategy, character limits, and more.

**Example Usage:**

```python
file_path = 'path/to/file.txt'
metadata = {'key1': 'value1'}

# hi-res mode for thorough extraction
client.documents.create(
    file_path=file_path,
    metadata=metadata,
    ingestion_mode="hi-res"
)

# fast mode for quick processing
client.documents.create(
    file_path=file_path,
    ingestion_mode="fast"
)

# custom mode for full control
client.documents.create(
    file_path=file_path,
    ingestion_mode="custom",
    ingestion_config={
        "provider": "unstructured_local",
        "strategy": "auto",
        "chunking_strategy": "by_title",
        "new_after_n_chars": 256,
        "max_characters": 512,
        "combine_under_n_chars": 64,
        "overlap": 100,
    }
)
```

## Ingesting Documents

A `Document` represents ingested content in R2R. When you ingest a file, text, or chunks:

1. The file (or text) is parsed into text.
2. Text is chunked into manageable units.
3. Embeddings are generated for semantic search.
4. Content is stored for retrieval and optionally linked to the knowledge graph.

In a **full** R2R installation, ingestion is asynchronous. You can monitor ingestion status and confirm when documents are ready:

```zsh
client.documents.list()

# [
#  DocumentResponse(
#    id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'), 
#    collection_ids=[UUID('122fdf6a-e116-546b-a8f6-e4cb2e2c0a09')], 
#    owner_id=UUID('2acb499e-8428-543b-bd85-0d9098718220'), 
#    document_type=<documenttype.pdf: 'pdf'="">, 
#    metadata={'title': 'DeepSeek_R1.pdf', 'version': 'v0'}, 
#    version='v0', 
#    size_in_bytes=1768572, 
#    ingestion_status=<ingestionstatus.success: 'success'="">, 
#    extraction_status=<graphextractionstatus.pending: 'pending'="">, 
#    created_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 126759, tzinfo=TzInfo(UTC)), 
#    updated_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 160114, tzinfo=TzInfo(UTC)), 
#    ingestion_attempt_number=None, 
#    summary="The document contains a comprehensive overview of DeepSeek-R1, a series of reasoning models developed by DeepSeek-AI, which includes DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero utilizes large-scale reinforcement learning (RL) without supervised fine-tuning, showcasing impressive reasoning capabilities but facing challenges like readability and language mixing. To enhance performance, DeepSeek-R1 incorporates multi-stage training and cold-start data, achieving results comparable to OpenAI's models on various reasoning tasks. The document details the models' training processes, evaluation results across multiple benchmarks, and the introduction of distilled models that maintain reasoning capabilities while being smaller and more efficient. It also discusses the limitations of current models, such as language mixing and sensitivity to prompts, and outlines future research directions to improve general capabilities and efficiency in software engineering tasks. The findings emphasize the potential of RL in developing reasoning abilities in large language models and the effectiveness of distillation techniques for smaller models.", summary_embedding=None, total_tokens=29673)] total_entries=1
#   ), ...
# ]
```

An `ingestion_status` of `"success"` confirms the document is fully ingested. You can also check the R2R dashboard at [http://localhost:7273](http://localhost:7273) for ingestion progress and status.

For more details on creating documents, [refer to the Create Document API](/api-and-sdks/documents/create-document).

## Ingesting Pre-Processed Chunks

If you have pre-processed chunks from your own pipeline, you can directly ingest them. This is especially useful if you've already divided content into logical segments.

```python
chunks = ["This is my first parsed chunk", "This is my second parsed chunk"]
client.documents.create(
    chunks=chunks,
    ingestion_mode="fast"  # use fast for a quick chunk ingestion
)
```

## Deleting Documents and Chunks

To remove documents or chunks, call their respective `delete` methods:

```python
# Delete a document
delete_response = client.documents.delete(document_id)

# Delete a chunk
delete_response = client.chunks.delete(chunk_id)
```

You can also delete documents by specifying filters using the [`by-filter`](/api-and-sdks/documents/delete-document-by-filter) route.

## Additional Configuration &amp; Concepts

* **Light vs. Full Deployments:**
  * Light (default) uses R2R's built-in parser and supports synchronous ingestion.
  * Full deployments orchestrate ingestion tasks asynchronously and integrate with more complex providers like `unstructured_local`.

* **Provider Configuration:**\
  Settings in `r2r.toml` or at runtime (`ingestion_config`) can adjust parsing and chunking strategies:
  * `fast` and `hi-res` modes are influenced by strategies like `"auto"` or `"hi_res"` in the unstructured provider.
  * `custom` mode allows you to override chunk size, overlap, excluded parsers, and more at runtime.

For detailed configuration options, see:

* [Data Ingestion Configuration](/self-hosting/configuration/ingestion)

## Conclusion

R2R's ingestion is flexible and efficient, allowing you to tailor ingestion to your needs:

* Use `fast` for quick processing.
* Use `hi-res` for high-quality, multimodal analysis.
* Use `custom` for advanced, granular control.

You can easily ingest documents or pre-processed chunks, update their content, and delete them when no longer needed. Combined with powerful retrieval and knowledge graph capabilities, R2R enables seamless integration of advanced document management into your applications.


# Knowledge Graphs

&gt; Building and managing graphs through collections

## Overview

R2R allows you to build and analyze knowledge graphs from your documents through a collection-based architecture. The system extracts entities and relationships from documents, enabling richer search capabilities that understand connections between information.

The process works in several key stages:

* Documents are first ingested and entities/relationships are extracted
* Collections serve as containers for documents and their corresponding graphs
* Extracted information is pulled into the collection's graph
* Communities can be built to identify higher-level concepts
* The resulting graph enhances search with relationship-aware queries

Collections in R2R are flexible containers that support multiple documents and provide features for access control and graph management. A document can belong to multiple collections, allowing for different organizational schemes and sharing patterns.

The resulting knowledge graphs improve search accuracy by understanding relationships between concepts rather than just performing traditional document search.

<steps>
  ### Ingestion and Extraction

  Before we can extract entities and relationships from a document, we must ingest a file. After we've successfully ingested a file, we can `extract` the entities and relationships from document.

  In the following script, we fetch *The Gift of the Magi* by O. Henry and ingest it our R2R server. We then begin the extraction process, which may take a few minutes to run.

  <tabs>
    <tab title="Python">
      ```python
      import requests
      from r2r import R2RClient
      import tempfile
      import os

      # Set up the client
      client = R2RClient("http://localhost:7272")

      # Fetch the text file
      url = "https://www.gutenberg.org/cache/epub/7256/pg7256.txt"
      response = requests.get(url)

      # Create a temporary file
      temp_dir = tempfile.gettempdir()
      temp_file_path = os.path.join(temp_dir, "gift_of_the_magi.txt")
      with open(temp_file_path, 'w') as temp_file:
          temp_file.write(response.text)

      # Ingest the file
      ingest_response = client.documents.create(file_path=temp_file_path)
      document_id = ingest_response["results"]["document_id"]

      # Extract entities and relationships
      extract_response = client.documents.extract(document_id)

      # View extracted knowledge
      entities = client.documents.list_entities(document_id)
      relationships = client.documents.list_relationships(document_id)

      # Clean up the temporary file
      os.unlink(temp_file_path)
      ```
    </tab>
  </tabs>

  As this script runs, we see indications of successful ingestion and extraction.

  <tabs>
    <tab title="Ingestion">
      
        <img src="file://4ec6245d-5ee1-4999-bd6b-1e00ee856cfa/" alt="Successful ingestion and extraction in the R2R dashboard.">
      
    </tab>

    <tab title="Entities">
      
        <img src="file://f4adc358-4f90-47d7-9465-b5555a2b3fe5/" alt="Viewing the entity in the dashboard.">
      
    </tab>
  </tabs>

  ### Deduplication

  If you would like to deduplicate the extracted entities, you can run the following method. To learn more about deduplication, view our [deduplication documentation here](/documentation/deduplication).

  <tabs>
    <tab title="Python">
      ```python
      from r2r import R2RClient

      # Set up the client
      client = R2RClient("http://localhost:7272")

      client.documents.deduplicate("20e29a97-c53c-506d-b89c-1f5346befc58")
      ```
    </tab>
  </tabs>

  While the exact number of extracted entities and relationships will differ across models, this particular document produces approximately 120 entities, with only 20 distinct entities.

  ### Managing Collections

  Graphs are built within a collection, allowing for us to add many documents to a graph, and to share our graphs with other users. When we ingested the file above, it was added into our default collection.

  Each collection has a description which is used in the graph creation process. This can be set by the user, or generated using an LLM.

  <tabs>
    <tab title="Python">
      ```python
      from r2r import R2RClient

      # Set up the client
      client = R2RClient("http://localhost:7272")

      # Update the description of the default collection
      collection_id = "122fdf6a-e116-546b-a8f6-e4cb2e2c0a09"
      update_result = client.collections.update(
          id=collection_id,
          generate_description=True, # LLM generated
      )
      ```
    </tab>
  </tabs>

  
    <img src="file://55323f39-fd4d-4f7f-aee9-383c02b038bc/" alt="The resulting description.">
  

  ### Pulling Extractions into the Graph

  Our graph will not contain the extractions from our documents until we `pull` them into the graph. This gives developers more granular control over the creation and management of graphs.

  Recall that we already extracted the entities and relationships for the graph; this means that we can `pull` a document into many graphs without having to rerun the extraction process.

  <tabs>
    <tab title="Python">
      ```python
      from r2r import R2RClient

      # Set up the client
      client = R2RClient("http://localhost:7272")

      # Pull the extractions from all docments into the default collection
      collection_id = "122fdf6a-e116-546b-a8f6-e4cb2e2c0a09"
      client.graphs.pull(
          collection_id=collection_id
      )
      ```
    </tab>
  </tabs>

  As soon as we `pull` the extractions into the graph, we can begin using the graph in our searches. We can confirm that the entities and relationships were pulled into the collection, as well.

  <tabs>
    <tab title="Entities">
      
        <img src="file://19c52ae0-3011-49f1-b4c8-54410e674bc6/" alt="Successful ingestion and extraction in the R2R dashboard.">
      
    </tab>

    <tab title="Entity Visualization">
      
        <img src="file://8fd95859-50d8-40e1-b2a3-899a7686db9a/" alt="Entity distribution chart.">
      
    </tab>
  </tabs>

  ### Building Communities

  To further enhance our graph we can build communities, which clusters over the entities and relationships inside our graph. This allows us to capture higher-level concepts that exist within our data.

  <tabs>
    <tab title="Python">
      ```python
      from r2r import R2RClient

      # Set up the client
      client = R2RClient("http://localhost:7272")

      # Build the communities for the default collection
      collection_id = "122fdf6a-e116-546b-a8f6-e4cb2e2c0a09"
      client.graphs.build(
          collection_id=collection_id
      )
      ```
    </tab>
  </tabs>

  We can see that the resulting communities capture overall themes and concepts within the story.

  
    <img src="file://d577591f-5125-484f-8552-1580e758d7b2/" alt="The communities generated for the collection.">
  

  ### Graph Search

  Now that we have built our graph we can query over it. Good questions for graphs might require deep understanding of relationships and ideas that span across multiple documents.

  <tabs>
    <tab title="Python">
      ```python
      from r2r import R2RClient

      # Set up the client
      client = R2RClient("http://localhost:7272")

      results = client.retrieval.search("""
          What items did Della and Jim each originally own,
          what did they do with those items, and what did they
          ultimately give each other?
          """,
          search_settings={
              "graph_settings": {"enabled": True},
          }
      )
      ```
    </tab>
  </tabs>

  
    <img src="file://d0e18a27-b22c-433f-9b1d-be1d2747e0e5/" alt="Performing a searhc over the graph.">
  
</steps>


# User-Defined Agent Tools

&gt; Define custom tools for the RAG Agent

There are many cases where it is helpful to define custom tools for the RAG Agent. R2R allows for users to define custom tools, passing these definitions into the Agent at server start.

### Defining New Tools

There is a directory in the R2R repository, `/docker/user_tools`, which is mounted to the R2R docker container. It is here that we will place our custom tool files.

There, we will find a README.md file, which includes a template for our new tool:

<tabs>
  <tab title="Python">
    ```python
    from core.base.agent.tools.base import Tool


    class ToolNameTool(Tool):
        """
        A user defined tool.
        """

        def __init__(self):
            super().__init__(
                name="tool_name",
                description="A natural language tool description that is shown to the agent.",
                parameters={
                    "type": "object",
                    "properties": {
                        "input_parameter": {
                            "type": "string",
                            "description": "Define any input parameters by their name and type",
                        },
                    },
                    "required": ["input_parameter"],
                },
                results_function=self.execute,
                llm_format_function=None,
            )

        async def execute(self, input_parameter: str, *args, **kwargs):
            """
            Implementation of the tool.
            """

            # Any custom tool logic can go here

            output_response = some_method(input_parameter)

            result = AggregateSearchResult(
                generic_tool_result=[web_response],
            )

            # Add to results collector if context is provided
            if context and hasattr(context, "search_results_collector"):
                context.search_results_collector.add_aggregate_result(result)

            return result
    ```
  </tab>
</tabs>

This template has two basic methods:

1. `__init__` is where we define the tool. The description that we make here is shown to the agent.
2. `execute` is where we define any custom tool logic and interact with the inputs.

### Writing our new tool

Below, we have an example of a toy tool, which takes an integer and string input, returning a silly message to the agent. Should your tool require additional dependencies, be sure to include them in the `user_requirements.txt` file located in the `/docker` directory.

<tabs>
  <tab title="Python">
    ```python
    from r2r import Tool, AggregateSearchResult


    class SecretMethodTool(Tool):
        """
        A user defined tool.
        """

        def __init__(self):
            super().__init__(
                name="secret_method",
                description="Performs a secret method.",
                parameters={
                    "type": "object",
                    "properties": {
                        "number": {
                            "type": "string",
                            "description": "An integer input for the secret method.",
                        },
                        "string": {
                            "type": "string",
                            "description": "A string input for the secret method.",
                        },
                    },
                    "required": ["number", "string"],
                },
                results_function=self.execute,
                llm_format_function=None,
            )

        async def execute(self, number: int, string: str, *args, **kwargs):
            """
            Implementation of the tool.
            """
            
            output_response = f"Your order for {number} dancing flamingos has been received. They will arrive by unicycle courier within 3-5 business dreams. Please prepare {string} for them."
            
            result = AggregateSearchResult(
                generic_tool_result=output_response,
            )

            context = self.context
            # Add to results collector if context is provided
            if context and hasattr(context, "search_results_collector"):
                context.search_results_collector.add_aggregate_result(result)

            return result
    ```
  </tab>
</tabs>

Finally, we can modify our configuration file's `agent` section to include our new tool:

```toml
[agent]
rag_tools = ["secret_method"]
```

Finally, we can run the following and see that our agent called our new method, passed the required parameters, and understood its output:

<tabs>
  <tab title="Request">
    ```python
    client.retrieval.agent(
        message={"role": "user", "content": "Can you run the secret method tool? Feel free to use any parameters you want. I just want to see the output."},
    )
    ```
  </tab>

  <tab title="Response">
    ```zsh
    results=AgentResponse(messages=[Message(role='assistant', content='The secret method tool produced the following output:\n\n"Your order for 42 dancing flamingos has been received. They will arrive by unicycle courier within 3-5 business dreams. Please prepare Hello, World! for them."\n\nThis whimsical response seems to be a playful and humorous output generated by the tool.', name=None, function_call=None, tool_calls=None, tool_call_id=None, metadata={'citations': [], 'tool_calls': [{'name': 'secret_method', 'args': '{"number":"42","string":"Hello, World!"}'}], 'aggregated_search_result': '[]'}, structured_content=None, image_url=None, image_data=None)], conversation_id='12ad2d6b-1429-48ea-9077-711726d8cfde')
    ```
  </tab>
</tabs>


# Setting Up Email Verification

&gt; Enable verification for your production applications

Configuring your deployment to require email verification helps keep your deployment secure, prevents unauthorized account creation,
reduces spam registrations, and ensures you have valid contact information for your users.

Currently, R2R has integrations for both [Mailersend](https://www.mailersend.com/) and [Sendgrid](https://sendgrid.com/).

## Setup

Both Mailersend and Sendgrid require registration, but do offer free tiers for evaluating their services. Create an account with your desired
provider, and generate an API key.

<tabs>
  <tab title="Mailersend">
    * [Create an account](https://www.mailersend.com/signup)
    * [Generate an API key](https://www.mailersend.com/help/managing-api-tokens)
  </tab>

  <tab title="Sendgrid">
    * [Create an account](https://twilio.com/signup)
    * [Generate an API key](https://www.twilio.com/docs/sendgrid/ui/account-and-settings/api-keys)
  </tab>
</tabs>

## Creating a Template

Once you have registered for an account with your email provider, you will want to create an email template. Providers will have pre-made templates, or you can build these from scratch.

<img src="file://ae157df6-4638-40ad-9b89-5e7ecdd0763c/" alt="A Mailersend welcome template.">

Once you save a template, you will want to make note of the template id. These will go into the configuration files.

## Configuration Settings

We can then configure our deployment with the templates, redirect URL (`frontend_url`), and from email.

### Configuration File

<codeblocks>
  ```toml title="mailersend.toml"
  [email]
  provider = "mailersend"
  verify_email_template_id=""
  reset_password_template_id=""
  password_changed_template_id=""
  frontend_url=""
  from_email=""
  ```

  ```toml title="sendgrid.toml"
  [email]
  provider = "sendgrid"
  verify_email_template_id=""
  reset_password_template_id=""
  password_changed_template_id=""
  frontend_url=""
  from_email=""
  ```
</codeblocks>

### Environment Variables

It is required to set your provider API key in your environment:

```zsh
export MAILERSEND_API_KEY=‚Ä¶
export SENDGRID_API_KEY=‚Ä¶
```


# Maintenance &amp; Scaling

&gt; Learn how to maintain and scale your R2R system

<warning>
  User management features are currently restricted to:

  * Self-hosted instances
  * Enterprise tier cloud accounts

  Contact our sales team for Enterprise pricing and features.
</warning>

This guide covers essential maintenance tasks for R2R deployments, with a focus on vector index management and system updates.
Understanding when and how to build vector indices, as well as keeping your R2R installation current, is crucial for maintaining
optimal performance at scale.

## PostgreSQL VACUUM

PostgreSQL's VACUUM operation is a critical maintenance process that reclaims storage space occupied by deleted or obsolete data,
updates statistics for the query planner to optimize performance prevents transaction ID wraparound issues, and improves overall
database performance. In normal PostgreSQL operation, when you delete or update rows, the original data is not immediately removed
from disk but marked as obsolete. These obsolete rows (called "dead tuples") accumulate over time, consuming disk space and potentially
slowing down queries.

R2R includes automatic scheduled maintenance to optimize your PostgreSQL database:

```toml
[database.maintenance]
vacuum_schedule = "0 3 * * *"  # Run at 3:00 AM daily
```

Regular vacuum operations keep your database healthy, however it's recommended to schedule these operations during periods of low system usage.

## Vector Indices

### Do You Need Vector Indices?

Vector indices are **not necessary for all deployments**, especially in multi-user applications where each user typically queries their own subset of documents. Consider that:

* In multi-user applications, queries are usually filtered by user\_id, drastically reducing the actual number of vectors being searched
* A system with 1 million total vectors but 1000 users might only search through 1000 vectors per query
* Performance impact of not having indices is minimal when searching small per-user document sets

Only consider implementing vector indices when:

* Individual users are searching across hundreds of thousands of documents
* Query latency becomes a bottleneck even with user-specific filtering
* You need to support cross-user search functionality at scale

For development environments or smaller deployments, the overhead of maintaining vector indices often outweighs their benefits.

### Vector Index Management

R2R supports multiple indexing methods, with HNSW (Hierarchical Navigable Small World) being recommended for most use cases:

```python
# Create vector index

create_response = client.indices.create(
    {
        "table_name": "vectors",
        "index_method": "hnsw",
        "index_measure": "cosine_distance",
        "index_arguments": {
            "m": 16,              # Number of connections per element
            "ef_construction": 64 # Size of dynamic candidate list
        },
    }
)
# List existing indices
indices = client.indices.list()

# Delete an index
delete_response = client.indices.delete(
    index_name="ix_vector_cosine_ops_hnsw__20241021211541",
    table_name="vectors",
)
print('delete_response = ', delete_response)
```

#### Important Considerations

1. **Pre-warming Requirement**
   * New indices start "cold" and require warming for optimal performance
   * Initial queries will be slower until the index is loaded into memory
   * Consider implementing explicit pre-warming in production
   * Warming must be repeated after system restarts

2. **Resource Usage**
   * Index creation is CPU and memory intensive
   * Memory usage scales with both dataset size and `m` parameter
   * Consider creating indices during off-peak hours

3. **Performance Tuning**
   * HNSW Parameters:
     * `m`: 16-64 (higher = better quality, more memory)
     * `ef_construction`: 64-100 (higher = better quality, longer build time)
   * Distance Measures:
     * `cosine_distance`: Best for normalized vectors (most common)
     * `l2_distance`: Better for absolute distances
     * `max_inner_product`: Optimized for dot product similarity

## Scaling Strategies

### Horizontal Scaling

For applications serving many users, it is advantageous to scale the number of R2R replicas horizontally. This improves concurrent handling of requests and reliability.

1. **Load Balancing**
   * Deploy multiple R2R replicas behind a load balancer
   * Requests are distributed amongst the replicas
   * Particularly effective since most queries are user-specific

2. **Sharding**
   * Consider sharding by user\_id for large multi-user deployments
   * Each shard handles a subset of users
   * Maintains performance even with millions of total documents

#### Horizontal Scaling with Docker Swarm

R2R ships with an example compose file to deploy to [Swarm](https://docs.docker.com/engine/swarm/), an advanced Docker feature that manages a cluster of Docker daemons.

After cloning the R2R repository, we can initialize Swarm and start our stack:

```zsh
# Set the number of R2R replicas to create, defaults to 3 if not set
export R2R_REPLICAS=3

# Initialize swarm (if not already running)
docker swarm init

# Create overlay networks
docker network create --driver overlay r2r_r2r-network

# Source environment file
set -a
source /path/to/.env
set +a

# Deploy stacks
docker stack deploy -c R2R/py/r2r/compose.swarm.yaml r2r

# Commands to bring down stacks (when needed)
docker stack rm r2r
```

### Vertical Scaling

For applications requiring large single-user searches:

1. **Cloud Provider Solutions**
   * AWS RDS supports up to 1 billion vectors per instance
   * Scale up compute and memory resources as needed
   * Example instance types:
     * `db.r6g.16xlarge`: Suitable for up to 100M vectors
     * `db.r6g.metal`: Can handle 1B+ vectors

2. **Memory Optimization**
   ```python
   # Optimize for large vector collections
   client.indices.create(
       table_name="vectors",
       index_method="hnsw",
       index_arguments={
           "m": 32,              # Increased for better performance
           "ef_construction": 80  # Balanced for large collections
       }
   )
   ```

### Multi-User Considerations

1. **Filtering Optimization**
   ```python
   # Efficient per-user search
   response = client.retrieval.search(
       "query",
       search_settings={
           "filters": {
               "user_id": {"$eq": "current_user_id"}
           }
       }
   )
   ```

2. **Collection Management**
   * Group related documents into collections
   * Enable efficient access control
   * Optimize search scope

3. **Resource Allocation**
   * Monitor per-user resource usage
   * Implement usage quotas if needed
   * Consider dedicated instances for power users

### Performance Monitoring

Monitor these metrics to inform scaling decisions:

1. **Query Performance**
   * Average query latency per user
   * Number of vectors searched per query
   * Cache hit rates

2. **System Resources**
   * Memory usage per instance
   * CPU utilization
   * Storage growth rate

3. **User Patterns**
   * Number of active users
   * Query patterns and peak usage times
   * Document count per user


# Orchestration

&gt; Learn how orchestration is handled inside R2R

<warning>
  User management features are currently restricted to:

  * Self-hosted instances
  * Enterprise tier cloud accounts

  Contact our sales team for Enterprise pricing and features.
</warning>

R2R uses [Hatchet](https://docs.hatchet.run/home) for orchestrating complex workflows, particularly for ingestion and knowledge graph construction processes.

Hatchet is a distributed, fault-tolerant task queue that solves scaling problems like concurrency, fairness, and rate limiting. It allows R2R to distribute functions between workers with minimal configuration.

### Key Concepts

1. **Workflows**: Sets of functions executed in response to external triggers.
2. **Workers**: Long-running processes that execute workflow functions.
3. **Managed Queue**: Low-latency queue for handling real-time tasks.

## Orchestration in R2R

### Benefits of orchestration

1. **Scalability**: Efficiently handles large-scale tasks.
2. **Fault Tolerance**: Built-in retry mechanisms and error handling.
3. **Flexibility**: Easy to add or modify workflows as R2R's capabilities expand.

### Workflows in R2R

1. **IngestFilesWorkflow**: Handles file ingestion, parsing, chunking, and embedding.
2. **UpdateFilesWorkflow**: Manages the process of updating existing files.
3. **KgExtractAndStoreWorkflow**: Extracts and stores knowledge graph information.
4. **CreateGraphWorkflow**: Orchestrates the creation of knowledge graphs.
5. **EnrichGraphWorkflow**: Handles graph enrichment processes like node creation and clustering.

## Orchestration GUI

By default, the R2R Docker ships with with Hatchet's front-end application on port 7274. This can be accessed by navigating to `http://localhost:7274`.

You may login with the following credentials:

<note>
  **Email:** [admin@example.com](mailto:admin@example.com)

  **Password:** Admin123!!
</note>

### Login


  <img src="file://810e7217-30c5-4b93-9d60-6af165a67be1/">


### Running Tasks

The panel below shows the state of the Hatchet workflow panel at `http://localhost:7274/workflow-runs` immediately after calling `r2r documents create-samples`:


  <img src="file://41c15adf-9703-4c5d-908e-9f05a9279c53/">


### Inspecting a workflow

You can inspect a workflow within Hatchet and can even attempt to retry the job from directly in the GUI in the case of failure:


  <img src="file://de8d9036-583f-4a59-bad2-1c31c845b7d8/">


### Long running tasks

Hatchet supports long running tasks, which is very useful during knowledge graph construction:


  <img src="file://5e6ea6a8-b3fd-422d-9b4c-9c2bb7bbcf15/">


## Coming Soon

In the coming day(s) / week(s) we will further highlight the available feature set and best practices for orchestrating your ingestion workflows inside R2R.


# Local LLMs

&gt; Run R2R with Local LLMs

## Overview

There are many amazing LLMs and embedding models that can be run locally. R2R fully supports using these models, giving you full control over your data and infrastructure.

Running models locally can be ideal for sensitive data handling, reducing API costs, or situations where internet connectivity is limited. While cloud-based LLMs often provide cutting-edge performance,
local models offer a compelling balance of capability, privacy, and cost-effectiveness for many use cases.

<warning>
  Local LLM features are currently restricted to:

  * Self-hosted instances
  * Enterprise tier cloud accounts

  Contact our sales team for Enterprise pricing and features.
</warning>

<steps>
  ### Serving Local Models

  <note>
    For this cookbook, we'll serve our local models via Ollama. [You may follow the instructions on their official website to install.](https://ollama.com/)

    You can also follow along using LM Studio. To get started with LM Studio, see our [Local LLM documentation](/self-hosting/local-rag).

    R2R supports [LiteLLM](https://github.com/BerriAI/litellm) for routing embedding and completion requests. This allows for OpenAI-compatible endpoints to be called and seamlessly routed to, if you are serving local models another way.
  </note>

  We must first download the models that we wish to run and start our ollama server. The following command will 'pull' the models and begin the Ollama server via `http://localhost:11434`.

  <tabs>
    <tab title="Bash">
      ```Zsh
      ollama pull llama3.1
      ollama pull mxbai-embed-large
      ```
    </tab>
  </tabs>

  <error>
    Ollama has a default context window size of 2048 tokens. Many of the prompts and processes that R2R uses requires larger window sizes.

    It is recommended to set the context size to a minimum of 16k tokens. The following guideline is generally useful to determine what your system can handle:

    * 8GB RAM/VRAM: \~4K-8K context
    * 16GB RAM/VRAM: \~16K-32K context
    * 24GB+ RAM/VRAM: 32K+ context

    To change the default context window you must first create a Modelfile for Ollama, where you can set `num_ctx`:

    ```Zsh
    echo 'FROM llama3.1
    PARAMETER num_ctx 16000' &gt; Modelfile
    ```

    Then you must create a manifest for that model:

    ```Zsh
    ollama create llama3.1 -f Modelfile
    ```
  </error>

  <tabs>
    <tab title="Bash">
      Then, we can start the Ollama server:

      ```Zsh
      ollama serve
      ```
    </tab>
  </tabs>

  ### Configuring R2R

  Now that our models have been loaded and our Ollama server is ready, we can launch our R2R server.

  The standard distribution of R2R includes a configuration file for running `llama3.1` and `mxbai-embed-large`. If you wish to utilize other models, you must create a custom config file and pass this to your server.

  <accordiongroup>
    <accordion title="ollama.toml">
      ```Toml
      [app]
      # LLM used for internal operations, like deriving conversation names
      fast_llm = "ollama/llama3.1"

      # LLM used for user-facing output, like RAG replies
      quality_llm = "ollama/llama3.1"

      # LLM used for ingesting visual inputs
      vlm = "ollama/llama3.2-vision" # TODO - Replace with viable candidate

      # LLM used for transcription
      audio_lm = "ollama/llama3.1" # TODO - Replace with viable candidate

      [embedding]
      provider = "ollama"
      base_model = "mxbai-embed-large"
      base_dimension = 1_024
      batch_size = 128
      add_title_as_prefix = true
      concurrent_request_limit = 2

      [completion_embedding]
      provider = "ollama"
      base_model = "mxbai-embed-large"
      base_dimension = 1_024
      batch_size = 128
      add_title_as_prefix = true
      concurrent_request_limit = 2

      [agent]
      tools = ["local_search"]

      [agent.generation_config]
      model = "ollama/llama3.1"

      [completion]
      provider = "litellm"
      concurrent_request_limit = 1

      [completion.generation_config]
      temperature = 0.1
      top_p = 1
      max_tokens_to_sample = 1_024
      stream = false
      ```
    </accordion>
  </accordiongroup>

  We launch R2R by specifying this configuration file:

  ```Zsh
  export R2R_CONFIG_NAME=ollama
  python -m r2r.serve
  ```

  Since we're serving with Docker, once R2R successfully launches the R2R dashboard opens for us. We can upload a document and see requests hit our Ollama server.

  
    <img src="file://7445198a-39da-4902-8019-37eb494690fb/" alt="The processed document and the Ollama server logs.">
  

  ### Retrieval and Search

  Now that we have ingested our file, we can perform RAG and chunk search over it. Here, we see that we are able to get relevant results and correct answers‚Äîall without needing to make a request out to an external provider!

  <tabs>
    <tab title="Local RAG">
      
        <img src="file://ee8313b5-42a0-4f71-8242-22d6e58297fc/" alt="A RAG search done with local LLMs.">
      
    </tab>

    <tab title="Local Search">
      
        <img src="file://e3cfa570-2972-4f04-b7b8-08d7f46d2132/" alt="A semantic serach done with LLMs.">
      
    </tab>
  </tabs>

  ### Extracting Entities and Relationships

  If we'd like to build a graph for our document, we must first extract the entities and relationships that it contains. Through the dashboard
  we can select the 'Document Extraction' action in the documents table. This will start the extraction process in the background, which uses named entity
  recognition to find entities and relationships.

  Note that this process can take quite a bit of time, depending on the size of your document and the hardware running your model. Once the process is complete,
  we will see that the `extraction` status has turned green.

  <tabs>
    <tab title="Successful Extraction">
      
        <img src="file://0113a7e6-f4e2-4b1b-9587-e61d7871135d/" alt="Successful extraction on the documents table.">
      
    </tab>

    <tab title="Extracted Entities">
      
        <img src="file://df168a97-2820-4c89-9bfd-ea586e2ace80/" alt="A semantic serach done with LLMs.">
      
    </tab>

    <tab title="Extracted Relationships">
      
        <img src="file://299b5878-0e67-49b4-b144-7a119c158e81/" alt="A semantic serach done with LLMs.">
      
    </tab>
  </tabs>

  ### Graph RAG

  Now we must `pull` the document extractions into the graph. This is done at the collection level, and creates a copy of our extractions for searching over and creating communities with.

  Then, we can conduct search, RAG, or agent queries that utilize the graph.

  <tabs>
    <tab title="Graph RAG">
      
        <img src="file://f5cc8755-6eb7-477d-91cc-ce94852f0661/" alt="A search that utilizes the entities and relationships from the graph.">
      
    </tab>

    <tab title="Pulling Extractions into Graph">
      
        <img src="file://8675ccec-6ac3-460a-ada3-c69eebcd9689/" alt="A semantic serach done with LLMs.">
      
    </tab>
  </tabs>

  ### Building communities

  We can go one step further and create communities over the entities and relationships in the graph. By clustering over the closely related extractions, we can
  further develop the understanding of how these entities and relationships interact. This can be particularly helpful in sets of documents where we see overarching
  or recuring themes.

  We trigger the extraction procedure, which produces a number of communities. Now, when we run queries over our graph we can utilize the communities to provide context that
  better encompasses overall concepts and ideas throughout our documents.

  <tabs>
    <tab title="RAG with Communities">
      
        <img src="file://67879996-1968-4473-b0bf-2643e8fb33fb/" alt="A RAG search that utilizes communities.">
      
    </tab>

    <tab title="Generated Communities">
      
        <img src="file://2790a459-02ee-4825-868b-ec2df0302638/" alt="A semantic serach done with LLMs.">
      
    </tab>
  </tabs>
</steps>


# Structured Output

&gt; Enable JSON Outputs

## Overview

Structured outputs allow users to ensure that the retrieval response generated by the LLM follows a user-defined structure. This provides reliable type-safety, making it easier to generate high-quality, production-ready applications.

R2R supports passing Pydantic models via our Python SDK.

With this, you can:

* Define the exact structure you expect for responses
* Automatically validate that responses match your schema
* Access response fields with proper typing and autocompletion
* Handle errors gracefully when responses don't match expectations

## Using Structured Outputs with R2R

The example below demonstrates how to define a simple Pydantic model that specifies the expected structure for responses to a query about Hopfield Networks.
The model includes fields for the main answer, a confidence score, additional comments, and even a related joke.

```Python
from r2r import R2RClient, GenerationConfig
from pydantic import BaseModel
import json

# Define a response model
class ResponseModel(BaseModel):
    answer: str
    confidence: float
    comments: str
    related_joke: str

rag_response = client.retrieval.rag(
    query="What is a Hopfield Network?",
    rag_generation_config=GenerationConfig(
        response_format=ResponseModel
    )
)
```

## Processing the Response

Once you've received a response, you can parse it as JSON and validate it against your Pydantic model. This ensures that the response contains all required fields with the correct data types.

```Python
content = json.loads(rag_response.results.completion)
print(json.dumps(content, indent=2))

response_obj = ResponseModel.model_validate(content)
print("\nAs a Pydantic object:")
print(f"Confidence: {response_obj.confidence}")
print(f"Comments: {response_obj.comments}")
print(f"Related Joke: {response_obj.related_joke}")
print("\nDetailed Answer:")
print(response_obj.answer)
```

## Example Output

Here's what the output looks like when running the code above:

```zsh wordWrap
{
  "answer": "A Hopfield Network is a type of recurrent neural network introduced by John Hopfield in 1982, designed to function as an associative memory system. It consists of binary nodes with symmetric weights, and its dynamics are governed by an energy function that decreases over time, leading the network to stable states that represent stored memories [1], [2].",
  "confidence": 0.95,
  "comments": "The Hopfield Network is a foundational concept in neural networks, and its principles are widely studied in computational neuroscience and machine learning.",
  "related_joke": "Why did the neural network go to therapy? It had too many weights to carry!"
}

As a Pydantic object:
Confidence: 0.95
Comments: The Hopfield Network is a foundational concept in neural networks, and its principles are widely studied in computational neuroscience and machine learning.
Related Joke: Why did the neural network go to therapy? It had too many weights to carry!

Detailed Answer:
A Hopfield Network is a type of recurrent neural network introduced by John Hopfield in 1982, designed to function as an associative memory system. It consists of binary nodes with symmetric weights, and its dynamics are governed by an energy function that decreases over time, leading the network to stable states that represent stored memories [1], [2].
```


# MCP

## Overview

The R2R Retrieval System is a Model Context Protocol (MCP) server that enhances Claude with retrieval and search capabilities. This server enables Claude to search through your knowledge base, perform vector searches, graph searches, web searches, and document searches, making it a powerful tool for retrieving relevant information.

## Features

* **Vector Search**: Find relevant text chunks based on semantic similarity
* **Graph Search**: Explore relationships between entities in your knowledge graph
* **Web Search**: Retrieve information from online sources
* **Document Search**: Access and query local context documents
* **RAG (Retrieval-Augmented Generation)**: Generate answers based on retrieved context

## Installation

### Prerequisites

* Claude Desktop (macOS or Windows)
* Node.js
* Python 3.6 or higher
* `mcp` Python package

### Local Installation

1. Install the R2R MCP server locally:

```bash
pip install mcp
mcp install r2r/mcp.py -v R2R_API_URL=http://localhost:7272
```

2. Start your local R2R API service at the specified URL.

### Cloud Installation

For cloud deployment, use your API key:

```bash
pip install mcp
mcp install r2r/mcp.py -v R2R_API_KEY=your_api_key_here
```

## Adding to Claude Desktop

**Note: This section is only necessary if the pip installation method fails.** In most cases, the pip installation above should be sufficient to make the R2R server available to Claude.

1. Open Claude Desktop and access the Settings:
   * On macOS: Click on the Claude menu and select "Settings..."
   * On Windows: Click on the Claude menu and select "Settings..."

2. In Settings, click on "Developer" in the left sidebar, then click "Edit Config"

3. Add the R2R server to your configuration file:

```json
{
  "mcpServers": {
    "r2r": {
      "command": "mcp",
      "args": ["run", "/my/path/to/R2R/py/r2r/mcp.py"]
    }
  }
}
```

4. Save the configuration file and restart Claude Desktop

5. After restarting, you should see the hammer icon in the bottom right corner of the input box, indicating that MCP tools are available

## Using the R2R Retrieval System

Once configured, Claude can automatically use the R2R tools when appropriate. You can also explicitly request Claude to use these tools:

* **Search**: Ask Claude to search your knowledge base with specific queries
  Example: "Search for information about vector databases in our documentation"

* **RAG**: Request Claude to generate answers based on retrieved context
  Example: "Use RAG to answer: What are the best practices for knowledge graph integration?"

## Available Tools

The R2R server provides two primary tools:

1. **search**: Performs retrieval operations and returns formatted results
   * Searches across vector, graph, web, and document sources
   * Returns source IDs and content for further reference

2. **rag**: Performs Retrieval-Augmented Generation
   * Retrieves relevant context and generates an answer
   * Provides a coherent response based on the knowledge base

## Example Outputs

When using the search tool, you'll receive structured results like:

```
Vector Search Results:
Source ID [abc1234]:
Text content from the vector search...

Graph Search Results:
Source ID [def5678]:
Entity Name: Sample Entity
Description: This is a description of the entity...

Web Search Results:
Source ID [ghi9012]:
Title: Sample Web Page
Link: https://example.com
Snippet: A snippet from the web page...

Local Context Documents:
Full Document ID: jkl3456...
Shortened Document ID: jkl3456
Document Title: Sample Document
Summary: A summary of the document...

Chunk ID abc1234:
Text content from the document chunk...
```

## Troubleshooting

* If the server doesn't appear in Claude, check that the configuration file is formatted correctly
* Ensure that the R2R service is running at the specified URL for local installations
* Verify that your API key is valid for cloud installations
* Check the Claude Desktop logs for any error messages

## Next Steps

* Explore other MCP servers that can be integrated with Claude
* Consider building custom tools to extend the R2R functionality
* Contribute to the MCP community by sharing your experiences and use cases

***

For more information on MCP and its capabilities, refer to the official MCP documentation. For specific questions about the R2R Retrieval System, please contact your system administrator or developer.


# Web Development

&gt; Learn how to build webapps powered by RAG using R2R

Web developers can easily integrate R2R into their projects using the [R2R JavaScript client](https://www.npmjs.com/package/r2r-js).
For more extensive reference and examples of how to use the r2r-js library, we encourage you to look at the [R2R Application](https://github.com/SciPhi-AI/R2R-Application) and its source code.

## Hello R2R‚ÄîJavaScript

R2R gives developers configurable vector search and RAG right out of the box, as well as direct method calls instead of the client-server architecture seen throughout the docs:

```python r2r-js/examples/hello_r2r.js

const { r2rClient } = require("r2r-js");

const client = new r2rClient("http://localhost:7272");

async function main() {
  const files = [
    { path: "examples/data/raskolnikov.txt", name: "raskolnikov.txt" },
  ];

  const EMAIL = "admin@example.com";
  const PASSWORD = "change_me_immediately";
  console.log("Logging in...");
  await client.users.login(EMAIL, PASSWORD);

  console.log("Ingesting file...");
  const documentResult = await client.documents.create({
      file: { path: "examples/data/raskolnikov.txt", name: "raskolnikov.txt" },
      metadata: { title: "raskolnikov.txt" },
  });

  console.log("Document result:", JSON.stringify(documentResult, null, 2));

  console.log("Performing RAG...");
  const ragResponse = await client.rag({
    query: "What does the file talk about?",
    rag_generation_config: {
      model: "openai/gpt-4o",
      temperature: 0.0,
      stream: false,
    },
  });

  console.log("Search Results:");
  ragResponse.results.search_results.chunk_search_results.forEach(
    (result, index) =&gt; {
      console.log(`\nResult ${index + 1}:`);
      console.log(`Text: ${result.metadata.text.substring(0, 100)}...`);
      console.log(`Score: ${result.score}`);
    },
  );

  console.log("\nCompletion:");
  console.log(ragResponse.results.completion.choices[0].message.content);
}

main();
```

## r2r-js Client

### Installing

To get started, install the R2R JavaScript client with [npm](https://www.npmjs.com/package/r2r-js):

<tabs>
  <tab title="npm">
    ```zsh
    npm install r2r-js
    ```
  </tab>
</tabs>

### Creating the Client

First, we create the R2R client and specify the base URL where the R2R server is running:

```javascript
const { r2rClient } = require("r2r-js");

// http://localhost:7272 or the address that you are running the R2R server
const client = new r2rClient("http://localhost:7272");
```

### Log into the server

Sign into the server to authenticate the session. We'll use the default superuser credentials:

```javascript
const EMAIL = "admin@example.com";
const PASSWORD = "change_me_immediately";
console.log("Logging in...");
await client.users.login(EMAIL, PASSWORD);
```

### Ingesting Files

Specify the files that we'll ingest:

```javascript
const file = { path: "examples/data/raskolnikov.txt", name: "raskolnikov.txt" }
];
console.log("Ingesting file...");
const ingestResult = await client.documents.create(
  file: { path: "examples/data/raskolnikov.txt", name: "raskolnikov.txt" },
  metadata: { title: "raskolnikov.txt" },
)
console.log("Ingest result:", JSON.stringify(ingestResult, null, 2));
...
/* Ingest result: {
  "results": {
    "processed_documents": [
      "Document 'raskolnikov.txt' processed successfully."
    ],
    "failed_documents": [],
    "skipped_documents": []
  }
} */
```

This command processes the ingested, splits them into chunks, embeds the chunks, and stores them into your specified Postgres database. Relational data is also stored to allow for downstream document management, which you can read about in the [quickstart](/documentation/quickstart).

### Performing RAG

We'll make a RAG request,

```javascript
console.log("Performing RAG...");
  const ragResponse = await client.rag({
    query: "What does the file talk about?",
    rag_generation_config: {
      model: "openai/gpt-4o",
      temperature: 0.0,
      stream: false,
    },
  });

console.log("Search Results:");
  ragResponse.results.search_results.chunk_search_results.forEach(
    (result, index) =&gt; {
      console.log(`\nResult ${index + 1}:`);
      console.log(`Text: ${result.metadata.text.substring(0, 100)}...`);
      console.log(`Score: ${result.score}`);
    },
  );

  console.log("\nCompletion:");
  console.log(ragResponse.results.completion.choices[0].message.content);
...
/* Performing RAG...
Search Results:

Result 1:
Text: praeterire culinam eius, cuius ianua semper aperta erat, cogebatur. Et quoties praeteribat,
iuvenis ...
Score: 0.08281802143835804

Result 2:
Text: In vespera praecipue calida ineunte Iulio iuvenis e cenaculo in quo hospitabatur in
S. loco exiit et...
Score: 0.052743945852283036

Completion:
The file discusses the experiences and emotions of a young man who is staying in a small room in a tall house.
He is burdened by debt and feels anxious and ashamed whenever he passes by the kitchen of his landlady, whose
door is always open [1]. On a particularly warm evening in early July, he leaves his room and walks slowly towards
a bridge, trying to avoid encountering his landlady on the stairs. His room, which is more like a closet than a
proper room, is located under the roof of the five-story house, while the landlady lives on the floor below and
provides him with meals and services [2].
*/
```

## Connecting to a Web App

R2R can be easily integrated into web applications. We'll create a simple Next.js app that uses R2R for query answering. [We've created a template repository with this code.](https://github.com/SciPhi-AI/r2r-webdev-template)

Alternatively, you can add the code below to your own Next.js project.

![R2R Dashboard Overview](file:b1c6b3b4-5f48-4f87-b77d-74b5726dd5b4)

### Setting up an API Route

First, we'll create an API route to handle R2R queries. Create a file named `r2r-query.ts` in the `pages/api` directory:

<accordion title="r2r-query.ts" icon="code">
  ```typescript
  import { NextApiRequest, NextApiResponse } from 'next';
  import { r2rClient } from 'r2r-js';

  const client = new r2rClient("http://localhost:7272");

  export default async function handler(req: NextApiRequest, res: NextApiResponse) {
    if (req.method === 'POST') {
      const { query } = req.body;

      try {
        // Login with each request. In a production app, you'd want to manage sessions.
        await client.users.login("admin@example.com", "change_me_immediately");

        const response = await client.rag({
          query: query,
          rag_generation_config: {
            model: "openai/gpt-4o",
            temperature: 0.0,
            stream: false,
          }
        });

        res.status(200).json({ result: response.results.completion.choices[0].message.content });
      } catch (error) {
        res.status(500).json({ error: error instanceof Error ? error.message : 'An error occurred' });
      }
    } else {
      res.setHeader('Allow', ['POST']);
      res.status(405).end(`Method ${req.method} Not Allowed`);
    }
  }
  ```
</accordion>

This API route creates an R2R client, logs in, and processes the incoming query using the RAG method.

### Frontend: React Component

Next, create a React component to interact with the API. Here's an example `index.tsx` file:

<accordion title="index.tsx" icon="code">
  ```tsx
  import React, { useState } from 'react';
  import styles from '@/styles/R2RWebDevTemplate.module.css';

  const R2RQueryApp: React.FC = () =&gt; {
    const [query, setQuery] = useState('');
    const [result, setResult] = useState('');
    const [isLoading, setIsLoading] = useState(false);

    const performQuery = async () =&gt; {
      setIsLoading(true);
      setResult('');

      try {
        const response = await fetch('/api/r2r-query', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({ query }),
        });

        if (!response.ok) {
          throw new Error('Network response was not ok');
        }

        const data = await response.json();
        setResult(data.result);
      } catch (error) {
        setResult(`Error: ${error instanceof Error ? error.message : String(error)}`);
      } finally {
        setIsLoading(false);
      }
    };

    return (
      <div classname="{styles.appWrapper}">
        <h1 classname="{styles.title}">R2R Web Dev Template</h1>
        <p>A simple template for making RAG queries with R2R.
          Make sure that your R2R server is up and running, and that you've ingested files!
        </p>
        <p>
          Check out the <a href="https://r2r-docs.sciphi.ai/" target="_blank" rel="noopener noreferrer">R2R Documentation</a> for more information.
        </p>
        <input type="text" value="{query}" onchange="{(e)" ==""> setQuery(e.target.value)}
          placeholder="Enter your query here"
          className={styles.queryInput}
        /&gt;
        <button onclick="{performQuery}" disabled="{isLoading}" classname="{styles.submitButton}">
          Submit Query
        </button>
        {isLoading ? (
          <div classname="{styles.spinner}">
        ) : (
          <div classname="{styles.resultDisplay}">{result}</div>
        )}
      </div>
    );
  };

  export default R2RQueryApp;
  ```


This component creates a simple interface with an input field for the query and a button to submit it. When the button is clicked, it sends a request to the API route we created earlier and displays the result.

### Template Repository

For a complete working example, you can check out our template repository. This repository contains a simple Next.js app with R2R integration, providing a starting point for your own R2R-powered web applications.

For more advanced examples, check out the [source code for the R2R Dashboard.](https://github.com/SciPhi-AI/R2R-Application)

[R2R Web App Template Repository](https://github.com/SciPhi-AI/r2r-webdev-template)

To use this template:

1. Clone the repository
2. Install dependencies with `pnpm install`
3. Make sure your R2R server is running
4. Start the development server with `pnpm dev`

This template provides a foundation for building more complex applications with R2R, demonstrating how to integrate R2R's powerful RAG capabilities into a web interface.


# Evals

## Overview

This guide demonstrates how to evaluate your R2R RAG outputs using the Ragas evaluation framework.

In this tutorial, you will:

* Prepare a sample dataset in R2R
* Use R2R's `/rag` endpoint to perform Retrieval-Augmented Generation
* Install and configure Ragas for evaluation
* Evaluate the generated responses using multiple metrics
* Analyze evaluation traces for deeper insights

## Setting Up Ragas for R2R Evaluation

### Installing Ragas

First, install Ragas and its dependencies:

```python
%pip install ragas langchain-openai -q
```

### Configuring Ragas with OpenAI

Ragas uses an LLM to perform evaluations. Set up an OpenAI model as the evaluator:

```python
from langchain_openai import ChatOpenAI
from ragas.llms import LangchainLLMWrapper

# Make sure your OPENAI_API_KEY environment variable is set
llm = ChatOpenAI(model="gpt-4o-mini")
evaluator_llm = LangchainLLMWrapper(llm)

# If you'll be using embeddings for certain metrics
from langchain_openai import OpenAIEmbeddings
from ragas.embeddings import LangchainEmbeddingsWrapper
evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())
```

## Sample Dataset and R2R RAG Implementation

For this guide, we assume you have:

1. An initialized R2R client
2. A dataset about AI companies already ingested into R2R
3. Basic knowledge of R2R's RAG capabilities

Here's a quick example of using R2R's `/rag` endpoint to generate an answer:

```python
from r2r import R2RClient

client = R2RClient()  # Assuming R2R_API_KEY is set in your environment

query = "What makes Meta AI's LLaMA models stand out?"

search_settings = {
    "limit": 2,
    "graph_settings": {"enabled": False, "limit": 2},
}

response = client.retrieval.rag(
    query=query,
    search_settings=search_settings
)

print(response.results.generated_answer)
```

The output might look like:

```
Meta AI's LLaMA models stand out due to their open-source nature, which supports innovation and experimentation by making high-quality models accessible to researchers and developers [1]. This approach democratizes AI development, fostering collaboration across industries and enabling researchers without access to expensive resources to work with advanced AI models [2].
```

## Evaluating R2R with Ragas

Ragas provides a comprehensive evaluation framework specifically designed for RAG systems. The R2R-Ragas integration makes it easy to assess the quality of your R2R implementation.

### Creating a Test Dataset

First, prepare a set of test questions and reference answers:

```python
questions = [
    "Who are the major players in the large language model space?",
    "What is Microsoft's Azure AI platform known for?",
    "What kind of models does Cohere provide?",
]

references = [
    "The major players include OpenAI (GPT Series), Anthropic (Claude Series), Google DeepMind (Gemini Models), Meta AI (LLaMA Series), Microsoft Azure AI (integrating GPT Models), Amazon AWS (Bedrock with Claude and Jurassic), Cohere (business-focused models), and AI21 Labs (Jurassic Series).",
    "Microsoft's Azure AI platform is known for integrating OpenAI's GPT models, enabling businesses to use these models in a scalable and secure cloud environment.",
    "Cohere provides language models tailored for business use, excelling in tasks like search, summarization, and customer support.",
]
```

### Collecting R2R Responses

Generate responses using your R2R implementation:

```python
r2r_responses = []

search_settings = {
    "limit": 2,
    "graph_settings": {"enabled": False, "limit": 2},
}

for que in questions:
    response = client.retrieval.rag(query=que, search_settings=search_settings)
    r2r_responses.append(response)
```

### The R2R-Ragas Integration

Ragas includes a dedicated integration for R2R that handles the conversion of R2R's response format to Ragas's evaluation dataset format:

```python
from ragas.integrations.r2r import transform_to_ragas_dataset

# Convert R2R responses to Ragas format
ragas_eval_dataset = transform_to_ragas_dataset(
    user_inputs=questions, 
    r2r_responses=r2r_responses, 
    references=references
)

print(ragas_eval_dataset)
# Output: EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=3)
```

The `transform_to_ragas_dataset` function extracts the necessary components from R2R responses, including:

* The generated answer
* The retrieved context chunks
* Citation information

### Key Evaluation Metrics for R2R

Ragas offers several metrics that are particularly useful for evaluating R2R implementations:

```python
from ragas.metrics import AnswerRelevancy, ContextPrecision, Faithfulness
from ragas import evaluate

# Define the metrics to use
ragas_metrics = [
    AnswerRelevancy(llm=evaluator_llm),  # How relevant is the answer to the query?
    ContextPrecision(llm=evaluator_llm),  # How precisely were the right documents retrieved?
    Faithfulness(llm=evaluator_llm)       # Does the answer stick to facts in the context?
]

# Run the evaluation
results = evaluate(dataset=ragas_eval_dataset, metrics=ragas_metrics)
```

Each metric provides valuable insights:

* **Answer Relevancy**: Measures how well the R2R-generated response addresses the user's query
* **Context Precision**: Evaluates if R2R's retrieval mechanism is bringing back the most relevant documents
* **Faithfulness**: Checks if R2R's generated answers accurately reflect the information in the retrieved documents

### Interpreting Evaluation Results

The evaluation results show detailed scores for each sample and metric:

```python
# View results as a dataframe
df = results.to_pandas()
print(df)
```

Example output:

```
   user_input                                    retrieved_contexts                                           response                                          reference  answer_relevancy  context_precision  faithfulness
0  Who are the major players...                  [In the rapidly advancing field of...]                      The major players in the large language...         The major players include OpenAI...         1.000000              1.0     1.000000
1  What is Microsoft's Azure AI...              [Microsoft's Azure AI platform is famous for...]            Microsoft's Azure AI platform is known for...      Microsoft's Azure AI platform is...         0.948908              1.0     0.833333
2  What kind of models does Cohere provide?     [Cohere is well-known for its language models...]          Cohere provides language models tailored for...    Cohere provides language models...         0.903765              1.0     1.000000
```

### Advanced Visualization with Ragas App

For a more interactive analysis, upload results to the Ragas app:

```python
# Make sure RAGAS_APP_TOKEN is set in your environment
results.upload()
```

This generates a shareable dashboard with:

* Detailed scores per metric and sample
* Visual comparisons across metrics
* Trace information showing why scores were assigned
* Suggestions for improvement

You can examine:

* Which queries R2R handled well
* Where retrieval or generation could be improved
* Patterns in your RAG system's performance

## Advanced Evaluation Features

### Non-LLM Metrics for Fast Evaluation

In addition to LLM-based metrics, you can use non-LLM metrics for faster evaluations:

```python
from ragas.metrics import BleuScore

# Create a BLEU score metric
bleu_metric = BleuScore()

# Add it to your evaluation
quick_metrics = [bleu_metric]
quick_results = evaluate(dataset=ragas_eval_dataset, metrics=quick_metrics)
```

### Custom Evaluation Criteria with AspectCritic

For tailored evaluations specific to your use case, AspectCritic allows you to define custom evaluation criteria:

```python
from ragas.metrics import AspectCritic

# Define a custom evaluation aspect
custom_metric = AspectCritic(
    name="factual_accuracy",
    llm=evaluator_llm,
    definition="Verify if the answer accurately states company names, model names, and specific capabilities without any factual errors."
)

# Evaluate with your custom criteria
custom_results = evaluate(dataset=ragas_eval_dataset, metrics=[custom_metric])
```

### Training Your Own Metric

If you want to fine-tune metrics to your specific requirements:

1. Use the Ragas app to annotate evaluation results
2. Download the annotations as JSON
3. Train your custom metric:

```python
from ragas.config import InstructionConfig, DemonstrationConfig

demo_config = DemonstrationConfig(embedding=evaluator_embeddings)
inst_config = InstructionConfig(llm=evaluator_llm)

# Train your metric with your annotations
metric.train(
    path="your-annotations.json", 
    demonstration_config=demo_config, 
    instruction_config=inst_config
)
```

## Conclusion

This guide demonstrated how to use Ragas to thoroughly evaluate your R2R RAG implementation. By leveraging these evaluation tools, you can:

1. Measure the quality of your R2R system across multiple dimensions
2. Identify specific areas for improvement in retrieval and generation
3. Track performance improvements as you refine your implementation
4. Establish benchmarks for consistent quality

Through regular evaluation with Ragas, you can optimize your R2R configuration to deliver the most accurate, relevant, and helpful responses to your users.

For more information on R2R features, refer to the [R2R documentation](https://r2r-docs.sciphi.ai/). To explore additional evaluation metrics and techniques with Ragas, visit the [Ragas documentation](https://docs.ragas.io/).


# R2R Installation

Welcome to the R2R self-hosting installation guide. For those interested in a managed cloud solution, [refer to the quickstart here](/documentation/quickstart).

R2R offers powerful features for your RAG applications, including:

* **Flexibility**: Run with cloud-based LLMs or entirely on your local machine
* **State-of-the-Art Tech**: Advanced RAG techniques like [hybrid search](/documentation/search-and-rag), [graphs](/cookbooks/graphs), [advanced RAG](/documentation/advanced-rag), and [agentic RAG](/documentation/retrieval/agentic-rag).
* **Auth &amp; Orchestration**: Production must-haves like [auth](/documentation/user-auth) and [orchestration](/cookbooks/orchestration).

## Choose Your System

<cardgroup cols="{2}">
  <card title="R2R Light" icon="feather" href="/self-hosting/installation/light">
    A lightweight version of R2R, **perfect for quick prototyping and simpler applications**. Some advanced features, like orchestration may not be available.
  </card>

  <card title="R2R" icon="server" href="/self-hosting/installation/full">
    The full-featured R2R system, ideal **for advanced use cases and production deployments**. Includes all components and capabilities, such as **Hatchet** for orchestration and **Unstructured** for parsing.
  </card>
</cardgroup>

Choose the system that best aligns with your requirements and proceed with the installation guide.


# R2R Light Installation

This guide will walk you through installing and running R2R on your local system without using Docker. This method allows for more customization and control over the R2R source code.

## Prerequisites

Before starting, ensure you have the following installed and/or available in the cloud:

* Python 3.12 or higher
* pip (Python package manager)
* Git
* Postgres + pgvector

## Install the extra dependencies

First, install the with the additional `core` dependencies:

```zsh
pip install 'r2r[core]'
```

The `core` dependencies, combined with a Postgres database, provide the necessary components to deploy a user-facing R2R application into production.

If you need advanced features like orchestration or parsing with `Unstructured.io` then refer to the <a href="https://r2r-docs.sciphi.ai/self-hosting/installation/full"> full installation </a>.

## Environment Setup

R2R requires connections to various services. Set up the following environment variables based on your needs:

<accordiongroup>
  <accordion title="Cloud LLM Providers" icon="language">
    Refer to the [documentation here](/self-hosting/configuration/llm) for detailed information on LLM configuration inside R2R.

    ```zsh
     # Set cloud LLM settings
     export OPENAI_API_KEY=sk-...
     # export ANTHROPIC_API_KEY=...
     # ...
    ```

    Note, cloud providers are optional as R2R can be run entirely locally. For more information on local installation, [refer here](/self-hosting/local-rag).
  </accordion>

  <accordion title="Postgres+pgvector" icon="database">
    With R2R you can connect to your own instance of Postgres+pgvector or a remote cloud instance. [Refer here](/self-hosting/configuration/database) for detailed documentation on configuring Postgres inside R2R.

    ```zsh
     # Set Postgres+pgvector settings
     R2R_POSTGRES_USER=$YOUR_POSTGRES_USER
     R2R_POSTGRES_PASSWORD=$YOUR_POSTGRES_PASSWORD
     R2R_POSTGRES_HOST=$YOUR_POSTGRES_HOST
     R2R_POSTGRES_PORT=$YOUR_POSTGRES_PORT
     R2R_POSTGRES_DBNAME=$YOUR_POSTGRES_DBNAME
     R2R_PROJECT_NAME=$YOUR_PROJECT_NAME # see note below
    ```

    <note>
      The `R2R_PROJECT_NAME` environment variable defines the tables within your Postgres database where the selected R2R project resides. If the required tables for R2R do not exist then they will be created by R2R during initialization.
    </note>

    If you are unfamiliar with Postgres then <a href="https://supabase.com/docs"> Supabase's free cloud offering </a> is a good place to start.
  </accordion>

  <accordion title="Web Tool Providers" icon="globe">
    If you plan to use web-based tools with R2R's Agentic RAG features, you'll need to set up the following:

    ```bash
     # For web_search tool (uses Serper API)
     export SERPER_API_KEY=your_serper_api_key_here
     
     # For web_scrape tool (uses Firecrawl API)
     export FIRECRAWL_API_KEY=your_firecrawl_api_key_here
    ```

    You can obtain these API keys from:

    * Serper: [https://serper.dev/](https://serper.dev/)
    * Firecrawl: [https://www.firecrawl.dev/](https://www.firecrawl.dev/)

    <note>
      These environment variables are only required if you plan to use the `web_search` or `web_scrape` tools with the Agentic RAG functionality. R2R will function without these for local document operations.
    </note>
  </accordion>
</accordiongroup>

## Running R2R

After installing the r2r library, you can start R2R using the following command:

```zsh
python -m r2r.serve
```

For local LLM usage:

```zsh
export R2R_CONFIG_NAME=ollama
python -m r2r.serve
```

## Python Development Mode

For those looking to develop R2R locally:

1. Clone and install dependencies:
   ```zsh
   git clone https://github.com/SciPhi-AI/R2R.git
   cd R2R/py
   pip install -e .[core]
   ```

2. Setup environment:
   Follow the steps listed in the Environment Setup section above. Additionally, you may introduce a local .env file to make development easier, and you can customize your local `r2r.toml` to suit your specific needs.

3. Start your server:

```zsh
python -m r2r.serve
```

## Next Steps

After successfully installing R2R:

1. **Verify Installation**: Ensure all components are running correctly by accessing the R2R API at [http://localhost:7272/v3/health](http://localhost:7272/v3/health).

2. **Quick Start**: Follow our [R2R Quickstart Guide](/self-hosting/quickstart) to set up your first RAG application.

3. **In-Depth Tutorial**: For a more comprehensive understanding, work through our [R2R Walkthrough](/documentation/walkthrough).

4. **Customize Your Setup**: Configure R2R components with the [Configuration Guide](/self-hosting/configuration/overview).

If you encounter any issues during installation or setup, please use our [Discord community](https://discord.gg/p6KqD2kjtB) or [GitHub repository](https://github.com/SciPhi-AI/R2R) to seek assistance.


# R2R Full Installation

<warning>
  This installation guide is for Full R2R. For solo developers or teams prototyping, we recommend starting with 

  <a href="https://r2r-docs.sciphi.ai/self-hosting/installation/light">R2R Light</a>

  .
</warning>

This guide will walk you through installing and running R2R using Docker, which is the quickest and easiest way to get started.

## Prerequisites

* Docker installed on your system. If you haven't installed Docker yet, please refer to the [official Docker installation guide](https://docs.docker.com/engine/install/).

## Installation

<steps>
  <step title="Clone the R2R repository">
    Clone the R2R repository for access to the Docker compose files:

    ```zsh
    git clone https://github.com/SciPhi-AI/R2R.git
    cd R2R/docker
    ```
  </step>

  <step title="Set environment variables">
    <note>
      The full R2R installation uses a pre-built custom configuration [`full.toml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/full.toml) rather than the default [`r2r.toml`](https://github.com/SciPhi-AI/R2R/blob/main/py/r2r/r2r.toml).
    </note>

    Navigate to the env directory and set up your environment variables:

    ```zsh
    cd env
    # Edit r2r-full.env with your preferred text editor
    sudo nano r2r-full.env
    ```

    ### Required Environment Variables

    ### Configuration Selection (choose one)

    | Variable          | Description                     | Default         |
    | ----------------- | ------------------------------- | --------------- |
    | `R2R_CONFIG_NAME` | Uses a predefined configuration | `full` (OpenAI) |
    | `R2R_CONFIG_PATH` | Path to your custom TOML config | None            |

    &gt; Set `R2R_CONFIG_NAME=full_ollama` to use local models instead of cloud providers.

    ### LLM API Keys (at least one required)

    | Provider  | Environment Variable | Used With                          |
    | --------- | -------------------- | ---------------------------------- |
    | OpenAI    | `OPENAI_API_KEY`     | `R2R_CONFIG_NAME=full`             |
    | Anthropic | `ANTHROPIC_API_KEY`  | Custom config or runtime overrides |
    | Ollama    | `OLLAMA_API_BASE`    | `R2R_CONFIG_NAME=full_ollama`      |

    &gt; For Ollama, the default value is `http://host.docker.internal:11434`

    ### External Agent Tools (optional)

    | Tool         | Environment Variable | Purpose                | Provider Link                           |
    | ------------ | -------------------- | ---------------------- | --------------------------------------- |
    | `web_search` | `SERPER_API_KEY`     | Enable web search tool | [Serper](https://serper.dev/)           |
    | `web_scrape` | `FIRECRAWL_API_KEY`  | Enable web scrape tool | [Firecrawl](https://www.firecrawl.dev/) |

    <note>
      These environment variables are only required if you plan to use the `web_search` or `web_scrape` tools with the Agentic RAG functionality. R2R will function without these for local document operations.
    </note>

    When starting R2R with agent tools, include these variables with your launch command:

    ```bash
    # Example with Cloud LLMs and Agent Tools
    export OPENAI_API_KEY=sk-...
    export ANTHROPIC_API_KEY=sk-...
    export SERPER_API_KEY=your_serper_api_key_here
    export FIRECRAWL_API_KEY=your_firecrawl_api_key_here

    COMPOSE_PROFILES=postgres docker compose -f compose.full.yaml up -d
    ```

    <a href="https://r2r-docs.sciphi.ai/self-hosting/configuration/overview">See the full configuration guide</a> for additional options.
  </step>

  <step title="Custom Configuration (Optional)">
    If you're using a custom configuration file instead of the built-in options, follow these steps:

    1. Create a TOML configuration file in the `user_configs` directory:

    ```zsh
    # Navigate to the user_configs directory
    cd user_configs

    # Create a new configuration file (e.g., my_config.toml)
    touch my_config.toml

    # Edit the file with your configuration settings
    nano my_config.toml
    ```

    2. Update your `r2r-full.env` file to point to this configuration:

    ```
    R2R_CONFIG_PATH=/app/user_configs/my_config.toml
    ```

    <error>
      The path in `R2R_CONFIG_PATH` must use the container path (`/app/user_configs/`), not your local system path.

      Make sure the specified configuration file actually exists in the `user_configs` directory. The application will fail to start if it cannot find the file at the specified path.
    </error>

    For examples and configuration templates, see the [Configuration Guide](/self-hosting/configuration/overview).
  </step>

  <step title="Start the R2R services">
    Return to the docker directory and start the services:

    ```zsh
    cd ..
    docker compose -f compose.full.yaml --profile postgres up -d
    # `--profile postgres` can be omitted when using external Postgres
    ```
  </step>

  <step title="Interact with R2R">
    Ether install the Python or JS SDK, or navigate to [http://localhost:7273](http://localhost:7273) to interact with R2R via the dashboard.

    To install the Python SDK:

    ```zsh
    pip install r2r
    ```
  </step>
</steps>

## Next Steps

After successfully installing R2R:

1. **Verify Installation**: Ensure all components are running correctly by accessing the R2R API at [http://localhost:7272/v3/health](http://localhost:7272/v3/health).

2. **Quick Start**: Follow our [R2R Quickstart Guide](/self-hosting/quickstart) to set up your first RAG application.

3. **In-Depth Tutorial**: For a more comprehensive understanding, work through our [R2R Walkthrough](/documentation/walkthrough).

4. **Customize Your Setup**: [Configuration](/self-hosting/configuration/overview) your R2R system.

If you encounter any issues during installation or setup, please use our [Discord community](https://discord.gg/p6KqD2kjtB) or [GitHub repository](https://github.com/SciPhi-AI/R2R) to seek assistance.


# Quickstart

Getting started with R2R is easy.

<steps>
  ### Deployment Checks

  Start by checking that you have correctly deployed your R2R instance locally:

  ```zsh
  curl http://localhost:7272/v3/health
  # {"results":{"response":"ok"}}
  ```

  ### Install the SDK

  R2R offers a Python and JavaScript SDK to interact with.

  <tabs>
    <tab title="Python">
      ```zsh
      pip install r2r
      ```
    </tab>

    <tab title="JavaScript">
      ```zsh
      npm i r2r-js
      ```
    </tab>
  </tabs>

  ### Ingesting files

  When you ingest files into R2R, the server accepts the task, processes and chunks the file, and generates a summary of the document.

  <tabs>
    <tab title="Python">
      ```python
      client.documents.create_sample(hi_res=True)
      # to ingest your own document, client.documents.create(file_path="/path/to/file")
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      clients.documents.createSample({ ingestionMode: "hi-res" })
      // to ingest your own document, client.documents.create({filePath: })
      ```
    </tab>
  </tabs>

  Example output:

  ```plaintext
  IngestionResponse(message='Document created and ingested successfully.', task_id=None, document_id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'))
  ```

  ### Getting file status

  After file ingestion is complete, you can check the status of your documents by listing them.

  <tabs>
    <tab title="Python">
      ```python
      client.documents.list()
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      clients.documents.list()
      ```
    </tab>

    <tab title="Curl">
      ```zsh
      curl -X GET https://api.sciphi.ai/v3/documents \
        -H "Content-Type: application/json"
      ```
    </tab>
  </tabs>

  Example output:

  ```plaintext
  [
    DocumentResponse(
      id=UUID('e43864f5-a36f-548e-aacd-6f8d48b30c7f'), 
      collection_ids=[UUID('122fdf6a-e116-546b-a8f6-e4cb2e2c0a09')], 
      owner_id=UUID('2acb499e-8428-543b-bd85-0d9098718220'), 
      document_type=<documenttype.pdf: 'pdf'="">, 
      metadata={'title': 'DeepSeek_R1.pdf', 'version': 'v0'}, 
      version='v0', 
      size_in_bytes=1768572, 
      ingestion_status=<ingestionstatus.success: 'success'="">, 
      extraction_status=<graphextractionstatus.pending: 'pending'="">, 
      created_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 126759, tzinfo=TzInfo(UTC)), 
      updated_at=datetime.datetime(2025, 2, 8, 3, 31, 39, 160114, tzinfo=TzInfo(UTC)), 
      ingestion_attempt_number=None, 
      summary="The document contains a comprehensive overview of DeepSeek-R1, a series of reasoning models developed by DeepSeek-AI, which includes DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero utilizes large-scale reinforcement learning (RL) without supervised fine-tuning, showcasing impressive reasoning capabilities but facing challenges like readability and language mixing. To enhance performance, DeepSeek-R1 incorporates multi-stage training and cold-start data, achieving results comparable to OpenAI's models on various reasoning tasks. The document details the models' training processes, evaluation results across multiple benchmarks, and the introduction of distilled models that maintain reasoning capabilities while being smaller and more efficient. It also discusses the limitations of current models, such as language mixing and sensitivity to prompts, and outlines future research directions to improve general capabilities and efficiency in software engineering tasks. The findings emphasize the potential of RL in developing reasoning abilities in large language models and the effectiveness of distillation techniques for smaller models.", summary_embedding=None, total_tokens=29673)] total_entries=1
    ), ...
  ]
  ```

  ### Executing a search

  Perform a search query:

  <tabs>
    <tab title="Python">
      ```python
      client.retrieval.search(
        query="What is DeepSeek R1?",
      )
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      client.retrieval.search({
        query: "What is DeepSeek R1?",
      })
      ```
    </tab>

    <tab title="Curl">
      ```zsh
      curl -X POST https://api.sciphi.ai/v3/retrieval/search \
        -H "Content-Type: application/json" \
        -d '{
          "query": "What is DeepSeek R1?"
        }'
      ```
    </tab>
  </tabs>

  The search query will use basic similarity search to find the most relevant documents. You can use advanced search methods like [hybrid search](/documentation/hybrid-search) or [graph search](/documentation/graphs) depending on your use case.

  Example output:

  ```plaintext
  AggregateSearchResult(
    chunk_search_results=[
      ChunkSearchResult(
        score=0.643, 
        text="Document Title: DeepSeek_R1.pdf
        Text: could achieve an accuracy of over 70%.
        DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a
        models ability to follow format instructions. These improvements can be linked to the inclusion
        of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL
        training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,
        indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its
        significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale
        RL, which not only boosts reasoning capabilities but also improves performance across diverse
        domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an
        average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that
        DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying
        its robustness across multiple tasks."
      ), ...
    ],
    graph_search_results=[],
    web_search_results=[],
    context_document_results=[]
  )
  ```

  ### RAG

  Generate a RAG response:

  <tabs>
    <tab title="Python">
      ```python
      client.retrieval.rag(
        query="What is DeepSeek R1?",
      )
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      client.retrieval.rag({
        query: "What is DeepSeek R1?",
      })
      ```
    </tab>

    <tab title="Curl">
      ```zsh
      curl -X POST https://api.sciphi.ai/v3/retrieval/rag \
        -H "Content-Type: application/json" \
        -d '{
          "query": "What is DeepSeek R1?"
        }'
      ```
    </tab>
  </tabs>

  Example output:

  ```plaintext
  RAGResponse(
    generated_answer='DeepSeek-R1 is a model that demonstrates impressive performance across various tasks, leveraging reinforcement learning (RL) and supervised fine-tuning (SFT) to enhance its capabilities. It excels in writing tasks, open-domain question answering, and benchmarks like IF-Eval, AlpacaEval2.0, and ArenaHard [1], [2]. DeepSeek-R1 outperforms its predecessor, DeepSeek-V3, in several areas, showcasing its strengths in reasoning and generalization across diverse domains [1]. It also achieves competitive results on factual benchmarks like SimpleQA, although it performs worse on the Chinese SimpleQA benchmark due to safety RL constraints [2]. Additionally, DeepSeek-R1 is involved in distillation processes to transfer its reasoning capabilities to smaller models, which perform exceptionally well on benchmarks [4], [6]. The model is optimized for English and Chinese, with plans to address language mixing issues in future updates [8].', 
    search_results=AggregateSearchResult(
      chunk_search_results=[ChunkSearchResult(score=0.643, text=Document Title: DeepSeek_R1.pdf ...)]
    ),
    citations=[Citation(index=1, rawIndex=1, startIndex=305, endIndex=308, snippetStartIndex=288, snippetEndIndex=315, sourceType='chunk', id='e760bb76-1c6e-52eb-910d-0ce5b567011b', document_id='e43864f5-a36f-548e-aacd-6f8d48b30c7f', owner_id='2acb499e-8428-543b-bd85-0d9098718220', collection_ids=['122fdf6a-e116-546b-a8f6-e4cb2e2c0a09'], score=0.6433466439465674, text='Document Title: DeepSeek_R1.pdf\n\nText: could achieve an accuracy of over 70%.\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\nmodels ability to follow format instructions. These improvements can be linked to the inclusion\nof instruction-following...]
    metadata={'id': 'chatcmpl-B0BaZ0vwIa58deI0k8NIuH6pBhngw', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}}], 'created': 1739384247, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion', 'service_tier': 'default', 'system_fingerprint': 'fp_4691090a87', ...}
  ```

  ### Streaming RAG

  Generate a streaming RAG response:

  <tabs>
    <tab title="Python">
      ```python
      from r2r import (
          CitationEvent,
          FinalAnswerEvent,
          MessageEvent,
          SearchResultsEvent,
          R2RClient,
      )

      client = R2RClient("http://localhost:7272")

      result_stream = client.retrieval.rag(
          query="What is DeepSeek R1?",
          search_settings={"limit": 25},
          rag_generation_config={"stream": True},
      )

      # can also do a switch on `type` field
      for event in result_stream:
          if isinstance(event, SearchResultsEvent):
              print("Search results:", event.data)
          elif isinstance(event, MessageEvent):
              print("Partial message:", event.data.delta)
          elif isinstance(event, CitationEvent):
              print("New citation detected:", event.data.raw_index)
          elif isinstance(event, FinalAnswerEvent):
              print("Final answer:", event.data.generated_answer)
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      ...
      ```
    </tab>

    <tab title="Curl">
      ```bash
      curl -X POST https://api.sciphi.ai/v3/retrieval/rag \
        -H "Content-Type: application/json" \
        -d '{
          "query": "What is DeepSeek R1?"
        }'
      ```
    </tab>
  </tabs>

  Example output:

  ```plaintext
  Search results: id='run_1' object='rag.search_results' data={'chunk_search_results': [{'id': '1e40ee7e-2eef-524f-b5c6-1a1910e73ccc', 'document_id': '652075c0-3a43-519f-9625-f581e7605bc5', 'owner_id': '2acb499e-8428-543b-bd85-0d9098718220', 'collection_ids': ['122fdf6a-e116-546b-a8f6-e4cb2e2c0a09'], 'score': 0.7945216641038179, 'text': 'data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately ... 
  ...
  Partial message: {'content': [MessageDelta(type='text', text={'value': 'Deep', 'annotations': []})]}
  Partial message: {'content': [MessageDelta(type='text', text={'value': 'Seek', 'annotations': []})]}
  Partial message: {'content': [MessageDelta(type='text', text={'value': '-R', 'annotations': []})]}
  ...
  Final answer: DeepSeek-R1 is a large language model developed by the DeepSeek-AI research team. It is a reasoning model that has been trained using multi-stage training and cold-start data before reinforcement learning (RL). The model demonstrates superior performance on various benchmarks, including MMLU, MMLU-Pro, GPQA Diamond, and FRAMES, particularly in STEM-related questions. ...
  ```

  ### Reasoning Agent with RAG (agentic-rag)

  Using the R2R Reasoning Agent, retrieval-augmented generation is combined with step-by-step reasoning to produce higher quality responses from your documents.

  <tabs>
    <tab title="Python">
      ```python
      streaming_response = client.retrieval.agentic-rag(
        message={"role":"user", "content": "What does deepseek r1 imply?"},
        rag_generation_config={
          "stream": True,
          "model": "anthropic/claude-3-5-sonnet-20241022",
        }
      )

      for chunk in streaming_response:
          print(chunk)
      ```
    </tab>

    <tab title="JavaScript">
      ```javascript
      // 1) Initiate a streaming RAG request
      const resultStream = await client.retrieval.rag({
      query: "What is DeepSeek R1?",
      searchSettings: { limit: 25 },
      ragGenerationConfig: { stream: true },
      });

      // 2) Check if we got an async iterator (streaming)
      if (Symbol.asyncIterator in resultStream) {
      // 2a) Loop over each event from the server
      for await (const event of resultStream) {
          switch (event.event) {
          case "search_results":
              console.log("Search results:", event.data);
              break;
          case "message":
              console.log("Partial message delta:", event.data.delta);
              break;
          case "citation":
              console.log("New citation event:", event.data);
              break;
          case "final_answer":
              console.log("Final answer:", event.data.generated_answer);
              break;
          // ... add more cases if you have other event types, e.g. tool_call / tool_result
          default:
              console.log("Unknown or unhandled event:", event);
          }
      }
      } else {
      // 2b) If streaming was NOT enabled or server didn't send SSE,
      //     we'd get a single response object instead.
      console.log("Non-streaming RAG response:", resultStream);
      }
      ```
    </tab>
  </tabs>

  Example output:

  ```plaintext
  <thought>Calling function: local_search, with payload {"query":"DeepSeek R1"}</thought>
  <thought>The search results provide a comprehensive overview of DeepSeek-R1, highlighting its capabilities and performance across various benchmarks and tasks. DeepSeek-R1 is a reasoning model developed by DeepSeek-AI, which leverages reinforcement learning (RL) and instruction-following data to enhance its performance. It excels in tasks such as writing, open-domain question answering, and handling fact-based queries. The model outperforms its predecessor, DeepSeek-V3, in several areas, although it falls short in some complex tasks like function calling and multi-turn interactions. DeepSeek-R1 also demonstrates strong performance in educational tasks and creative writing, showcasing its versatility and robustness.Key points about DeepSeek-R1 include:- It achieves impressive results on benchmarks like IF-Eval, AlpacaEval2.0, and ArenaHard, indicating strengths in writing and question answering [Source 1].- The model is used as a teacher to distill reasoning capabilities into smaller models, which also perform well on benchmarks [Source 2].- It outperforms DeepSeek-V3 on factual benchmarks like SimpleQA but has limitations in language mixing and certain complex tasks [Sources 3, 5].- DeepSeek-R1 demonstrates expert-level performance in coding tasks and strong results in educational benchmarks like MMLU and GPQA Diamond [Sources 6, 9].Overall, DeepSeek-R1 is a powerful model with a focus on reasoning and instruction-following, achieving competitive performance across a wide range of tasks.</thought>
  <response>DeepSeek-R1 is a reasoning model developed by DeepSeek-AI, known for its strong performance in writing tasks, open-domain question answering, and handling fact-based queries. It leverages reinforcement learning and instruction-following data to enhance its capabilities. The model outperforms its predecessor, DeepSeek-V3, in several areas and is used to distill reasoning capabilities into smaller models. Despite its strengths, it has limitations in complex tasks like function calling and language mixing. Overall, DeepSeek-R1 is a versatile and robust model with competitive performance across various benchmarks.
  ```
</response></graphextractionstatus.pending:></ingestionstatus.success:></documenttype.pdf:></steps>

## Additional Features

R2R offers the additional features below to enhance your document management and user experience.

### Graphs

R2R provides powerful entity and relationshipo extraction capabilities that enhance document understanding and retrieval. These can leveraged to construct knowledge graphs inside R2R. The system can automatically identify entities, build relationships between them, and create enriched knowledge graphs from your document collection.

<cardgroup cols="{2}">
  <card title="Knowledge Graphs" icon="diagram-project" href="/documentation/graphs">
    Automatically extract entities and relationships from documents to form knowledge graphs.
  </card>
</cardgroup>

### Users and Collections

R2R provides a complete set of user authentication and management features, allowing you to implement secure and feature-rich authentication systems or integrate with your preferred authentication provider. Further, collections exist to enable efficient access control and organization of users and documents.

<cardgroup cols="{2}">
  <card title="User Auth Cookbook" icon="key" href="/documentation/user-auth">
    Learn how to implement user registration, login, email verification, and more using R2R's built-in authentication capabilities.
  </card>

  <card title="Collections Cookbook" icon="database" href="/documentation/collections">
    Discover how to create, manage, and utilize collections in R2R for granular access control and document organization.
  </card>
</cardgroup>

## Next Steps

Now that you have a basic understanding of R2R's core features, you can explore more advanced topics:

* Dive into [document ingestion](/documentation/documents) and [the document reference](/api-and-sdks/documents/documents).
* Learn about [search and RAG](/documentation/hybrid-search) and the [retrieval reference](/api-and-sdks/retrieval/retrieval).
* Try advanced techniques like [knowledge-graphs](/documentation/graphs) and refer to the [graph reference](/api-and-sdks/graphs/graphs).
* Learn about [user authentication](/documentation/user-auth) to secure your application permissions and [the users API reference](/api-and-sdks/users/users).
* Organize your documents using [collections](/api-and-sdks/collections/collections) for granular access control.


# Overview

&gt; Configure your R2R deployment

R2R was built with configuration in mind and utilizes [TOML](https://toml.io) configuration files to define server-side variables.

The levels of configuration that are supported are:

1. **Server-side Configuration**: Define default configuration for your R2R deployment.
2. **Runtime Settings**: Dynamically override configuration settings when making API calls.

## Server-side Configuration

R2R's configuration format works by override. Default configuration values are defined in the [`r2r.toml`](https://github.com/SciPhi-AI/R2R/blob/main/py/r2r/r2r.toml) file.

A number of pre-defined configuration files ship with R2R, detailed below. For a complete list of configurable parameters and their defaults, refer to our [`all_possible_config.toml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/all_possible_config.toml) file.

<error>
  Editing pre-defined configurations while running R2R with Docker will not have an effect; refer to the 

  [installation guide](/self-hosting/installation/full)

   for instructions on how to use custom configs with Docker.
</error>

| Configuration File                                                                                      | Usage                                                        |
| ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
| [r2r.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/r2r/r2r.toml)                                  | The default R2R configuration file.                          |
| [full.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/full.toml)                       | Includes orchestration with Hatchet.                         |
| [full\_azure.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/full_azure.toml)          | Includes orchestration with Hatchet and Azure OpenAI models. |
| [full\_lm\_studio.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/full_lm_studio.toml) | Includes orchestration with Hatchet and LM Studio models.    |
| [full\_ollama.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/full_ollama.toml)        | Includes orchestration with Hatchet and Ollama models.       |
| [r2r\_azure.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/r2r_azure.toml)            | Configured to run Azure OpenAI models.                       |
| [gemini.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/gemini.toml)                   | Configured to run Gemini models.                             |
| [lm\_studio.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/lm_studio.toml)            | Configured to run LM Studio models.                          |
| [ollama.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/ollama.toml)                   | Configured to run Ollama models.                             |
| [r2r\_with\_auth.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/r2r_with_auth.toml)   | Configured to require user verification.                     |
| [tavily.toml](https://github.com/SciPhi-AI/R2R/blob/main/py/core/configs/tavily.toml)                   | Configured to use the Tavily tool.                           |

### Custom Configuration Files

To create your own custom configuration:

1. Create a new file named `my_r2r.toml` in your project directory.
2. Add only the settings you wish to customize. For example:

```toml my_r2r.toml
[app]
# LLM used for user-facing responses (high-quality outputs)
quality_llm = "openai/gpt-4o"
# LLM used for internal summarizations and similar tasks (fast responses)
fast_llm = "openai/gpt-4o-mini"

[completion]
  [completion.generation_config]
  temperature = 0.7
  top_p = 0.9
  max_tokens_to_sample = 1024
  stream = false
  add_generation_kwargs = {}
```

3. Launch the R2R server with your custom configuration:

```zsh
export R2R_CONFIG_PATH=path_to_your_config
python -m r2r.serve
```

R2R will use your specified settings, falling back to the defaults defined in the main configuration files for any unspecified options.

## Runtime Settings

When calling endpoints, such as `retrieval/search` or `retrieval/rag`, you can override server-side configurations on-the-fly. This allows for dynamic control over search settings, model selection, prompt customization, and more.

For example, using the Python SDK:

```python
client = R2RClient("http://localhost:7272")

response = client.retrieval.rag(
    "Who was Aristotle?",
    rag_generation_config={
        "model": "anthropic/claude-3-haiku-20240307",  # Overrides the default quality_llm
        "temperature": 0.7
    },
    search_settings={
        "limit": 100,           # Number of search results to return
        "use_hybrid_search": True  # Enable semantic + full-text search
    }
)
```

[Refer here](/self-hosting/configuration/retrieval/overview) to learn more about configuring and dynamically setting your retrieval system.


# Database

&gt; Configuring your Database

<aside>
  **Configure the database provider in your TOML file:**

  <tabs>
    <tab title="Postgres">
      ```toml
      [database]
      provider = "postgres"
      user = "postgres"
      password = "postgres"
      host = "localhost"
      port = 5432
      db_name = "postgres"
      project_name = "r2r_default"
      ```

      <paramfield path="provider" type="string" required="{true}">
        The database provider to use. Must be `postgres`.
      </paramfield>

      <paramfield path="user" type="string">
        The Postgres user that will interact with the database. Defaults to `postgres`.
      </paramfield>

      <paramfield path="password" type="string">
        The Postgres user's password. Defaults to `postgres`
      </paramfield>

      <paramfield path="host" type="string">
        The hostname where the Postgres database is run. Defaults to `localhost`.
      </paramfield>

      <paramfield path="port" type="string">
        The port where the Postgres database is accessible. Defaults to `5432`.
      </paramfield>

      <paramfield path="db_name" type="string">
        The Postgre database name to connect to. Defaults to `postgres`.
      </paramfield>

      <paramfield path="project_name" type="string">
        The Postgre schema name that will contain all R2R tables. Defaults to `r2r_default`.
      </paramfield>
    </tab>
  </tabs>
</aside>

## Postgres Database

R2R uses Postgres as the sole provider for relational and vector search queries. This means that Postgres is involved in handling authentication, document management, and search across R2R. For robust search capabilities, R2R leverages the `pgvector` extension and `ts_rank` to implement [customizable hybrid search](/documentation/search-and-rag).

<note>
  R2R chooses Postgres as its core technology for several reasons:

  * **Versatility**: Postgres is a robust, advanced database that can handle both relational data and vector embeddings.
  * **Simplicity**: By using Postgres for both traditional data and vector search, R2R eliminates the need for complex syncing between separate databases.
  * **Familiarity**: Many developers are already comfortable with Postgres, making it easier to integrate R2R into existing workflows.
  * **Extensibility**: Postgres's rich ecosystem of extensions allows R2R to leverage advanced features and optimizations.

  Read more about [Postgres here](https://www.postgresql.org/).
</note>

## Database Configuration

To customize the database settings, you can modify the `database` section in your TOML configuration file. [Learn more about working with R2R config files](/self-hosting/configuration/overview).

Some Postgres settings have corresponding environment variables that can be set, as an alternative.

The `database` section of the R2R configuration file is described to the right; alternatively, set the following environment variables:

```zsh
R2R_POSTGRES_USER=‚Ä¶
R2R_POSTGRES_PASSWORD=‚Ä¶
R2R_POSTGRES_HOST=‚Ä¶
R2R_POSTGRES_PORT=‚Ä¶
R2R_POSTGRES_DBNAME=‚Ä¶
R2R_PROJECT_NAME=‚Ä¶
```

## Advanced Postgres Features in R2R

R2R leverages several advanced Postgres features to provide powerful search and retrieval capabilities:

### pgvector Extension

R2R uses the `pgvector` extension to enable efficient vector similarity search. This is crucial for semantic search operations. The `collection.py` file defines a custom `Vector` type that interfaces with `pgvector`:

```python
class Vector(UserDefinedType):
    # ... (implementation details)

    class comparator_factory(UserDefinedType.Comparator):
        def l2_distance(self, other):
            return self.op("&lt;-&gt;", return_type=Float)(other)

        def max_inner_product(self, other):
            return self.op("&lt;#&gt;", return_type=Float)(other)

        def cosine_distance(self, other):
            return self.op("&lt;=&gt;", return_type=Float)(other)
```

This allows R2R to perform efficient vector similarity searches using different distance measures.

### Hybrid Search

R2R implements a sophisticated hybrid search which combines full-text search and vector similarity search. This approach provides more accurate and contextually relevant results. Key components of the hybrid search include:

1. **Full-Text Search**: Utilizes Postgres's built-in full-text search capabilities with `ts_rank` and `websearch_to_tsquery`.
2. **Semantic Search**: Performs vector similarity search using `pgvector`.
3. **Reciprocal Rank Fusion (RRF)**: Merges results from full-text and semantic searches.

In addition, R2R offers robust logical filters on metadata (e.g., operations such as `eq`, `neq`, `gt`, `gte`, `lt`, `lte`, `like`, `ilike`, `in`, and `nin`). Refer to the [retrieval API documentation](/api-and-sdks/retrieval/retrieval) for all available inputs.

### Indexing

#### Vector Similarity Search

R2R supports two primary indexing methods for vector similarity search through pgvector: **HNSW** (Hierarchical Navigable Small World) and **IVF-Flat** (Inverted File with Flat Storage).

* **HNSW** offers faster search times and better recall but requires more memory and slower build times, making it ideal for production environments where query speed is critical.
* **IVF-Flat** provides a balanced approach with faster index construction and lower memory usage, suitable for scenarios requiring a trade-off between build speed and query performance.

Both methods support cosine, L2, and inner product distance measures. See the [index API Reference](/api-and-sdks/indices/indices) for detailed configuration options and management endpoints.

#### Full-Text Search

R2R uses GIN (Generalized Inverted Index) indexing to optimize full-text searches:

```python
Index(f"idx_{name}_fts", "fts", postgresql_using="gin"),
```

This indexing strategy allows for efficient full-text search.

### JSON Support

R2R leverages Postgres's JSONB type for flexible metadata storage:

```python
Column(
    "metadata",
    postgresql.JSONB,
    server_default=text("'{}'::jsonb"),
    nullable=False,
)
```

This allows for efficient storage and querying of structured metadata alongside vector embeddings.

## Performance Considerations

When setting up Postgres for R2R, consider the following performance optimizations:

1. **Indexing**: Ensure proper indexing for both full-text and vector searches. While R2R automatically creates necessary indexes, you may need to optimize them based on your specific usage patterns.
2. **Hardware**: For large-scale deployments, consider using dedicated Postgres instances with sufficient CPU and RAM to handle vector operations efficiently.
3. **Vacuuming**: Regular vacuuming helps maintain database performance, especially for tables with frequent updates or deletions.
4. **Partitioning**: For very large datasets, consider table partitioning to improve query performance.

&gt; **Maintenance Tip:** Regularly schedule maintenance tasks (e.g., VACUUM and REINDEX operations) to ensure optimal performance.

## Additional Recommendations

* **Extension Installation:**\
  Ensure that your Postgres instance has the required extensions installed. For example, you can run:
  ```sql
  CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
  CREATE EXTENSION IF NOT EXISTS vector;
  CREATE EXTENSION IF NOT EXISTS pg_trgm;
  CREATE EXTENSION IF NOT EXISTS fuzzystrmatch;
  ```
  Refer to the [Postgres documentation](https://www.postgresql.org/docs/current/) for further details.

* **Schema Upgrades:**\
  If you encounter errors related to old table names or schema mismatches, please run `r2r db upgrade` (or follow the migration documentation) to update your database schema to the latest version.

By incorporating these recommendations, you ensure that your Postgres configuration is not only functionally complete but also optimized for performance and maintainability. The clarification regarding `app.project_name` helps users understand that this value drives the schema (or table name prefix) for all R2R tables, even though it isn‚Äôt set directly within the `[database]` section.


# File Storage

&gt; Configuring File Storage

<aside>
  **Configure the file storage provider in your TOML file:**

  <tabs>
    <tab title="Postgres">
      ```toml
      [file]
      provider = "postgres"
      ```

      <paramfield path="provider" type="string" required="{true}">
        The file storage provider to use‚Äîeither `postgres` or `s3`
      </paramfield>
    </tab>

    <tab title="S3">
      ```toml
      [file]
      provider = "s3"
      bucket_name = ""
      endpoint_url = ""
      region_name = ""
      aws_access_key_id = ""
      aws_secret_access_key = ""
      ```

      <paramfield path="provider" type="string" required="{true}">
        The file storage provider to use‚Äîeither `postgres` or `s3`
      </paramfield>

      <paramfield path="endpoint_url" type="string" required="{true}">
        The URL of the S3 endpoint to connect to
      </paramfield>

      <paramfield path="bucket_name" type="string" required="{true}">
        The name of the S3 bucket to use for storage
      </paramfield>

      <paramfield path="region_name" type="string" required="{true}">
        The AWS region where the S3 bucket is located (e.g., us-east-1)
      </paramfield>

      <paramfield path="aws_access_key_id" type="string" required="{true}">
        The AWS access key ID used for authentication
      </paramfield>

      <paramfield path="aws_secret_access_key" type="string" required="{true}">
        The AWS secret access key used for authentication
      </paramfield>
    </tab>
  </tabs>
</aside>

Your R2R deployment can be configured to utilize the [Postgres large object module](https://www.postgresql.org/docs/current/lo.html)
or [S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) to store the raw files that have been sent to the server.

While Postgres large objects provide a convenient single-system approach, S3-compatible storage offers several advantages for
production deployments including better scalability, cost efficiency, backup simplicity, and reduced database load.

For smaller deployments or development environments, Postgres storage is perfectly adequate. Consider choosing S3 if your file storage
needs grow or when deploying to production environments with higher availability requirements.

## File Storage Configuration

To customize the file storage settings, you can modify the `file` section in your TOML configuration file. [Learn more about working with R2R config files](/self-hosting/configuration/overview).

## Postgres Large Objects

When using Postgres as your file storage provider, R2R leverages Postgres' large object module to efficiently store binary data. This approach keeps your files and metadata in a single database, simplifying your infrastructure.
To use Postgres for file storage, simply set the provider to "postgres" in your configuration file as shown in the example above. No additional parameters are required, as R2R will use your existing Postgres connection.

## S3

R2R's S3 integration is designed to work with any S3 compatible storage API.

### MinIO

[MinIO](https://github.com/minio/minio) offers high-performance, S3 compatible object storage that is software-defined and 100% open source under GNU AGPL v3.

The R2R Docker Compose file includes a MinIO profile, which will pull the official MinIO image. The MinIO container exposes a web console, availible at [http://localhost:9001](http://localhost:9001).
The default username and password for the console are both `minioadmin`, and configurable in the `/docker/env/minio.env` file.


  <img src="file://c1c04c54-73a0-4890-971a-1775bfa7a876/" alt="MinIO console with files.">


### Supabase

At the time which this documentation was written, there were known bugs with Supabase S3 storage, which resulted in errors for the following file
types; this issue seems related to bugs reported in the [Supabase Github repository](https://github.com/supabase/storage/issues/639).

Note that the following file types may not work with Supabase S3 storage: `HEIC, JPEG, MSG, PPT, TIFF, P7S, PNG, JPG, JSON, XLS, DOC`


# Embedding

&gt; Configuring Embeddings

<aside>
  **Configure the embedding provider in your TOML file:**

  <tabs>
    <tab title="LiteLLM">
      ```toml
      [embedding]
      provider = "litellm"
      base_model = "openai/text-embedding-3-small"
      base_dimension = 512
      concurrent_request_limit = 256
      max_retries = 3
      initial_backoff = 1.0
      max_backoff = 64.0

      [completion_embedding]
      provider = "litellm"
      base_model = "openai/text-embedding-3-small"
      base_dimension = 512
      concurrent_request_limit = 256
      ```
    </tab>

    <tab title="OpenAI">
      ```toml
      [embedding]
      provider = "openai"
      base_model = "text-embedding-3-small"
      base_dimension = 512
      concurrent_request_limit = 256
      max_retries = 3
      initial_backoff = 1.0
      max_backoff = 64.0

      [completion_embedding]
      provider = "openai"
      base_model = "text-embedding-3-small"
      base_dimension = 512
      concurrent_request_limit = 256
      ```
    </tab>

    <tab title="Ollama">
      ```toml
      [embedding]
      provider = "ollama"
      base_model = "mxbai-embed-large"
      base_dimension = 1_024
      batch_size = 128
      concurrent_request_limit = 2
      max_retries = 3
      initial_backoff = 1.0
      max_backoff = 64.0

      [completion_embedding]
      provider = "ollama"
      base_model = "mxbai-embed-large"
      base_dimension = 1_024
      batch_size = 128
      concurrent_request_limit = 2
      ```
    </tab>
  </tabs>

  <paramfield path="provider" type="string" required="{true}">
    The embedding provider to use. Either `litellm`, `openai`, or `ollama`.
  </paramfield>

  <paramfield path="base_model" type="string" required="{true}">
    The model to request embeddings from.
  </paramfield>

  <paramfield path="base_dimension" type="number" required="{true}">
    The number of dimensions of your embedding. For models that do not support passing a dimension parameter, set `nan`.
  </paramfield>

  <paramfield path="batch_size" type="number">
    Controls how many text inputs are processed together in a single API call to the Ollama embedding service.
  </paramfield>

  <paramfield path="concurrent_request_limit" type="number">
    The number of concurrent requests to be made at a time.
  </paramfield>

  <paramfield path="max_retries" type="number">
    The number of times to retry getting an embedding, should a request fail.
  </paramfield>

  <paramfield path="initial_backoff" type="number">
    The initial backoff time applied to a failed embedding.
  </paramfield>

  <paramfield path="max_backoff" type="number">
    The largest possible backoff time that can be applied to a failed embedding.
  </paramfield>
</aside>

R2R uses embeddings as the foundation for semantic search and similarity matching capabilities. The embedding system is responsible for converting text into high-dimensional vectors that capture semantic meaning, enabling powerful search and retrieval operations.

## Embedding Configuration

The embedding settings are configurable through the `embedding` section in your TOML configuration file. [Learn more about working with R2R config files](/self-hosting/configuration/overview).

### Environment Variables

Provider dependent environment variables must be set. These may include:

```zsh
OPENAI_API_KEY=‚Ä¶
AZURE_API_KEY=‚Ä¶
GEMINI_API_KEY=‚Ä¶
```

## LiteLLM

[LiteLLM](https://github.com/BerriAI/litellm) offers a Python SDK to call 100+ LLM APIs in OpenAI format

## OpenAI

The OpenAI embedding provider makes direct use of the [OpenAI Python SDK](https://github.com/openai/openai-python).

## Ollama

The Ollama embedding provider makes direct use of the [Ollama Python SDK](https://github.com/ollama/ollama-python).


# LLMs

&gt; Configuring LLMs

<aside>
  **Configure the LLM provider in your TOML file:**

  <tabs>
    <tab title="R2R">
      ```toml
      [app]
      fast_llm = "openai/gpt-4o-mini"
      quality_llm = "openai/gpt-4o"
      reasoning_llm = "openai/o3-mini"
      planning_llm = "anthropic/claude-3-7-sonnet-20250219"

      [completion]
      provider = "r2r"
      concurrent_request_limit = 64

        [completion.generation_config]
        temperature = 0.1
        top_p = 1
        max_tokens_to_sample = 4_096
        stream = false
        add_generation_kwargs = { }
      ```
    </tab>

    <tab title="LiteLLM">
      ```toml
      [app]
      fast_llm = "openai/gpt-4o-mini"
      quality_llm = "openai/gpt-4o"
      reasoning_llm = "openai/o3-mini"
      planning_llm = "anthropic/claude-3-7-sonnet-20250219"

      [completion]
      provider = "litellm"
      concurrent_request_limit = 64

        [completion.generation_config]
        temperature = 0.1
        top_p = 1
        max_tokens_to_sample = 4_096
        stream = false
        add_generation_kwargs = { }
      ```
    </tab>

    <tab title="OpenAI">
      ```toml
      [app]
      fast_llm = "gpt-4o-mini"
      quality_llm = "gpt-4o"
      reasoning_llm = "o3-mini"
      planning_llm = "o3"

      [completion]
      provider = "r2r"
      concurrent_request_limit = 64

        [completion.generation_config]
        temperature = 0.1
        top_p = 1
        max_tokens_to_sample = 4_096
        stream = false
        add_generation_kwargs = { }
      ```
    </tab>
  </tabs>

  Application Level

  <paramfield path="fast_llm" type="string" required="{true}">
    The LLM used for internal operations, like deriving conversation names.
  </paramfield>

  <paramfield path="quality_llm" type="string" required="{true}">
    The LLM used for user-facing output, like RAG replies.
  </paramfield>

  <paramfield path="reasoning_llm" type="string" required="{true}">
    Reasoning model, used for the `research` agent.
  </paramfield>

  <paramfield path="planning_llm" type="string" required="{true}">
    Planning model, used for the `research` agent.
  </paramfield>

  Completion Level

  <paramfield path="provider" type="string" required="{true}">
    The LLM provider to use. Either `r2r`, `lite_llm`, `anthropic`, `azure_foundry`, or `openai`.
  </paramfield>

  <paramfield path="concurrent_request_limit" type="number">
    The number of concurrent requests to be made at a time.
  </paramfield>

  Generation Config Level

  <paramfield path="temperature" type="number">
    Controls randomness in output generation. Lower values (0.0-0.3) produce more deterministic responses, while higher values (0.7-1.0) produce more creative, varied responses. Defaults to 0.1.
  </paramfield>

  <paramfield path="top_p" type="number">
    Controls diversity via nucleus sampling. The model considers tokens comprising the top\_p probability mass. Value range: 0.0-1.0, where 1.0 considers all tokens. Defaults to 1.0.
  </paramfield>

  <paramfield path="max_tokens_to_sample" type="number">
    Maximum number of tokens the model will generate in response. Defaults to 4,096.
  </paramfield>

  <paramfield path="stream" type="boolean">
    When true, responses are streamed as they're generated rather than returned only when complete. Defaults to false.
  </paramfield>

  <paramfield path="add_generation_kwargs" type="object">
    Additional parameters to pass to the model. Format as a dictionary. Defaults to empty object.
  </paramfield>
</aside>

R2R uses Large Language Models (LLMs) as the core reasoning engine for RAG operations, providing sophisticated text generation and analysis capabilities.

<note>
  R2R uses LiteLLM as to route LLM requests because of their provider flexibility. Read more about [LiteLLM here](https://docs.litellm.ai/).
</note>

## LLM Configuration

The LLM system can be customized through the `completion` section in your `r2r.toml` file. [Learn more about working with R2R config files](/self-hosting/configuration/overview).

For more detailed information on configuring other search and RAG settings at runtime, please refer to the [RAG Configuration documentation](/self-hosting/configuration/retrieval/rag).

### Environment Variables

Provider dependent environment variables must be set. These may include:

```zsh
OPENAI_API_KEY=‚Ä¶
ANTHROPIC_API_KEY=‚Ä¶
AZURE_API_KEY=‚Ä¶
```

## R2R

The R2R LLM provider offers a robust gateway for OpenAI, Anthropic, and Azure Foundry models.

## LiteLLM

[LiteLLM](https://github.com/BerriAI/litellm) offers a Python SDK to call 100+ LLM APIs in OpenAI format

## OpenAI

The OpenAI LLM provider makes direct use of the [OpenAI Python SDK](https://github.com/openai/openai-python).


# Email

&gt; Configuring Email

<aside>
  **Configure the email provider in your TOML file:**

  <tabs>
    <tab title="Console Mock">
      ```toml
      [email]
      provider = "console_mock"
      ```

      <paramfield path="provider" type="string" required="{true}">
        The email provider to use. Either `console_mock`, `smtp`, `sendgrid`, or `mailersend`.
      </paramfield>
    </tab>

    <tab title="SMTP">
      ```toml
      [email]
      provider = "smtp"
      smtp_host = ""
      smtp_port =
      smtp_username = ""
      smtp_password = ""
      from_email = ""
      ```

      <paramfield path="provider" type="string" required="{true}">
        The email provider to use. Either `console_mock`, `smtp`, `sendgrid`, or `mailersend`.
      </paramfield>

      <paramfield path="smtp_host" type="string" required="{true}">
        The hostname or IP address of the SMTP server used for sending emails.
      </paramfield>

      <paramfield path="smtp_port" type="number" required="{true}">
        The port number of the SMTP server. Common values are 25 (unencrypted), 465 (SSL), or 587 (TLS).
      </paramfield>

      <paramfield path="smtp_username" type="string" required="{true}">
        The username for authenticating with the SMTP server.
      </paramfield>

      <paramfield path="smtp_password" type="string" required="{true}">
        The password for authenticating with the SMTP server.
      </paramfield>

      <paramfield path="from_email" type="string" required="{true}">
        The email address that will appear as the sender for all outgoing emails.
      </paramfield>
    </tab>

    <tab title="Sendgrid">
      ```toml
      [email]
      provider = "sendgrid"
      verify_email_template_id=""
      reset_password_template_id=""
      password_changed_template_id=""
      frontend_url=""
      from_email=""
      ```

      <paramfield path="provider" type="string" required="{true}">
        The email provider to use. Either `console_mock`, `smtp`, `sendgrid`, or `mailersend`.
      </paramfield>

      <paramfield path="verify_email_template_id" type="string" required="{true}">
        The email template to use for email verification.
      </paramfield>

      <paramfield path="reset_password_template_id" type="string" required="{true}">
        The email template to use for password resets.
      </paramfield>

      <paramfield path="password_changed_template_id" type="string" required="{true}">
        The email template to use for password changes.
      </paramfield>

      <paramfield path="frontend_url" type="string" required="{true}">
        The URL to redirect users to after they verify.
      </paramfield>

      <paramfield path="from_email" type="string" required="{true}">
        The email address that will appear as the sender for all outgoing emails.
      </paramfield>
    </tab>

    <tab title="Mailersend">
      ```toml
      [email]
      provider = "mailersend"
      verify_email_template_id=""
      reset_password_template_id=""
      password_changed_template_id=""
      frontend_url=""
      from_email=""
      ```

      <paramfield path="provider" type="string" required="{true}">
        The email provider to use. Either `console_mock`, `smtp`, `sendgrid`, or `mailersend`.
      </paramfield>

      <paramfield path="verify_email_template_id" type="string" required="{true}">
        The email template to use for email verification.
      </paramfield>

      <paramfield path="reset_password_template_id" type="string" required="{true}">
        The email template to use for password resets.
      </paramfield>

      <paramfield path="password_changed_template_id" type="string" required="{true}">
        The email template to use for password changes.
      </paramfield>

      <paramfield path="frontend_url" type="string" required="{true}">
        The URL to redirect users to after they verify.
      </paramfield>

      <paramfield path="from_email" type="string" required="{true}">
        The email address that will appear as the sender for all outgoing emails.
      </paramfield>
    </tab>
  </tabs>
</aside>

R2R integrates with email service providers, providing a full-service authentication system all within R2R. For a complete example of working with email providers, visit our [email cookbook](/cookbooks/email).

## Email Configuration

To customize the email provider, you can modify the `email` section in your TOML configuration file. [Learn more about working with R2R config files](/self-hosting/configuration/overview).

### Environment Variables

Provider dependent environment variables must be set. These may include:

```zsh
MAILERSEND_API_KEY=‚Ä¶
SENDGRID_API_KEY=‚Ä¶
```

## Sendgrid

[SendGrid](https://sendgrid.com) is a cloud-based platform and email service provider that helps businesses send transactional and marketing emails. It offers features like an email API, SMTP service for reliable sending, and email marketing campaigns.

## MailerSend

[MailerSend](https://www.mailersend.com) offers email sending with high deliverability, easy integration, and a robust API supported by extensive documentation.


# Crypto

&gt; Configuring Cryptography

<aside>
  **Configure the crypto provider in your TOML file:**

  <tabs>
    <tab title="bcrypt">
      ```toml
      [crypto]
      provider = "bcrypt"
      ```

      <paramfield path="provider" type="string" required="{true}">
        The crypto provider to use. Either `bcrypt` or `nacl`.
      </paramfield>
    </tab>

    <tab title="NaCL">
      ```toml
      [crypto]
      provider = "nacl"
      ```

      <paramfield path="provider" type="string" required="{true}">
        The crypto provider to use. Either `bcrypt` or `nacl`.
      </paramfield>
    </tab>
  </tabs>
</aside>

R2R supports two of the main python cryptography providers.

## Cryptography Configuration

To customize the crypto provider, you can modify the `crypto` section in your TOML configuration file. [Learn more about working with R2R config files](/self-hosting/configuration/overview).

## bcrypt

The [Python bcrypt library](https://github.com/pyca/bcrypt) provides "acceptable password hashing for your software and your servers." It's specifically designed for secure password storage and is resistant to brute force attacks due to its adaptive work factor. While bcrypt is excellent for password hashing, it has a more limited cryptographic scope compared to NaCL.

## NaCL

[PyNaCl](https://github.com/pyca/pynacl) is a Python binding to libsodium, which is a fork of the Networking and Cryptography library. It is recommended to utilize NaCL in production deployments for enhanced security.


# Auth

&gt; Configuring Auth

<aside>
  **Configure the auth provider in your TOML file:**

  <tabs>
    <tab title="R2R">
      ```toml
      [auth]
      provider = "r2r"
      access_token_lifetime_in_minutes =
      refresh_token_lifetime_in_days =
      require_authentication = false
      require_email_verification = false
      default_admin_email = "admin@example.com"
      default_admin_password = "change_me_immediately"
      ```
    </tab>

    <tab title="Supabase">
      ```toml
      [auth]
      provider = "supabase"
      access_token_lifetime_in_minutes =
      refresh_token_lifetime_in_days =
      require_authentication = false
      require_email_verification = false
      default_admin_email = "admin@example.com"
      default_admin_password = "change_me_immediately"
      ```
    </tab>
  </tabs>

  <paramfield path="provider" type="string" required="{true}">
    The authentication provider to use. Either `r2r` or `supabase`.
  </paramfield>

  <paramfield path="access_token_lifetime_in_minutes" type="number">
    The duration (in minutes) that access tokens remain valid before requiring renewal. Defaults to 60000 minutes (approximately 41.7 days).
  </paramfield>

  <paramfield path="refresh_token_lifetime_in_days" type="number">
    The duration (in days) that refresh tokens remain valid. Refresh tokens are used to obtain new access tokens without requiring re-authentication. Defaults to 7 days.
  </paramfield>

  <paramfield path="require_authentication" type="boolean">
    When set to true, all API endpoints require authentication. When false, endpoints may be accessed without authentication. Defaults to false.
  </paramfield>

  <paramfield path="require_email_verification" type="boolean">
    When set to true, users must verify their email addresses before being granted full access. Defaults to false.
  </paramfield>

  <paramfield path="default_admin_email" type="string">
    The email address for the default administrator account that is created during initial setup. Defaults to "[admin@example.com](mailto:admin@example.com)".
  </paramfield>

  <paramfield path="default_admin_password" type="string">
    The password for the default administrator account. Should be changed immediately after initial setup. Defaults to "change\_me\_immediately".
  </paramfield>
</aside>

R2R provides a flexible authentication system that supports both server-side configuration and runtime customization. The authentication system manages user registration, login, session management, and access control.

## Authentication Configuration

To customize the auth provider, you can modify the `auth` section in your TOML configuration file. [Learn more about working with R2R config files](/self-hosting/configuration/overview).

## R2R

R2R has a robust set of user management auth features built-in. This includes user and token management, verification, and other account related functionalities. Refer directly to the [Users API Reference](/api-and-sdks/users) for more details.

## Supabase

Supabase Auth provides a complete authentication system that stores user data directly in the Supabase Postgres database associated with your project. It supports JWT-based authentication, social logins, and enterprise SSO options while integrating seamlessly with Row Level Security for fine-grained access control. When configured as your auth provider, it gives you all the essential user management features without needing additional authentication services.


# Scheduler

&gt; Configuring the Scheduler

<aside>
  **Configure the scheduler in your TOML file:**

  <tabs>
    <tab title="APScheduler">
      ```toml
      [scheduler]
      provider = "apscheduler"
      ```

      <paramfield path="provider" type="string" required="{true}">
        The scheduler to use. Currently, only `apscheduler` is supported.
      </paramfield>
    </tab>
  </tabs>
</aside>

## Scheduler Configuration

To customize the scheduler, you can modify the `scheduler` section in your TOML configuration file. [Learn more about working with R2R config files](/self-hosting/configuration/overview).


# Orchestration

&gt; Configuring Orchestration

<aside>
  **Configure the orchestration provider in your TOML file:**

  <tabs>
    <tab title="Simple">
      ```toml
      [crypto]
      provider = "simple"
      ```

      <paramfield path="provider" type="string" required="{true}">
        The orchestration provider to use. Either `simple` or `hatchet`.
      </paramfield>
    </tab>

    <tab title="Hatchet">
      ```toml
      [crypto]
      provider = "hatchet"
      kg_creation_concurrency_limit = 32
      ingestion_concurrency_limit = 16
      kg_concurrency_limit = 8
      ```

      <paramfield path="provider" type="string" required="{true}">
        The orchestration provider to use. Either `simple` or `hatchet`.
      </paramfield>

      <paramfield path="kg_creation_concurrency_limit" type="number">
        The maximum number of concurrent knowledge graph creation workflows that can be run at once.
      </paramfield>

      <paramfield path="ingestion_concurrency_limit" type="number">
        The maximum number of concurrent ingestion workflows that can be run at once.
      </paramfield>

      <paramfield path="kg_concurrency_limit" type="number">
        The maximum number of concurrent knowledge graph creation workflows that can be run at once.
      </paramfield>
    </tab>
  </tabs>
</aside>

R2R provides an integration with [Hatchet](https://hatchet.run) to run orchestrated workflows. For a complete example of working with Hatchet, visit our [orchestration cookbook](/cookbooks/orchestration).

## Orchestration Configuration

To customize the orchestration provider, you can modify the `orchestration` section in your TOML configuration file. [Learn more about working with R2R config files](/self-hosting/configuration/overview).

## Simple

R2R's 'simple' orchestration is a misnomer, as it indicates synchronous, non-orchestrated workflows. It is only suggested to use 'simple' orchestration for testing.

## Hatchet

[Hatchet](https://hatchet.run) offers a high-throughput, low-latency computing service that is built on an open-source, fault-tolerant queue, allowing work to be delivered as fast, without dropping a single task.

The R2R Docker Compose file includes a Hatchet profile, which will pull the official Hatchet image. The Hatchet container exposes a web dashboard, availible at [http://localhost:7274](http://localhost:7274).
The default username and password for the console are `admin@example.com` and `Admin123!!`.


# Agent

&gt; Configuring the Agent

<aside>
  **Configure the agent in your TOML file:**

  ```toml
  [agent]
  rag_agent_static_prompt = "static_rag_agent"
  rag_agent_dynamic_prompt = "dynamic_rag_agent"
  rag_tools = ["search_file_descriptions", "search_file_knowledge", "get_file_content"]
  research_tools = ["rag", "reasoning", "critique", "python_executor"]
  ```

  <paramfield path="rag_agent_static_prompt" type="string" required="{true}">
    A static prompt which does not give the agent specifics about the user's documents and collections. [See prompt here](https://github.com/SciPhi-AI/R2R/blob/97d498cd526cc907cac1ca9e41d19191d77bf19e/py/core/providers/database/prompts/static_rag_agent.yaml).
  </paramfield>

  <paramfield path="rag_agent_dynamic_prompt" type="string" required="{true}">
    A dynamic prompt which gives the agent specifics about the user's documents and collections. [See prompt here](https://github.com/SciPhi-AI/R2R/blob/97d498cd526cc907cac1ca9e41d19191d77bf19e/py/core/providers/database/prompts/dynamic_rag_agent.yaml).
  </paramfield>

  <paramfield path="rag_tools" type="string">
    The tools that are available to the `rag` agent.

    # The following tools are available to the `research` agent
  </paramfield>
</aside>

R2R's Agentic RAG orchestrates multi-step reasoning with Retrieval-Augmented Generation (RAG). The agent is configurable

Learn more about the [agent here](/documentation/retrieval/agentic-rag).


# Data Ingestion

&gt; Configuring ingestion

## Introduction

R2R's ingestion workflows transforms raw documents into structured, searchable content. It supports a wide range of file types and can run in different modes and configurations to suit your performance and quality requirements.

Data ingestion seamlessly integrates with R2R's vector databases and knowledge graphs, enabling advanced retrieval, analysis, and entity/relationship extraction at scale.

### Supported File Types

R2R supports ingestion of the following document types:

| Category          | File types                                |
| ----------------- | ----------------------------------------- |
| Image             | `.bmp`, `.heic`, `.jpeg`, `.png`, `.tiff` |
| MP3               | `.mp3`                                    |
| PDF               | `.pdf`                                    |
| CSV               | `.csv`                                    |
| E-mail            | `.eml`, `.msg`, `.p7s`                    |
| EPUB              | `.epub`                                   |
| Excel             | `.xls`, `.xlsx`                           |
| HTML              | `.html`                                   |
| Markdown          | `.md`                                     |
| Org Mode          | `.org`                                    |
| Open Office       | `.odt`                                    |
| Plain text        | `.txt`                                    |
| PowerPoint        | `.ppt`, `.pptx`                           |
| reStructured Text | `.rst`                                    |
| Rich Text         | `.rtf`                                    |
| TSV               | `.tsv`                                    |
| Word              | `.doc`, `.docx`                           |
| Code              | `.py`, `.js`, `.ts`, `.css`               |

### Deployment Options

R2R ingestion works in two main deployment modes:

* **Light**:\
  Uses R2R's built-in parsing for synchronous ingestion. This mode is simple, fast, and supports all file types locally. It's ideal for lower-volume scenarios or quick testing.

* **Full**:\
  Employs workflow orchestration to run asynchronous ingestion tasks at higher throughput. It can leverage external providers like `unstructured_local` or `unstructured_api` for more advanced parsing capabilities and hybrid (text + image) analysis.

### Ingestion Modes

When creating or updating documents, you can select an ingestion mode based on your needs:

* **`fast`**: Prioritizes speed by skipping certain enrichment steps like summarization.
* **`hi-res`**: Aims for high-quality extraction, potentially leveraging visual language models for PDFs and images. Recommended for complex or multimodal documents.
* **`custom`**: Offers full control via `ingestion_config`, allowing you to tailor parsing, chunking, and enrichment parameters.

## Core Concepts

### Document Processing

Ingestion in R2R covers the entire lifecycle of a document's preparation for retrieval:

1. **Parsing**: Converts source files into text.
2. **Chunking**: Breaks text into semantic segments.
3. **Embedding**: Transforms segments into vector representations for semantic search.
4. **Storing**: Persists chunks and embeddings for retrieval.
5. **Knowledge Graph Integration**: Optionally extracts entities and relationships for graph-based analysis.

Each ingested document is associated with user permissions and metadata, enabling comprehensive access control and management.

## Ingestion Architecture

R2R's ingestion is modular and extensible:

```mermaid
graph TD
    A[Input Documents] --&gt; B[Parsing]
    B --&gt; C[Embedding]
    B --&gt; D[Graph Building]
    C --&gt; E[Vector Database]
    D --&gt; F[Graph Database]
```

This structure allows you to customize components (e.g., choose a different parser or embedding model) without disrupting the entire system.

### Multimodal Support

For documents that contain images, complex layouts, or mixed media (like PDFs), using `hi-res` mode can unlock visual language model (VLM) capabilities. On a **full** deployment, `hi-res` mode may incorporate `unstructured_local` or `unstructured_api` to handle these advanced parsing scenarios.

## Configuration

### Key Configuration Areas

Ingestion behavior is primarily managed through your `r2r.toml` configuration file:

```toml
[ingestion]
provider = "r2r" # or `unstructured_local` | `unstructured_api`
chunking_strategy = "recursive"
chunk_size = 1024
chunk_overlap = 512
```

* **Provider**: Determines which parsing engine is used (`r2r` built-in or `unstructured_*` providers).
* **Chunking Strategy &amp; Parameters**: Control how text is segmented into chunks.
* **Other Settings**: Adjust file parsing logic, excluded parsers, and integration with embeddings or knowledge graphs.

### Configuration Impact

Your ingestion settings influence:

1. **[Postgres Configuration](/self-hosting/configuration/database)**:\
   Ensures that vector and metadata storage are optimized for semantic retrieval.

2. **[Embedding Configuration](/self-hosting/configuration/embedding)**:\
   Defines the vector models and parameters used to embed document chunks and queries.

3. **Ingestion Settings Themselves**:\
   Affect parsing complexity, chunk sizes, and the extent of enrichment during ingestion.

## Document Management

### Document Ingestion

R2R supports multiple ingestion methods:

* **File Ingestion**: Provide a file path and optional metadata:
  ```python
  ingest_response = client.documents.create(
      file_path="path/to/file.txt",
      metadata={"key1": "value1"},
      ingestion_mode="fast", # choose fast, hi-res, or custom
      # ingestion_config = {...} # `custom` setting allows for full specification
  )
  ```

* **Direct Chunk Ingestion**: Supply pre-processed text segments:
  ```python
  chunks = ["Pre-chunked content", "other pre-chunked content", ...]
  ingest_response = client.documents.create(chunks=chunks)
  ```

## Next Steps

* Review [Embedding Configuration](/self-hosting/configuration/embedding) to optimize semantic search.
* Check out other configuration guides for integrating retrieval and knowledge graph capabilities.


# Retrieval Configuration

&gt; Configure your retrieval system

## Introduction

Search in R2R combines vector-based semantic search and knowledge graph querying to provide powerful information retrieval capabilities. The system leverages both semantic similarity and relationship-based context to deliver accurate and contextually relevant results.

<note>
  R2R's search capabilities are built on [Postgres](/self-hosting/configuration/database), which provides:

  * Vector similarity search through the `pgvector` extension
  * Full-text search using `ts_rank` and `websearch_to_tsquery`
  * Efficient indexing with HNSW and IVF-Flat methods
  * Flexible metadata filtering using JSONB
  * Feature-complete user and document management

  This integrated approach ensures high performance and reliability while simplifying the overall architecture.
</note>

## Server-Side Configuration

The base configuration for search capabilities is defined in your `r2r.toml` file:

```toml
[embedding]
provider = "litellm"
base_model = "openai/text-embedding-3-small"
base_dimension = 512
batch_size = 128
concurrent_request_limit = 256
```

These settings directly impact how R2R performs search operations, as embeddings are used during semantic search. See the [embedding configuration](/self-hosting/configuration/embedding) for detailed parameter information.

## Vector Search Configuration

Vector search can be configured both through server-side settings and runtime parameters:

```python
chunk_settings = {
    "use_semantic_search": True,
    "filters": ["$and": [{"document_type": {"$eq": "article"}}, {"collection_ids: {"$overlap": "c3291abf-8a4e-5d9d-80fd-232ef6fd8526"]}],
    "limit": 20,
    "use_hybrid_search": True,
    "selected_collection_ids": [
}
```

For hybrid search, additional weights can be specified:

```python
hybrid_settings = {
    "full_text_weight": 1.0,
    "semantic_weight": 5.0
}
```

See the [Search API Reference](/api-and-sdks/retrieval/retrieval) for complete parameter details.

## Knowledge Graph Search Configuration

Knowledge graph search provides relationship-aware search capabilities:

```python
graph_settings = {
    "enabled": True,
    "entity_types": ["Person", "Organization"],
    "relationships": ["worksFor", "foundedBy"]
}
```

See the [Knowledge Graph API Reference](/api-and-sdks/graphs/graphs) for complete parameter details.

## Retrieval Architecture

```mermaid
graph TD
    A[User Query]
    A --&gt; C[Vector Search]
    A --&gt; D[Knowledge Graph Search]
    C --&gt; E[Search Results]
    D --&gt; E
```

## Usage Examples

### Basic Search

```python
response = client.retrieval.search(
    "query",
    search_settings={
        "chunk_settings": chunk_settings,
        "graph_settings": graph_settings
    }
)
```

### Advanced Filtering

```python
filters = {
    "$and": [
        {"publication_date": {"$gte": "2023-01-01"}},
        {"author": {"$in": ["John Doe", "Jane Smith"]}}
    ]
}

search_settings["filters"] = filters

response = client.retrieval.search("query", search_settings=search_settings)
```


# RAG

&gt; Configure your end-to-end RAG

## RAG Customization

RAG (Retrieval-Augmented Generation) in R2R can be extensively customized to suit various use cases. The main components for customization are:

1. **Generation Configuration**: Control the language model's behavior.
2. **Search Settings**: Fine-tune the retrieval process.
3. **Task Prompt Override**: Customize the system prompt for specific tasks.

### LLM Provider Configuration

Refer to the LLM configuration [page here](/self-hosting/configuration/llm).

### Retrieval Configuration

Refer to the retrieval configuration [page here](/self-hosting/configuration/retrieval/overview).

### Combining LLM and Retrieval Configuration for RAG

The `rag_generation_config` parameter allows you to customize the language model's behavior. Default settings are set on the server-side using the `r2r.toml`, as described in in previous configuraiton guides. These settings can be overridden at runtime as shown below:

```python
# Configure graphRAG search
# Configure LLM generation
rag_generation_config = {
    "model": "anthropic/claude-3-opus-20240229",
    "temperature": 0.7,
    "top_p": 0.95,
    "max_tokens_to_sample": 1500,
    "stream": True,
    "functions": None,  # For function calling, if supported
    "tools": None,  # For tool use, if supported
    "add_generation_kwargs": {},  # Additional provider-specific parameters
    "api_base": None  # Custom API endpoint, if needed
}
```

When performing a RAG query you can combine these vector search, knowledge graph search, and generation settings at runtime:

```python
response = client.retrieval.rag(
    "What are the latest advancements in quantum computing?",
    rag_generation_config=rag_generation_config,
    search_settings={
        "use_semantic_search": True,
        "limit": 20,
        "use_hybrid_search": True,
    }
)
```

R2R defaults to the specified server-side settings when no runtime overrides are specified.

### RAG Prompt Override

For specialized tasks, you can override the default RAG task prompt at runtime:

```python
task_prompt_override = """You are an AI assistant specializing in quantum computing.
Your task is to provide a concise summary of the latest advancements in the field,
focusing on practical applications and breakthroughs from the past year."""

response = client.retrieval.rag(
    "What are the latest advancements in quantum computing?",
    rag_generation_config=rag_generation_config,
    task_prompt_override=task_prompt_override
)
```

This prompt can also be set statically on as part of the server configuration process.

## Agent-based Interaction

R2R supports multi-turn conversations and complex query processing through its agent endpoint:

```python
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "What are the key differences between quantum and classical computing?"}
]

response = client.retrieval.agent(
    messages=messages,
    vector_search_settings=vector_search_settings,
    rag_generation_config=rag_generation_config,
)
```

The agent can break down complex queries into sub-tasks, leveraging both retrieval and generation capabilities to provide comprehensive responses. The settings specified in the example above will propagate to the agent and it's tools.

By leveraging these configuration options, you can fine-tune R2R's retrieval and generation process to best suit your specific use case and requirements.


# Graphs

&gt; Configure your R2R graph system.

R2R supports robust knowledge graph functionality to enhance document understanding and retrieval.

By default, R2R creates graphs by first extracting the entities and relationships associated with a given document. Next collections can be formed out of your ingested documents. For each collection, a corresponding graph can be built over the input documents.

You can find out more about this with the [knowledge graph cookbook](/documentation/graphs) and the [GraphRAG cookbook](/documentation/graphs). To configure the knowledge graph settings for your project, edit the `database` section in your `r2r.toml` file:

```toml r2r.toml
[database]
default_collection_name = "Default"
default_collection_description = "Your default collection."
collection_summary_prompt = "collection_summary"

  [database.graph_creation_settings]
    graph_entity_description_prompt = "graph_entity_description"
    graph_extraction_prompt = "graph_extraction"
    entity_types = [] # if empty, all entities are extracted
    relation_types = [] # if empty, all relations are extracted
    automatic_deduplication = true # enable automatic deduplication of entities

  [database.graph_enrichment_settings]
    graph_communities_prompt = "graph_communities"
```

<note>
  Setting configuration values in the `r2r.toml` will override environment variables by default.
</note>

### Knowledge Graph Operations

1. **Entity Management**: Add, update, and retrieve entities in the knowledge graph.
2. **Relationship Management**: Create and query relationships between entities.
3. **Batch Import**: Efficiently import large amounts of data using batched operations.
4. **Vector Search**: Perform similarity searches on entity embeddings.
5. **Community Detection**: Identify and manage communities within the graph.

### Customization

You can customize the knowledge graph extraction and search processes by modifying the `kg_triples_extraction_prompt` and adjusting the model configurations in `kg_extraction_settings` and `graph_settings`. Moreover, you can customize the LLM models used in various parts of the knowledge graph creation process. All of these options can be selected at runtime, with the only exception being the specified database provider. For more details, refer to the knowledge graph settings in the [search API](/api-and-sdks/retrieval/search-app).

By leveraging the knowledge graph capabilities, you can enhance R2R's understanding of document relationships and improve the quality of search and retrieval operations.

## Next Steps

For more detailed information on configuring specific components of the ingestion process, please refer to the following pages:

* [Ingestion Configuration](/self-hosting/configuration/ingestion)
* [Retrieval Configuration](/self-hosting/configuration/retrieval/overview)


# Prompts

&gt; Configure your prompts

## Prompt Management in R2R

R2R provides a flexible system for managing prompts, allowing you to create, update, retrieve, and delete prompts dynamically. This system is crucial for customizing the behavior of language models and ensuring consistent interactions across your application.

## Default Prompts

R2R comes with a set of default prompts that are loaded from YAML files located in the [`py/core/providers/database/prompts`](https://github.com/SciPhi-AI/R2R/tree/main/py/core/providers/database/prompts) directory. These default prompts provide a starting point for various tasks within the R2R system.

For example, the default RAG (Retrieval-Augmented Generation) prompt is defined as follows:

```yaml
default_rag:
  template: &gt;
    ## Task:

    Answer the query given immediately below given the context which follows later. Use line item references to like [1], [2], ... refer to specifically numbered items in the provided context. Pay close attention to the title of each given source to ensure it is consistent with the query.


    ### Query:

    {query}


    ### Context:

    {context}


    ### Query:

    {query}


    REMINDER - Use line item references to like [1], [2], ... refer to specifically numbered items in the provided context.

    ## Response:
  input_types:
    query: str
    context: str
```

### Default Prompt Usage

<warning>
   This table can fall out of date, refer to the 

  [prompts directory](https://github.com/SciPhi-AI/R2R/tree/main/py/core/providers/database/prompts)

   in the R2R repository as a source of truth. 
</warning>

| Prompt File                                                                                                                                                                  | Purpose                                                                                                                                                        |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`default_rag.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/default_rag.yaml)                                                         | Default prompt for Retrieval-Augmented Generation (RAG) tasks. It instructs the model to answer queries based on provided context, using line item references. |
| [`graphrag_community_reports.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/graphrag_communities.yaml)                                 | Used in GraphRAG to generate reports about communities or clusters in the knowledge graph.                                                                     |
| [`graphrag_entity_description.yaml.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/graphrag_entity_description.yaml)                    | System prompt for the "map" phase in GraphRAG, used to process individual nodes or edges.                                                                      |
| [`graphrag_map_system.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/graphrag_map_system.yaml)                                         | System prompt for the "map" phase in GraphRAG, used to process individual nodes or edges.                                                                      |
| [`graphrag_reduce_system.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/graphrag_reduce_system.yaml)                                   | System prompt for the "reduce" phase in GraphRAG, used to combine or summarize information from multiple sources.                                              |
| [`graphrag_triples_extraction_few_shot.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/graphrag_relationships_extraction_few_shot.yaml) | Few-shot prompt for extracting subject-predicate-object triplets in GraphRAG, with examples.                                                                   |
| [`hyde.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/hyde.yaml)                                                                       | Related to Hypothetical Document Embeddings (HyDE) for improving retrieval performance.                                                                        |
| [`rag_agent.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/rag_agent.yaml)                                                             | Defines the behavior and instructions for the RAG agent, which coordinates the retrieval and generation process.                                               |
| [`rag_fusion.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/rag_fusion.yaml)                                                           | Used in RAG fusion techniques, possibly for combining information from multiple retrieved passages.                                                            |
| [`system.yaml`](https://github.com/SciPhi-AI/R2R/blob/main/py/core/providers/database/prompts/default_system.yaml)                                                           | Contains general system-level prompts or instructions for the R2R system.                                                                                      |

You can find the full list of default prompts and their contents in the [prompts directory](https://github.com/SciPhi-AI/R2R/tree/main/py/core/providers/database/prompts).

## Prompt Provider

R2R uses a postgres class to manage prompts. This allows for storage, retrieval, and manipulation of prompts, leveraging both a Postgres database and YAML files for flexibility and persistence.

Key features of prompts inside R2R:

1. **Database Storage**: Prompts are stored in a Postgres table, allowing for efficient querying and updates.
2. **YAML File Support**: Prompts can be loaded from YAML files, providing an easy way to version control and distribute default prompts.
3. **In-Memory Cache**: Prompts are kept in memory for fast access during runtime.

## Prompt Structure

Each prompt in R2R consists of:

* **Name**: A unique identifier for the prompt.
* **Template**: The actual text of the prompt, which may include placeholders for dynamic content.
* **Input Types**: A dictionary specifying the expected types for any dynamic inputs to the prompt.

## Managing Prompts

R2R provides several endpoints and SDK methods for managing prompts:

### Adding a Prompt

To add a new prompt:

```python
response = client.prompts.add_prompt(
    name="my_new_prompt",
    template="Hello, {name}! Welcome to {service}.",
    input_types={"name": "str", "service": "str"}
)
```

### Updating a Prompt

To update an existing prompt:

```python
response = client.prompts.update_prompt(
    name="my_existing_prompt",
    template="Updated template: {variable}",
    input_types={"variable": "str"}
)
```

### Retrieving a Prompt

To get a specific prompt:

```python
response = client.prompts.get_prompt(
    prompt_name="my_prompt",
    inputs={"variable": "example"},
    prompt_override="Optional override text"
)
```

Refer directly to the [Prompt API Reference](/api-and-sdks/prompts) for more details.

## Security Considerations

Access to prompt management functions is restricted to superusers to prevent unauthorized modifications to system prompts. Ensure that only trusted administrators have superuser access to your R2R deployment.

## Conclusion

R2R's prompt management system provides a powerful and flexible way to control the behavior of language models in your application. By leveraging this system effectively, you can create more dynamic, context-aware, and maintainable AI-powered features.


# Local LLMs

&gt; Learn how to run a Retrieval-Augmented Generation system locally using R2R

## Introduction

R2R natively supports RAG with local LLMs through [LM Studio](https://github.com/lmstudio-ai) and [Ollama](https://github.com/ollama).

<info>
  [Follow along with our Local LLM cookbook for a full walkthrough on how to use R2R with local LLMs!](/cookbooks/local-llms)
</info>

<tabs>
  <tab title="Ollama">
    To get started with Ollama, you must follow the instructions on their [official website](https://ollama.com/).

    To run R2R with default Ollama settings, which utilize `llama3.1` and  `mxbai-embed-large`, run:

    ```Zsh
    export R2R_CONFIG_NAME=ollama
    python -m r2r.serve
    ```

    ## Preparing Local LLMs

    <error>
      Ollama has a default context window size of 2048 tokens. Many of the prompts and processes that R2R uses requires larger window sizes.

      It is recommended to set the context size to a minimum of 16k tokens. The following guideline is generally useful to determine what your system can handle:

      * 8GB RAM/VRAM: \~4K-8K context
      * 16GB RAM/VRAM: \~16K-32K context
      * 24GB+ RAM/VRAM: 32K+ context

      To change the default you must first create a modelfile for Ollama, where you can set `num_ctx`:

      ```Zsh
      echo 'FROM llama3.1
      PARAMETER num_ctx 16000' &gt; Modelfile
      ```

      Then you must create a manifest for that model:

      ```Zsh
      ollama create llama3.1 -f Modelfile
      ```
    </error>

    Next, make sure that you have all the necessary LLMs installed:

    ```zsh
    # in a separate terminal
    ollama pull llama3.1
    ollama pull mxbai-embed-large
    ollama serve
    ```

    These commands will need to be replaced with models specific to your configuration when deploying R2R with a customized configuration.

    ## Configuration

    R2R uses a TOML configuration file for managing settings, which you can [read about here](/self-hosting/configuration/overview). For local setup, we'll use the default `ollama` configuration. This can be customized to your needs by setting up a standalone project.

    <accordion icon="gear" title="Local Configuration Details">
      The `ollama` configuration file (`core/configs/ollama.toml`) includes:

      ```toml
      [completion]
      provider = "litellm"
      concurrent_request_limit = 1

        [completion.generation_config]
        model = "ollama/llama3.1"
        temperature = 0.1
        top_p = 1
        max_tokens_to_sample = 1_024
        stream = false
        add_generation_kwargs = { }

      [database]
      provider = "postgres"

      [embedding]
      provider = "ollama"
      base_model = "mxbai-embed-large"
      base_dimension = 1_024
      batch_size = 32
      add_title_as_prefix = true
      concurrent_request_limit = 32

      [ingestion]
      excluded_parsers = [ "mp4" ]
      ```
    </accordion>
  </tab>

  <tab title="LM Studio">
    To get started with LM Studio, you must follow the instructions on their [official website](https://lmstudio.ai/).

    LiteLLM, which is used to route our LLM requests, requires us to set an API base and API key for LM Studio. The API key can be any value. You must adjust your LM Studio API base to the appropriate location; the default is shown below.

    ```Zsh
    export LM_STUDIO_API_BASE=http://127.0.0.1:1234
    export LM_STUDIO_API_KEY=1234
    ```

    To run R2R with default LM Studio local LLM settings, which utilize `llama-3.2-3b-instruct` and  `text-embedding-nomic-embed-text-v1.5`, run:

    ```Zsh
    export R2R_CONFIG_NAME=lm_studio
    python -m r2r.serve
    ```

    ## Preparing Local LLMs

    Next, make sure that you have all the necessary LLMs installed. Follow the [official documentation](https://lmstudio.ai/docs/basics) from LM Studio to download your LLM and embedding model and load it into memory.

    ## Configuration

    R2R uses a TOML configuration file for managing settings, which you can [read about here](/self-hosting/configuration/overview). For local setup, we'll use the default `ollama` configuration. This can be customized to your needs by setting up a standalone project.

    <accordion icon="gear" title="Local Configuration Details">
      The `ollama` configuration file (`core/configs/ollama.toml`) includes:

      ```toml
      [agent]
      system_instruction_name = "rag_agent"
      tool_names = ["local_search"]

        [agent.generation_config]
        model = "lm_studio/llama-3.2-3b-instruct"

      [completion]
      provider = "litellm"
      concurrent_request_limit = 1

        [completion.generation_config]
        model = "lm_studio/llama-3.2-3b-instruct"
        temperature = 0.1
        top_p = 1
        max_tokens_to_sample = 1_024
        stream = false
        add_generation_kwargs = { }

      [embedding]
      provider = "litellm"
      base_model = "lm_studio/text-embedding-nomic-embed-text-v1.5"
      base_dimension = nan
      batch_size = 128
      add_title_as_prefix = true
      concurrent_request_limit = 2

      [database]
      provider = "postgres"

        [database.graph_creation_settings]
          graph_entity_description_prompt = "graphrag_entity_description"
          entity_types = [] # if empty, all entities are extracted
          relation_types = [] # if empty, all relations are extracted
          fragment_merge_count = 4 # number of fragments to merge into a single extraction
          max_knowledge_relationships = 100
          max_description_input_length = 65536
          generation_config = { model = "lm_studio/llama-3.2-3b-instruct" } # and other params, model used for relationshipt extraction

        [database.graph_enrichment_settings]
          community_reports_prompt = "graphrag_community_reports"
          max_summary_input_length = 65536
          generation_config = { model = "lm_studio/llama-3.2-3b-instruct" } # and other params, model used for node description and graph clustering
          leiden_params = {}

        [database.graph_search_settings]
          generation_config = { model = "lm_studio/llama-3.2-3b-instruct" }


      [orchestration]
      provider = "simple"


      [ingestion]
      vision_img_model = "lm_studio/llama3.2-vision"
      vision_pdf_model = "lm_studio/llama3.2-vision"
      chunks_for_document_summary = 16
      document_summary_model = "lm_studio/llama-3.2-3b-instruct"

        [ingestion.extra_parsers]
          pdf = "zerox"
      ```
    </accordion>
  </tab>
</tabs>

For more information on how to configure R2R, [visit here](/self-hosting/configuration/overview).

<note>
  We are still working on adding local multimodal RAG features. Your feedback would be appreciated.
</note>

The ingestion and graph creation process has been tested across different language models. When selecting a model, consider the tradeoff between performance and model size‚Äîlarger models often generate more detailed graphs with more elements, while smaller models may be more efficient but produce simpler graphs.

| Model       | Entities | Relationships |
| ----------- | -------- | ------------- |
| llama3.1:8B | 76       | 60            |
| llama3.2:3B | 29       | 29            |

## Summary

The above steps are all you need to get RAG up and running with local LLMs in R2R. For detailed setup and basic functionality, refer back to the [R2R Quickstart](/documentation/quickstart). For more advanced usage and customization options, refer to the [basic configuration](/self-hosting/configuration/overview) or join the [R2R Discord community](https://discord.gg/p6KqD2kjtB).


# Introduction

&gt; Learn how to deploy R2R

# Deploying R2R

R2R is designed to be flexible and scalable, allowing deployment in various environments. This guide provides an overview of deployment options and resource recommendations to help you get started with R2R in a production setting.

## Deployment Options

1. **Local Docker or Local Build**: Ideal for development and testing. [Start here](/self-hosting/installation/overview).
2. **Single Cloud Instance**: Recommended for most small to medium-sized organizations.
3. **Container Orchestration** (Docker Swarm): Suitable for larger organizations or those requiring more granular resource control

## Resource Recommendations

When running R2R, we recommend:

* At least 4 vCPU cores
* 8+GB of RAM (16GB preferred)
* 50gb + 4x raw data size (*size of data to be ingested after converting to TXT*) of disk space

## Deployment Guides

For detailed, step-by-step instructions on deploying R2R in various environments, please refer to our specific deployment guides:

* [Local Deployment](/self-hosting/installation/overview)
* [Azure Deployment](/self-hosting/deployment/azure)
* [SciPhi Cloud](/self-hosting/deployment/sciphi/)

Choose the guide that best fits your infrastructure and scaling needs. Each guide provides specific instructions for setting up R2R in that environment, including necessary configurations and best practices.

By following these deployment recommendations and configuration guides, you'll be well on your way to leveraging R2R's powerful RAG capabilities in your production environment.


# Azure

&gt; Learn how to deploy R2R into Azure

# Deploying R2R on Azure

Azure provides a robust and scalable platform for deploying R2R. This guide will walk you through the process of setting up R2R on an Azure Virtual Machine, making it accessible both locally and publicly.

## Overview

Deploying R2R on Azure involves the following main steps:

1. Creating an Azure Virtual Machine
2. Installing necessary dependencies
3. Setting up R2R
4. Configuring port forwarding for local access
5. Exposing ports for public access (optional)

This guide assumes you have an Azure account and the necessary permissions to create and manage Virtual Machines.

## Creating an Azure Virtual Machine

1. Log in to the [Azure Portal](https://portal.azure.com/).
2. Click on "Create a resource" and search for "Virtual Machine".
3. Choose `Ubuntu Server 22.04 LTS - x64 Gen2` as the operating system.
4. Select a VM size with at least 16GB of RAM, 4-8 vCPU cores, and 500GB of disk for a small-mid sized organization (\&lt; 5000 users). The `D4s_v3` series is a good starting point.
5. Configure networking settings to allow inbound traffic on ports `22` (SSH), and optionally `7272` (R2R API) and `7273` (R2R Dashboard).
6. Review and create the VM.

## Exposing Ports for Public Access (Optional)

To make R2R publicly accessible:

1. Log in to the Azure Portal.

2. Navigate to your VM &gt; Networking &gt; Network Security Group.

3. Add new inbound security rules:
   * Destination port ranges: 7272
     * Protocol: TCP
     * Action: Allow
     * Priority: 1000 (or lower than conflicting rules)
     * Name: Allow\_7272
   * Destination port ranges: 7273
     * Protocol: TCP
     * Action: Allow
     * Priority: 1001
     * Name: Allow\_7273

4. Ensure R2R is configured to listen on all interfaces (0.0.0.0).

After starting your R2R application, users can access:

* The API at http\://YOUR\_VM\_ADDRESS:7272
* The dashboard at http\://YOUR\_VM\_ADDRESS:7273


  <img src="file://81575abd-05c8-4724-b95e-402672a3fc38/">


## Installing Dependencies

SSH into your newly created VM with a command like `ssh -i my_pem.pem azureuser@YOUR_VM_ADDRESS`:


  <img src="file://86b4ae36-90f9-48a9-b9b6-677b0d402a28/">


Now, run the following commands to install the necessary R2R dependencies:

```zsh
# Update package list
sudo apt update

# Install Git
sudo apt install git -y

# Install Docker
sudo apt-get update
sudo apt-get install ca-certificates curl -y
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release &amp;&amp; echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
sudo apt-get update

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y

# Add your user to the Docker group
sudo usermod -aG docker $USER
newgrp docker

# Verify Docker installation
docker run hello-world
```


  <img src="file://bc955d7a-484b-40b3-8247-ce43241d7afc/">


## Setting up R2R

1. Clone the R2R repository:

```zsh
git clone https://github.com/SciPhi-AI/R2R.git
cd R2R/docker
```

2. Set up environment variables:

```zsh
cd env
# Edit r2r-full.env with your preferred text editor
nano r2r-full.env
```

Add the necessary environment variables:

```
# Choose configuration
R2R_CONFIG_NAME=full

# Add your API key(s)
OPENAI_API_KEY=sk-...

# Optional - Add agent tool API keys if needed
# SERPER_API_KEY=your_serper_api_key_here
# FIRECRAWL_API_KEY=your_firecrawl_api_key_here
```

3. Start the R2R services:

```zsh
cd ..
docker compose -f compose.full.yaml --profile postgres up -d
```


  <img src="file://4512f2d5-105a-4733-9631-7dd66fff3bfc/">


4. Verify the health of the system:

```zsh
# Wait for services to start
sleep 30

# Check health
curl http://localhost:7272/v3/health
```

Should return something like:

```json
{"results":{"response":"ok"}}
```

5. Test ingesting and searching a sample document from a remote environment:

```zsh
# From your local machine
curl -X POST "http://YOUR_VM_ADDRESS:7272/v3/documents/create-sample"
sleep 10
curl -X POST "http://YOUR_VM_ADDRESS:7272/v3/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "Who was aristotle?"}'
```

Replace `YOUR_VM_ADDRESS` with your Azure VM's public IP address.

## Configuring Port Forwarding for Local Access

To access R2R from your local machine, use SSH port forwarding:

```zsh
ssh -i my_pem.pem -L 7272:localhost:7272 -L 7273:localhost:7273 azureuser@YOUR_VM_ADDRESS
```

This will allow you to access:

* The API at [http://localhost:7272](http://localhost:7272)
* The dashboard at [http://localhost:7273](http://localhost:7273)

Note that when using the R2R dashboard, you may still need to use the remote VM address as requests are made from the client-side.

## Troubleshooting

If you encounter issues:

1. Check Docker container status:

```
docker ps
docker logs <container_id>
```

2. Verify environment variables are correctly set in r2r-full.env.

3. Ensure ports are correctly exposed in your network security group.

4. Check disk space and system resources:

```
df -h
free -m
```

5. If services fail to start, try restarting Docker:

```
sudo systemctl restart docker
```

## Security Considerations

* Use HTTPS (port 443) with a valid SSL certificate for production.
* Restrict source IP addresses in the security rule if possible.
* Regularly update and patch your system and applications.
* Use Azure Security Center for monitoring security posture.
* Consider using Azure Private Link for secure private connections.
* Enable Just-in-Time VM access to restrict inbound traffic.
* Deploy Azure Firewall for enhanced network security.
* Configure Azure Sentinel for security information and event management.
* Remove or disable the security rules when not needed for testing.

## Conclusion

You have now successfully deployed R2R on Azure. The application should be accessible locally through SSH tunneling and optionally publicly through direct access to the Azure VM.

For more information on configuring and using R2R, refer to the [configuration documentation](/self-hosting/configuration/overview) or join our [Discord community](https://discord.gg/p6KqD2kjtB) for assistance.


# AWS

&gt; Learn how to deploy R2R into AWS

# Deploying R2R on Amazon Web Services (AWS)

Amazon Web Services (AWS) provides a robust and scalable platform for deploying R2R. This guide will walk you through the process of setting up R2R on an Amazon EC2 instance, making it accessible both locally and publicly.

## Overview

Deploying R2R on AWS involves the following main steps:

1. Creating an Amazon EC2 instance
2. Installing necessary dependencies
3. Setting up R2R
4. Configuring port forwarding for local access
5. Exposing ports for public access (optional)

This guide assumes you have an AWS account and the necessary permissions to create and manage EC2 instances.

## Creating an Amazon EC2 Instance

1. Log in to the [AWS Management Console](https://aws.amazon.com/console/).
2. Navigate to EC2 under "Compute" services.
3. Click "Launch Instance".
4. Choose an Amazon Machine Image (AMI):
   * Select "Ubuntu Server 22.04 LTS (HVM), SSD Volume Type"
5. Choose an Instance Type:
   * For a small-mid sized organization (\&lt; 5000 users), select t3.xlarge (4 vCPU, 16 GiB Memory) or higher
6. Configure Instance Details:
   * Leave default settings or adjust as needed
7. Add Storage:
   * Set the root volume to at least 500 GiB
8. Add Tags (optional):
   * Add any tags for easier resource management
9. Configure Security Group:
   * Create a new security group
   * Add rules to allow inbound traffic on ports 22 (SSH) and 7272 (R2R API)
10. Review and Launch:
    * Review your settings and click "Launch"
    * Choose or create a key pair for SSH access

## Installing Dependencies

SSH into your newly created EC2 instance:

```zsh
ssh -i /path/to/your-key.pem ubuntu@your-instance-public-dns
```

Now, run the following commands to install the necessary R2R dependencies:

```zsh
# Update package list
sudo apt update

# Install Git
sudo apt install git -y

# Install Docker
sudo apt-get update
sudo apt-get install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release &amp;&amp; echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null

sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y

# Add your user to the Docker group
sudo usermod -aG docker $USER
newgrp docker

# Verify Docker installation
docker run hello-world
```

## Setting up R2R

1. Clone the R2R repository:

```zsh
git clone https://github.com/SciPhi-AI/R2R.git
cd R2R/docker
```

2. Set up environment variables:

```zsh
cd env
# Edit r2r-full.env with your preferred text editor
nano r2r-full.env
```

Add the necessary environment variables:

```
# Choose configuration
R2R_CONFIG_NAME=full

# Add your API key(s)
OPENAI_API_KEY=sk-...

# Optional - Add agent tool API keys if needed
# SERPER_API_KEY=your_serper_api_key_here
# FIRECRAWL_API_KEY=your_firecrawl_api_key_here
```

3. Start the R2R services:

```zsh
cd ..
docker compose -f compose.full.yaml --profile postgres up -d
```

4. Verify the health of the system:

```zsh
# Wait for services to start
sleep 30

# Check health
curl http://localhost:7272/v3/health
```

5. Test ingesting and searching a sample document from a remote environment:

```zsh
# From your local machine
curl -X POST "http://YOUR_INSTANCE_IP:7272/v3/documents/create-sample"
sleep 10
curl -X POST "http://YOUR_INSTANCE_IP:7272/v3/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "Who was aristotle?"}'
```

Replace `YOUR_INSTANCE_IP` with your EC2 instance's public IP address.

## Configuring Port Forwarding for Local Access

To access R2R from your local machine, use SSH port forwarding:

```zsh
ssh -i /path/to/your-key.pem -L 7272:localhost:7272 -L 7273:localhost:7273 ubuntu@your-instance-public-dns
```

This will allow you to access:

* The API at [http://localhost:7272](http://localhost:7272)
* The dashboard at [http://localhost:7273](http://localhost:7273)

## Exposing Ports for Public Access (Optional)

To make R2R publicly accessible:

1. In the AWS Management Console, go to EC2 &gt; Security Groups.
2. Select the security group associated with your EC2 instance.
3. Click "Edit inbound rules".
4. Add new rules:
   * Type: Custom TCP, Port range: 7272, Source: Anywhere (0.0.0.0/0), Description: R2R API
   * Type: Custom TCP, Port range: 7273, Source: Anywhere (0.0.0.0/0), Description: R2R Dashboard
5. Click "Save rules".

After starting your R2R application, users can access:

* The API at http\://YOUR\_INSTANCE\_IP:7272
* The dashboard at http\://YOUR\_INSTANCE\_IP:7273

## Security Considerations

* Use HTTPS (port 443) with a valid SSL certificate for production.
* Restrict source IP addresses in the security group rule if possible.
* Regularly update and patch your system and applications.
* Use AWS VPC for network isolation.
* Enable and configure AWS CloudTrail for auditing.
* Use AWS IAM roles for secure access management.
* Consider using AWS Certificate Manager for SSL/TLS certificates.
* Monitor incoming traffic using AWS CloudWatch.
* Remove or disable the security group rule when not needed for testing.

## Troubleshooting

If you encounter issues:

1. Check Docker container status:

```
docker ps
docker logs <container_id>
```

2. Verify environment variables are correctly set in r2r-full.env.

3. Ensure ports are correctly exposed in your security group.

4. Check disk space and system resources:

```
df -h
free -m
```

## Conclusion

You have now successfully deployed R2R on Amazon Web Services. The application should be accessible locally through SSH tunneling and optionally publicly through direct access to the EC2 instance.

For more information on configuring and using R2R, refer to the [configuration documentation](/self-hosting/configuration/overview) or join our [Discord community](https://discord.gg/p6KqD2kjtB) for assistance.


# GCP

&gt; Learn how to deploy R2R into Google Cloud

# Deploying R2R on Google Cloud Platform

Google Cloud Platform (GCP) offers a robust and scalable environment for deploying R2R. This guide will walk you through the process of setting up R2R on a Google Compute Engine instance, making it accessible both locally and publicly.

## Overview

Deploying R2R on GCP involves the following main steps:

1. Creating a Google Compute Engine instance
2. Installing necessary dependencies
3. Setting up R2R
4. Configuring port forwarding for local access
5. Exposing ports for public access (optional)

This guide assumes you have a Google Cloud account and the necessary permissions to create and manage Compute Engine instances.

## Creating a Google Compute Engine Instance

1. Log in to the [Google Cloud Console](https://console.cloud.google.com/).
2. Navigate to "Compute Engine" &gt; "VM instances".
3. Click "Create Instance".
4. Choose the following settings:
   * Name: Choose a name for your instance
   * Region and Zone: Select based on your location/preferences
   * Machine Configuration:
     * Series: N1
     * Machine type: n1-standard-4 (4 vCPU, 15 GB memory) or higher
   * Boot disk:
     * Operating System: Ubuntu
     * Version: Ubuntu 22.04 LTS
     * Size: 500 GB
   * Firewall: Allow HTTP and HTTPS traffic
5. Click "Create" to launch the instance.

## Installing Dependencies

SSH into your newly created instance using the Google Cloud Console or gcloud command:

```zsh
gcloud compute ssh --zone "your-zone" "your-instance-name"
```

Now, run the following commands to install the necessary R2R dependencies:

```zsh
# Update package list
sudo apt update

# Install Git
sudo apt install git -y

# Install Docker
sudo apt-get update
sudo apt-get install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release &amp;&amp; echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null

sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y

# Add your user to the Docker group
sudo usermod -aG docker $USER
newgrp docker

# Verify Docker installation
docker run hello-world
```

## Setting up R2R

1. Clone the R2R repository:

```zsh
git clone https://github.com/SciPhi-AI/R2R.git
cd R2R/docker
```

2. Set up environment variables:

```zsh
cd env
# Edit r2r-full.env with your preferred text editor
nano r2r-full.env
```

Add the necessary environment variables:

```
# Choose configuration
R2R_CONFIG_NAME=full

# Add your API key(s)
OPENAI_API_KEY=sk-...

# Optional - Add agent tool API keys if needed
# SERPER_API_KEY=your_serper_api_key_here
# FIRECRAWL_API_KEY=your_firecrawl_api_key_here
```

3. Start the R2R services:

```zsh
cd ..
docker compose -f compose.full.yaml --profile postgres up -d
```

4. Verify the health of the system:

```zsh
# Wait for services to start
sleep 30

# Check health
curl http://localhost:7272/v3/health
```

5. Test ingesting and searching a sample document from a remote environment:

```zsh
# From your local machine
curl -X POST "http://YOUR_INSTANCE_IP:7272/v3/documents/create-sample"
sleep 10
curl -X POST "http://YOUR_INSTANCE_IP:7272/v3/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "Who was aristotle?"}'
```

Replace `YOUR_INSTANCE_IP` with your Google Compute Engine instance's external IP address.

## Configuring Port Forwarding for Local Access

To access R2R from your local machine, use SSH port forwarding:

```zsh
gcloud compute ssh --zone "your-zone" "your-instance-name" -- -L 7272:localhost:7272 -L 7273:localhost:7273
```

This will allow you to access:

* The API at [http://localhost:7272](http://localhost:7272)
* The dashboard at [http://localhost:7273](http://localhost:7273)

## Exposing Ports for Public Access (Optional)

To make R2R publicly accessible:

1. In the Google Cloud Console, go to "VPC network" &gt; "Firewall".

2. Click "Create Firewall Rule".

3. Configure the rule:
   * Name: Allow-R2R-Ports
   * Target tags: r2r-server
   * Source IP ranges: 0.0.0.0/0 (or restrict to specific IP ranges for better security)
   * Specified protocols and ports:
     * tcp:7272 (API)
     * tcp:7273 (Dashboard)

4. Click "Create".

5. Add the network tag to your instance:
   * Go to Compute Engine &gt; VM instances.
   * Click on your instance name.
   * Click "Edit".
   * Under "Network tags", add "r2r-server".
   * Click "Save".

After starting your R2R application, users can access:

* The API at http\://YOUR\_INSTANCE\_IP:7272
* The dashboard at http\://YOUR\_INSTANCE\_IP:7273

## Security Considerations

* Use HTTPS (port 443) with a valid SSL certificate for production.
* Restrict source IP addresses in the firewall rule if possible.
* Regularly update and patch your system and applications.
* Use GCP Identity and Access Management (IAM) for secure access control.
* Consider using Google Cloud Armor for additional security.
* Set up Cloud Monitoring for tracking system performance.
* Enable audit logging to track who is accessing your instance.
* Remove or disable the firewall rule when not needed for testing.

## Troubleshooting

If you encounter issues:

1. Check Docker container status:

```
docker ps
docker logs <container_id>
```

2. Verify environment variables are correctly set in r2r-full.env.

3. Ensure ports are correctly exposed in your firewall rules.

4. Check disk space and system resources:

```
df -h
free -m
```

5. Review GCP Stackdriver logs for any system-level issues.

## Conclusion

You have now successfully deployed R2R on Google Cloud Platform. The application should be accessible locally through SSH tunneling and optionally publicly through direct access to the Compute Engine instance.

For more information on configuring and using R2R, refer to the [configuration documentation](/self-hosting/configuration/overview) or join our [Discord community](https://discord.gg/p6KqD2kjtB) for assistance.


# SciPhi Cloud

&gt; Fully managed R2R for startups and enterprises

# SciPhi Cloud: Fully Managed R2R for Your Organization

SciPhi offers a fully managed, enterprise-grade solution for deploying and scaling R2R within your organization. The SciPhi Cloud offering provides all the benefits of R2R, including multimodal ingestion, hybrid search, GraphRAG, user management, and observability, in a hassle-free, scalable environment.

## Why Use SciPhi Cloud?

* **Fully Managed**: SciPhi handles the infrastructure, deployment, scaling, updates, and maintenance of R2R, so your team can focus on building RAG applications.
* **Scalable**: Seamlessly scale your R2R deployment to handle growing user bases, document collections, and query volumes.
* **Secure**: Benefit from enterprise-level security, compliance, and data privacy measures.
* **Customizable**: Tailor your R2R deployment to your organization's specific requirements and integrate with your existing systems and workflows.
* **Expert Support**: Get direct access to the SciPhi team for guidance, troubleshooting, and best practices.

## Getting Started

Getting started with SciPhi Cloud is easy:

1. **Contact SciPhi**: Reach out to their team at [founders@sciphi.ai](mailto:founders@sciphi.ai) to discuss your organization's RAG application needs and learn more about SciPhi Enterprise.
2. **Discovery**: SciPhi's experts will work with you to understand your requirements, existing systems, and goals for R2R within your organization.
3. **Deployment**: SciPhi will handle the deployment and configuration of R2R in your environment, whether cloud-based or on-premises, and integrate with your existing systems and workflows.
4. **Onboarding**: SciPhi will provide training and support to help your developers and users get up and running with R2R quickly and effectively.
5. **Ongoing Support**: SciPhi Cloud provides ongoing support, updates, and guidance as you scale and evolve your RAG applications.


# User Auth

&gt; A comprehensive guide to user authentication and management features in R2R

## Introduction

R2R provides a complete set of user authentication and management features, allowing developers to implement secure and feature-rich authentication systems, or to integrate directly with their authentication provider of choice.

In addition to this page, you refer to [documentation on configuring authentication](/self-hosting/configuration/auth) inside of R2R, or refer to the [user API reference](/api-and-sdks/users/users).

<note>
  When authentication is not required (require\_authentication is set to false, which is the default in `r2r.toml`), unauthenticated requests will default to using the credentials of the default admin user.

  This behavior ensures that operations can proceed smoothly in development or testing environments where authentication may not be enforced, but it should be used with caution in production settings.
</note>

## Basic Usage

### User Registration and Login

Let's start by registering a new user and logging in:

```python core/examples/scripts/run_auth_workflow.py
from r2r import R2RClient

client = R2RClient("http://localhost:7272") # Replace with your R2R deployment URL

# Register a new user
user_result = client.users.register("user1@test.com", "password123")
# {'results': {'email': 'user1@test.com', 'id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'hashed_password': '$2b$12$p6a9glpAQaq.4uzi4gXQru6PN7WBpky/xMeYK9LShEe4ygBf1L.pK', 'is_superuser': False, 'is_active': True, 'is_verified': False, 'verification_code_expiry': None, 'name': None, 'bio': None, 'profile_picture': None, 'created_at': '2024-07-16T22:53:47.524794Z', 'updated_at': '2024-07-16T22:53:47.524794Z'}}

# Login immediately <assuming email="" verification="" is="" disabled,="" as="" it="" by="" default="">
login_result = client.users.login("user1@test.com", "password123")
# {'results': {'access_token': {'token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMUB0ZXN0LmNvbSIsImV4cCI6MTcyMTE5OTI0Ni44Nzc2NTksInRva2VuX3R5cGUiOiJhY2Nlc3MifQ.P4RcCkCe0H5UHPHak7tRovIgyQcql4gB8NlqdDDk50Y', 'token_type': 'access'}, 'refresh_token': {'token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMUB0ZXN0LmNvbSIsImV4cCI6MTcyMTc3NTI0NiwidG9rZW5fdHlwZSI6InJlZnJlc2gifQ.VgfZ4Lhz0f2GW41NYv6KLrMCK3CdGmGVug7eTQp0xPU', 'token_type': 'refresh'}}}
```

This code snippet demonstrates the basic user registration and login process. The `register` method creates a new user account, while the `login` method authenticates the user and returns access and refresh tokens. In the example above, it was assumed that email verification was disabled.

### Email Verification (Optional)

If email verification is enabled in your R2R configuration, you'll need to verify the user's email before they can log in:

```python
verify_result = client.users.verify_email("verification_code_here")
# {"results": {"message": "Email verified successfully"}}
```

### Token Refresh

After logging in, you gain immediate access to user information such as general account details, documents overview, and utility functions like token refresh:

```python
refresh_result = client.users.refresh_access_token()
# {'results': {'access_token': {'token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMUB0ZXN0LmNvbSIsImV4cCI6MTcyMTIwMTk3Mi4zMzI4NTIsInRva2VuX3R5cGUiOiJhY2Nlc3MifQ.Ze9A50kefndAtu2tvcvMCiilFfAhrOV0l5A7RZgPvBY', 'token_type': 'access'}, 'refresh_token': {'token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMUB0ZXN0LmNvbSIsImV4cCI6MTcyMTc3Nzk3MiwidG9rZW5fdHlwZSI6InJlZnJlc2gifQ.NwzFH8e2tKO0bH1Hdm_eq39VqmGPf7xSNOOhDlKFQFQ', 'token_type': 'refresh'}}}
```

## Document Management

R2R allows users to manage their documents securely. Here's how to ingest and search a given users documents:

### Ingesting Documents

```python
import os

# client.users.login ... 

# Ingest a sample file for the logged-in user
script_path = os.path.dirname(__file__)
sample_file = os.path.join(script_path, "..", "data", "aristotle.txt")
ingestion_result = client.documents.create([sample_file])
# {'results': {'processed_documents': ["Document 'aristotle.txt' processed successfully."], 'failed_documents': [], 'skipped_documents': []}}
```

### Listing User Documents

```python
documents = client.documents.list()
# {'results': [{'document_id': '6ab698c6-e494-5441-a740-49395f2b1881', 'version': 'v0', 'size_in_bytes': 73353, 'metadata': {}, 'status': 'success', 'user_id': 'ba0c75eb-0b21-4eb1-a902-082476e5e972', 'title': 'aristotle.txt', 'created_at': '2024-07-16T16:25:27.634411Z', 'updated_at': '2024-07-16T16:25:27.634411Z'}]}
```

### Search &amp; RAG

```python
search_results = client.retrieval.search(query="Sample search query")
# {'results': {'chunk_search_results': [{ ... 'metadata': {'text': 'Aristotle[A] (Greek: ·ºàœÅŒπœÉœÑŒøœÑŒ≠ŒªŒ∑œÇ Aristot√©lƒìs, pronounced [aristot√©l…õÀês]; 384‚Äì322 BC) was an Ancient Greek philosopher and polymath. His writings cover a broad range of subjects spanning the natural sciences, philosophy, linguistics, economics, politics, psychology, and the arts. As the founder of the Peripatetic school of philosophy in the Lyceum in Athens, he began the wider Aristotelian tradition that followed, which set the groundwork for the development of modern science.', 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 0, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}, {'id': '781ce9e6-9e73-5012-8445-35b7d84f161c', 'score': 0.670799394202279, 'metadata': {'text': "Aristotle was born in 384 BC[C] in Stagira, Chalcidice,[2] about 55 km (34 miles) east of modern-day Thessaloniki.[3][4] His father, Nicomachus, was the personal physician to King Amyntas of Macedon. While he was young, Aristotle learned about biology and medical information, which was taught by his father.[5] Both of Aristotle's parents died when he was about thirteen, and Proxenus of Atarneus became his guardian.[6] Although little information about Aristotle's childhood has survived, he probably spent", 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 8, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}, {'id': 'f32cda7c-2538-5248-b0b6-4d0d45cc4d60', 'score': 0.667974928858889, 'metadata': {'text': 'Aristotle was revered among medieval Muslim scholars as "The First Teacher", and among medieval Christians like Thomas Aquinas as simply "The Philosopher", while the poet Dante called him "the master of those who know". His works contain the earliest known formal study of logic, and were studied by medieval scholars such as Peter Abelard and Jean Buridan. Aristotle\'s influence on logic continued well into the 19th century. In addition, his ethics, although always influential, gained renewed interest with', 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 5, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}, {'id': 'e6592fd5-e02e-5847-b158-79bbdd8710a2', 'score': 0.6647597950983339, 'metadata': {'text': "Little is known about Aristotle's life. He was born in the city of Stagira in northern Greece during the Classical period. His father, Nicomachus, died when Aristotle was a child, and he was brought up by a guardian. At 17 or 18, he joined Plato's Academy in Athens and remained there until the age of 37 (c.\u2009347 BC). Shortly after Plato died, Aristotle left Athens and, at the request of Philip II of Macedon, tutored his son Alexander the Great beginning in 343 BC. He established a library in the Lyceum,", 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 1, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}, {'id': '8c72faca-6d98-5129-b9ee-70769272e361', 'score': 0.6476034942146001, 'metadata': {'text': 'Among countless other achievements, Aristotle was the founder of formal logic,[146] pioneered the study of zoology, and left every future scientist and philosopher in his debt through his contributions to the scientific method.[2][147][148] Taneli Kukkonen, observes that his achievement in founding two sciences is unmatched, and his reach in influencing "every branch of intellectual enterprise" including Western ethical and political theory, theology, rhetoric, and literary analysis is equally long. As a', 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 175, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}, {'id': '3ce904cc-5835-551a-a85c-f00be1a5e8dc', 'score': 0.626156434278918, 'metadata': {'text': 'Aristotle has been called the father of logic, biology, political science, zoology, embryology, natural law, scientific method, rhetoric, psychology, realism, criticism, individualism, teleology, and meteorology.[151]', 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 177, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}, {'id': '6a15b09b-4bf1-5c1f-af24-fe659c8a011d', 'score': 0.624521989361129, 'metadata': {'text': 'after friends and relatives, and to deal with the latter as with beasts or plants".[13] By 335 BC, Aristotle had returned to Athens, establishing his own school there known as the Lyceum. Aristotle conducted courses at the school for the next twelve years. While in Athens, his wife Pythias died and Aristotle became involved with Herpyllis of Stagira. They had a son whom Aristotle named after his father, Nicomachus. If the Suda ‚Äì an uncritical compilation from the Middle Ages ‚Äì is accurate, he may also have', 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 16, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}, {'id': '19a755d0-770f-5c6f-991e-ca191a40c8d6', 'score': 0.614493374720815, 'metadata': {'text': "passed to Plato's nephew Speusippus, although it is possible that he feared the anti-Macedonian sentiments in Athens at that time and left before Plato died.[10] Aristotle then accompanied Xenocrates to the court of his friend Hermias of Atarneus in Asia Minor. After the death of Hermias, Aristotle travelled with his pupil Theophrastus to the island of Lesbos, where together they researched the botany and zoology of the island and its sheltered lagoon. While in Lesbos, Aristotle married Pythias, either", 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 12, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}, {'id': '33b2dbd7-2f3a-5450-9618-976a996bde2a', 'score': 0.6117302824500019, 'metadata': {'text': 'Transmission\nFurther information: List of writers influenced by Aristotle\nMore than 2300 years after his death, Aristotle remains one of the most influential people who ever lived.[142][143][144] He contributed to almost every field of human knowledge then in existence, and he was the founder of many new fields. According to the philosopher Bryan Magee, "it is doubtful whether any human being has ever known as much as he did".[145]', 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 174, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}, {'id': '2d101d42-6317-5d8c-85c3-fb9b6d947c68', 'score': 0.610827455968717, 'metadata': {'text': "The immediate influence of Aristotle's work was felt as the Lyceum grew into the Peripatetic school. Aristotle's students included Aristoxenus, Dicaearchus, Demetrius of Phalerum, Eudemos of Rhodes, Harpalus, Hephaestion, Mnason of Phocis, Nicomachus, and Theophrastus. Aristotle's influence over Alexander the Great is seen in the latter's bringing with him on his expedition a host of zoologists, botanists, and researchers. He had also learned a great deal about Persian customs and traditions from his", 'title': 'aristotle.txt', 'user_id': 'bf417057-f104-4e75-8579-c74d26fcbed3', 'version': 'v0', 'chunk_order': 181, 'document_id': 'a2645197-d07f-558d-ba55-f7a60eb29621', 'extraction_id': 'b7bbd497-311a-5dc8-8a51-79e2208739e0', 'associatedQuery': 'Who was Aristotle'}}], 'kg_search_results': []}}

rag_results = client.retrieval.rag(query="Sample search query")
# {'results': {'completion': {'id': 'chatcmpl-9llkGYsrG1YZaWkqYvzXr1eQNl0gA', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'The search results for the query "Sample search query" include various topics and excerpts related to Aristotle\'s works and other subjects. Here are the relevant references:\n\n1. **Categories of Aristotle\'s Works**:\n   - On Interpretation [1], [2]\n   - Prior Analytics [1], [2]\n   - Posterior Analytics [1], [2]\n   - Topics [1], [2]\n   - On Sophistical Refutations [1], [2]\n\n2. **Aristotle\'s Theory on Sense Perceptions and Memory**:\n   - Aristotle\'s belief that people receive sense perceptions and perceive them as impressions, leading to the weaving together of new experiences. The search for these impressions involves searching the memory itself, where recollection occurs when one retrieved experience naturally follows another [3], [4].\n\n3. **Medieval Judaism**:\n   - References to Medieval Judaism [5], [6].\n\n4. **Scientific Style**:\n   - References to Scientific Style [7], [8].\n\n5. **Recovery of Texts by Apellicon**:\n   - Apellicon\'s efforts to recover degraded texts by copying them into new manuscripts and using guesswork to fill in unreadable gaps [9], [10].\n\nThese references provide a broad overview of the topics related to the query, including Aristotle\'s works, his theories on memory, Medieval Judaism, scientific style, and the recovery of ancient texts.', 'role': 'assistant'}}], 'created': 1721171976, 'model': 'gpt-4o-2024-05-13', 'object': 'chat.completion', 'system_fingerprint': 'fp_5e997b69d8', 'usage': {'completion_tokens': 286, 'prompt_tokens': 513, 'total_tokens': 799}}, 'search_results': {'chunk_search_results': [{'id': 'd70e2776-befa-5b67-9da7-b76aedb7c101', 'score': 0.270276627830369, 'metadata': {'text': 'Categories\nOn Interpretation\nPrior Analytics\nPosterior Analytics\nTopics\nOn Sophistical Refutations', 'title': 'aristotle.txt', 'user_id': '76eea168-9f98-4672-af3b-2c26ec92d7f8', 'version': 'v0', 'chunk_order': 26, 'document_id': '4bb1e5e0-3bb3-54e0-bc71-69e68bce30c7', 'extraction_id': '9401dfe6-10dd-5eb1-8b88-de1927a6c556', 'associatedQuery': 'Sample search query'}}, {'id': 'f54c9cda-0053-5ea2-a22b-aaba6437518c', 'score': 0.270276627830369, 'metadata': {'text': 'Categories\nOn Interpretation\nPrior Analytics\nPosterior Analytics\nTopics\nOn Sophistical Refutations', 'title': 'aristotle.txt', 'user_id': '2acb499e-8428-543b-bd85-0d9098718220', 'version': 'v0', 'chunk_order': 26, 'document_id': '9fbe403b-c11c-5aae-8ade-ef22980c3ad1', 'extraction_id': 'bc497a0c-4b17-5e86-97d4-aa06474e0e5b', 'associatedQuery': 'Sample search query'}}, {'id': 'd0675bcd-23d1-5982-8114-1a6459faec3f', 'score': 0.242980153623792, 'metadata': {'text': 'Because Aristotle believes people receive all kinds of sense perceptions and perceive them as impressions, people are continually weaving together new impressions of experiences. To search for these impressions, people search the memory itself.[105] Within the memory, if one experience is offered instead of a specific memory, that person will reject this experience until they find what they are looking for. Recollection occurs when one retrieved experience naturally follows another. If the chain of', 'title': 'aristotle.txt', 'user_id': '76eea168-9f98-4672-af3b-2c26ec92d7f8', 'version': 'v0', 'chunk_order': 119, 'document_id': '4bb1e5e0-3bb3-54e0-bc71-69e68bce30c7', 'extraction_id': '9401dfe6-10dd-5eb1-8b88-de1927a6c556', 'associatedQuery': 'Sample search query'}}, {'id': '69aed771-061f-5360-90f1-0ce395601b98', 'score': 0.242980153623792, 'metadata': {'text': 'Because Aristotle believes people receive all kinds of sense perceptions and perceive them as impressions, people are continually weaving together new impressions of experiences. To search for these impressions, people search the memory itself.[105] Within the memory, if one experience is offered instead of a specific memory, that person will reject this experience until they find what they are looking for. Recollection occurs when one retrieved experience naturally follows another. If the chain of', 'title': 'aristotle.txt', 'user_id': '2acb499e-8428-543b-bd85-0d9098718220', 'version': 'v0', 'chunk_order': 119, 'document_id': '9fbe403b-c11c-5aae-8ade-ef22980c3ad1', 'extraction_id': 'bc497a0c-4b17-5e86-97d4-aa06474e0e5b', 'associatedQuery': 'Sample search query'}}, {'id': 'dadd2d48-a2b7-5e55-9a8c-1030712c5ca0', 'score': 0.20218510005651702, 'metadata': {'text': 'Medieval Judaism', 'title': 'aristotle.txt', 'user_id': '76eea168-9f98-4672-af3b-2c26ec92d7f8', 'version': 'v0', 'chunk_order': 202, 'document_id': '4bb1e5e0-3bb3-54e0-bc71-69e68bce30c7', 'extraction_id': '9401dfe6-10dd-5eb1-8b88-de1927a6c556', 'associatedQuery': 'Sample search query'}}, {'id': 'da81f692-40d9-599b-a69b-25b6a5179b47', 'score': 0.20218510005651702, 'metadata': {'text': 'Medieval Judaism', 'title': 'aristotle.txt', 'user_id': '2acb499e-8428-543b-bd85-0d9098718220', 'version': 'v0', 'chunk_order': 202, 'document_id': '9fbe403b-c11c-5aae-8ade-ef22980c3ad1', 'extraction_id': 'bc497a0c-4b17-5e86-97d4-aa06474e0e5b', 'associatedQuery': 'Sample search query'}}, {'id': '0c4fea20-f7ee-520f-ae1f-155ecb398e1f', 'score': 0.19056136124594703, 'metadata': {'text': 'Scientific style', 'title': 'aristotle.txt', 'user_id': '2acb499e-8428-543b-bd85-0d9098718220', 'version': 'v0', 'chunk_order': 92, 'document_id': '9fbe403b-c11c-5aae-8ade-ef22980c3ad1', 'extraction_id': 'bc497a0c-4b17-5e86-97d4-aa06474e0e5b', 'associatedQuery': 'Sample search query'}}, {'id': 'c3c3145a-5d9d-5362-9629-f9159a027a9d', 'score': 0.19051768949311598, 'metadata': {'text': 'Scientific style', 'title': 'aristotle.txt', 'user_id': '76eea168-9f98-4672-af3b-2c26ec92d7f8', 'version': 'v0', 'chunk_order': 92, 'document_id': '4bb1e5e0-3bb3-54e0-bc71-69e68bce30c7', 'extraction_id': '9401dfe6-10dd-5eb1-8b88-de1927a6c556', 'associatedQuery': 'Sample search query'}}, {'id': '63e3a252-90bd-5494-9f9f-aee772f4db54', 'score': 0.18900877964391904, 'metadata': {'text': 'Apellicon sought to recover the texts, many of which were seriously degraded at this point due to the conditions in which they were stored. He had them copied out into new manuscripts, and used his best guesswork to fill in the gaps where the originals were unreadable.[216]:\u200a5‚Äì6', 'title': 'aristotle.txt', 'user_id': '76eea168-9f98-4672-af3b-2c26ec92d7f8', 'version': 'v0', 'chunk_order': 228, 'document_id': '4bb1e5e0-3bb3-54e0-bc71-69e68bce30c7', 'extraction_id': '9401dfe6-10dd-5eb1-8b88-de1927a6c556', 'associatedQuery': 'Sample search query'}}, {'id': '2c1183a8-e130-5432-a311-ee1f0f194562', 'score': 0.18894388145542895, 'metadata': {'text': 'Apellicon sought to recover the texts, many of which were seriously degraded at this point due to the conditions in which they were stored. He had them copied out into new manuscripts, and used his best guesswork to fill in the gaps where the originals were unreadable.[216]:\u200a5‚Äì6', 'title': 'aristotle.txt', 'user_id': '2acb499e-8428-543b-bd85-0d9098718220', 'version': 'v0', 'chunk_order': 228, 'document_id': '9fbe403b-c11c-5aae-8ade-ef22980c3ad1', 'extraction_id': 'bc497a0c-4b17-5e86-97d4-aa06474e0e5b', 'associatedQuery': 'Sample search query'}}], 'kg_search_results': None}}}
```

## Advanced Authentication Features

R2R offers several advanced authentication features to enhance security and user experience:

### Password Management

Users can change their passwords and request password resets:

```python
# Change password
change_password_result = client.users.change_password("password123", "new_password")
# {"result": {"message": "Password changed successfully"}}

# Request password reset
reset_request_result = client.users.request_password_reset("user@example.com")
# {"result": {"message": "If the email exists, a reset link has been sent"}}

# Confirm password reset (after user receives reset token)
reset_confirm_result = client.users.confirm_password_reset("reset_token_here", "new_password")
# {"result": {"message": "Password reset successfully"}}
```

### User Profile Management

Users can view and update their profiles:

```python
# Update user profile, requires login
update_result = client.users.update_user(name="John Doe", bio="R2R enthusiast")
# {'results': {'email': 'user1@test.com', 'id': '76eea168-9f98-4672-af3b-2c26ec92d7f8', 'hashed_password': 'null', 'is_superuser': False, 'is_active': True, 'is_verified': True, 'verification_code_expiry': None, 'name': 'John Doe', 'bio': 'R2R enthusiast', 'profile_picture': None, 'created_at': '2024-07-16T23:06:42.123303Z', 'updated_at': '2024-07-16T23:22:48.256239Z'}}
```

### Account Deletion

Users can delete their accounts:

```python
# Delete account (requires password confirmation)
user_id = register_response["results"]["id"] # input unique id here
delete_result = client.delete_user(user_id, "password123")
# {'results': {'message': 'User account deleted successfully'}}
```

### Logout

To end a user session:

```python
logout_result = client.users.logout()
print(f"Logout Result:\n{logout_result}")
# {'results': {'message': 'Logged out successfully'}}
```

## Superuser Capabilities and Default Admin Creation

R2R includes powerful superuser capabilities and a mechanism for default admin creation, which are crucial for system management and initial setup. Let's explore these features:

### Superuser Capabilities

Superusers in R2R have elevated privileges that allow them to perform system-wide operations and access sensitive information. Some key superuser capabilities include:

1. **User Management**: Superusers can view, modify, and delete user accounts.
2. **System-wide Document Access**: They can access and manage documents across all users.
3. **Analytics and Observability**: Superusers have access to system-wide analytics and logs.
4. **Configuration Management**: They can modify system configurations and settings.

To use superuser capabilities, you need to authenticate as a superuser. The methods for accessing these features are the same as regular user methods, but with expanded scope and permissions.

### Default Admin Creation

R2R automatically creates a default admin user during initialization. This process is handled by the `R2RAuthProvider` class. Here's how it works:

1. During system initialization, R2R attempts to create a default admin user.
2. The admin email and password are typically set through environment variables or configuration files.
3. If the admin user already exists, R2R logs this information and continues without creating a duplicate.

The relevant part of the configuration that affects this process is:

```toml
[auth]
provider = "r2r"
access_token_lifetime_in_minutes = 60
refresh_token_lifetime_in_days = 7
require_authentication = true
require_email_verification = false
default_admin_email = "admin@example.com"
default_admin_password = "change_me_immediately"
```

* With `"require_authentication": false`, the system allows unauthenticated access for testing and development. In a production environment, this should be set to `true`.
* `"require_email_verification": false` means that email verification is not required for new users, including the default admin. For increased security in production, consider enabling this.

### Accessing Superuser Features

To access superuser features, you need to authenticate as the default admin or another user with superuser privileges. Here's an example of how to do this:

```python

from r2r import R2RClient

client = R2RClient("http://localhost:7272")

# Login as admin
login_result = client.users.login("admin@example.com", "change_me_immediately")

# Now you can access superuser features, for example:
users_overview = client.users.list()
# {'results': [{'user_id': '2acb499e-8428-543b-bd85-0d9098718220', 'num_files': 2, 'total_size_in_bytes': 73672, 'document_ids': ['c4967f03-1780-5161-8e1d-57b55aa65076', '9fbe403b-c11c-5aae-8ade-ef22980c3ad1']}, {'user_id': 'ac730ec3-7d3d-451a-a166-e7ac7c57b198', 'num_files': 1, 'total_size_in_bytes': 73353, 'document_ids': ['d4861e78-cf02-5184-9b6a-d5bdbddd39b2']}, {'user_id': 'e0514342-e51a-43e5-8aaa-665468102dce', 'num_files': 1, 'total_size_in_bytes': 73353, 'document_ids': ['f4fbe534-b7d6-5fec-9d41-9093b2112732']}]}

# Access system-wide logs
logs = client.logs()
# {'results': [{'run_id': '645cb90f-c281-4188-b93b-94fb383170f6', 'run_type': 'search', 'entries': [{'key': 'search_latency', 'value': '0.43'}, { ...

# Perform analytics
analytics_result = client.analytics(
    {"all_latencies": "search_latency"},
    {"search_latencies": ["basic_statistics", "search_latency"]}
)
# {'results': {'filtered_logs': {'search_latencies': [{'timestamp': '2024-07-18 18:10:26', 'log_id': '645cb90f-c281-4188-b93b-94fb383170f6', 'key': 'search_latency', 'value': '0.43', 'rn': 3}, {'timestamp': '2024-07-18 18:04:54', 'log_id': 'a22d6a4c-3e68-4f01-b129-c9cbb5ae0b86', 'key': 'search_latency', 'value': '0.76', 'rn': 3}, {'timestamp': '2024-07-18 17:45:04', 'log_id': '253aa7b2-5abc-46d4-9bc3-f1ee47598bc5', 'key': 'search_latency', 'value': '0.43', 'rn': 3}, {'timestamp': '2024-07-18 17:44:47', 'log_id': 'add3d166-392c-44e0-aec2-d00d97a584f9', 'key': 'search_latency', 'value': '0.71', 'rn': 3}, {'timestamp': '2024-07-18 17:43:40', 'log_id': '9b64d038-aa56-44c9-af5e-96091fff62f2', 'key': 'search_latency', 'value': '0.44', 'rn': 3}, {'timestamp': '2024-07-18 17:43:20', 'log_id': '2a433a26-dc5b-4460-823a-58f01070e44d', 'key': 'search_latency', 'value': '0.37', 'rn': 3}, {'timestamp': '2024-07-18 17:43:16', 'log_id': '71a05fb2-5993-45c3-af3d-5019a745a33d', 'key': 'search_latency', 'value': '0.72', 'rn': 3}, {'timestamp': '2024-07-16 23:19:35', 'log_id': 'fada5559-ccd1-42f3-81a1-c96dcbc2ff08', 'key': 'search_latency', 'value': '0.34', 'rn': 3}, {'timestamp': '2024-07-16 23:07:32', 'log_id': '530bc25c-efc9-4b10-b46f-529c64c89fdf', 'key': 'search_latency', 'value': '0.62', 'rn': 3}, {'timestamp': '2024-07-16 23:07:14', 'log_id': '5c977046-bd71-4d95-80ba-dcf16e33cfd5', 'key': 'search_latency', 'value': '0.72', 'rn': 3}, {'timestamp': '2024-07-16 23:06:44', 'log_id': 'a2bef456-7370-4977-83cf-a60de5abd0cf', 'key': 'search_latency', 'value': '0.44', 'rn': 3}, {'timestamp': '2024-07-16 23:02:10', 'log_id': 'ee8d8bb4-7bc5-4c92-a9a2-c3ddd9f6bcd9', 'key': 'search_latency', 'value': '0.53', 'rn': 3}, {'timestamp': '2024-07-16 22:00:48', 'log_id': '13701f93-a2ab-4182-abd0-a401aa8e720a', 'key': 'search_latency', 'value': '0.52', 'rn': 3}, {'timestamp': '2024-07-16 21:59:17', 'log_id': '17bd24d7-37e2-4838-9830-c92110f492c7', 'key': 'search_latency', 'value': '0.30', 'rn': 3}, {'timestamp': '2024-07-16 21:59:16', 'log_id': 'd1d1551b-80cc-4f93-878a-7d7579aa3b9e', 'key': 'search_latency', 'value': '0.43', 'rn': 3}, {'timestamp': '2024-07-16 21:55:46', 'log_id': '0291254e-262f-4d9d-ace2-b98c2eaae547', 'key': 'search_latency', 'value': '0.42', 'rn': 3}, {'timestamp': '2024-07-16 21:55:45', 'log_id': '74df032f-e9d6-4ba9-ae0e-1be58927c2b1', 'key': 'search_latency', 'value': '0.57', 'rn': 3}, {'timestamp': '2024-07-16 21:54:36', 'log_id': '413982fd-3588-42cc-8009-8c686a85f27e', 'key': 'search_latency', 'value': '0.55', 'rn': 3}, {'timestamp': '2024-07-16 03:35:48', 'log_id': 'ae79062e-f4f0-4fb1-90f4-4ddcb8ae0cc4', 'key': 'search_latency', 'value': '0.50', 'rn': 3}, {'timestamp': '2024-07-16 03:26:10', 'log_id': '9fd51a36-9fdf-4a70-89cb-cb0d43dd0b63', 'key': 'search_latency', 'value': '0.41', 'rn': 3}, {'timestamp': '2024-07-16 03:01:18', 'log_id': '6cb79d2e-c431-447e-bbbb-99f96d56784e', 'key': 'search_latency', 'value': '0.67', 'rn': 3}, {'timestamp': '2024-07-16 01:29:44', 'log_id': '34eea2d3-dd98-47fb-ae3b-05e1001850a5', 'key': 'search_latency', 'value': '0.42', 'rn': 3}, {'timestamp': '2024-07-16 01:29:25', 'log_id': '5204d260-4ad3-49ce-9b38-1043ceae65ac', 'key': 'search_latency', 'value': '0.58', 'rn': 3}]}, 'search_latencies': {'Mean': 0.517, 'Median': 0.5, 'Mode': 0.43, 'Standard Deviation': 0.132, 'Variance': 0.018}}}

```

### Security Considerations for Superusers

When using superuser capabilities, keep the following security considerations in mind:

1. **Limit Superuser Access**: Only grant superuser privileges to trusted individuals who require full system access.
2. **Use Strong Passwords**: Ensure that superuser accounts, especially the default admin, use strong, unique passwords.
3. **Enable Authentication and Verification**: In production, set `"require_authentication": true` and `"require_email_verification": true` for enhanced security.
4. **Audit Superuser Actions**: Regularly review logs of superuser activities to detect any unusual or unauthorized actions.
5. **Rotate Credentials**: Periodically update superuser credentials, including the default admin password.

By understanding and properly managing superuser capabilities and default admin creation, you can ensure secure and effective administration of your R2R deployment.

## Security Considerations

When implementing user authentication, consider the following security best practices:

1. **Use HTTPS**: Always use HTTPS in production to encrypt data in transit.
2. **Implement rate limiting**: Protect against brute-force attacks by limiting login attempts.
3. **Use secure password hashing**: R2R uses bcrypt for password hashing by default, which is a secure choice.
4. **Implement multi-factor authentication (MFA)**: Consider adding MFA for an extra layer of security.
5. **Regular security audits**: Conduct regular security audits of your authentication system.

## Customizing Authentication

R2R's authentication system is flexible and can be customized to fit your specific needs:

1. **Custom user fields**: Extend the User model to include additional fields.
2. **OAuth integration**: Integrate with third-party OAuth providers for social login.
3. **Custom password policies**: Implement custom password strength requirements.
4. **User roles and permissions**: Implement a role-based access control system.

## Troubleshooting

Here are some common issues and their solutions:

1. **Login fails after registration**: Ensure email verification is completed if enabled.
2. **Token refresh fails**: Check if the refresh token has expired; the user may need to log in again.
3. **Unable to change password**: Verify that the current password is correct.

## Conclusion

R2R provides a comprehensive set of user authentication and management features, allowing developers to create secure and user-friendly applications. By leveraging these capabilities, you can implement robust user authentication, document management, and access control in your R2R-based projects.

For more advanced use cases or custom implementations, refer to the R2R documentation or reach out to the community for support.


# Collections

&gt; A comprehensive guide to creating collections in R2R

## Introduction

A collection in R2R is a logical grouping of users and documents that allows for efficient access control and organization. Collections enable you to manage permissions and access to documents at a group level, rather than individually.

R2R provides robust document collection management, allowing developers to implement efficient access control and organization of users and documents. This cookbook will guide you through the collection capabilities in R2R.

For user authentication, please refer to the [User Auth Cookbook](/documentation/user-auth).

<note>
  Collection permissioning in R2R is still under development and as a result the is likely to API continue evolving in future releases.
</note>

```mermaid
graph TD
    A[User] --&gt; B(Authentication)
    B --&gt; C{Authenticated?}
    C --&gt;|Yes| D[Authenticated User]
    C --&gt;|No| E[Guest User]
    D --&gt; F[Collection Management]
    D --&gt; G[Document Management]
    D --&gt; H[User Profile Management]
    G --&gt; I[CRUD Operations]
    G --&gt; J[Search &amp; RAG]
    D --&gt; K[Logout]
    L[Admin] --&gt; M[Superuser Authentication]
    M --&gt; N[Superuser Capabilities]
    N --&gt; O[User Management]
    N --&gt; P[System-wide Document Access]
    N --&gt; R[Configuration Management]
```

*A diagram showing user and collection management across r2r*

## Basic Usage

<info>
  Collections currently follow a flat hierarchy wherein superusers are responsible for management operations. This functionality will expand as development on R2R continues.
</info>

### Collection CRUD operations

Let's start by creating a new collection:

```python
from r2r import R2RClient

client = R2RClient("http://localhost:7272")  # Replace with your R2R deployment URL

# Assuming you're logged in as an admin or a user with appropriate permissions
# For testing, the default R2R implementation will grant superuser privileges to anon api calls
collection_result = client.collections.create("Marketing Team", "Collection for marketing department")

print(f"Collection creation result: {collection_result}")
# {'results': {'collection_id': '123e4567-e89b-12d3-a456-426614174000', 'name': 'Marketing Team', 'description': 'Collection for marketing department', 'created_at': '2024-07-16T22:53:47.524794Z', 'updated_at': '2024-07-16T22:53:47.524794Z'}}
```

To retrieve details about a specific collection:

```python
collection_id = '123e4567-e89b-12d3-a456-426614174000'  # Use the collection_id from the creation result
collection_details = client.collections.retrieve(collection_id)

print(f"Collection details: {collection_details}")
# {'results': {'collection_id': '123e4567-e89b-12d3-a456-426614174000', 'name': 'Marketing Team', 'description': 'Collection for marketing department', 'created_at': '2024-07-16T22:53:47.524794Z', 'updated_at': '2024-07-16T22:53:47.524794Z'}}
```

You can update a collection's name or description:

```python
update_result = client.collections.update(
    collection_id,
    name="Updated Marketing Team",
    description="New description for marketing team"
)

print(f"Collection update result: {update_result}")
# {'results': {'collection_id': '123e4567-e89b-12d3-a456-426614174000', 'name': 'Updated Marketing Team', 'description': 'New description for marketing team', 'created_at': '2024-07-16T22:53:47.524794Z', 'updated_at': '2024-07-16T23:15:30.123456Z'}}
```

Lastly, you can delete a collection

### Listing Collections

```python
client.collections.delete(collection_id)
```

To get a list of all collections:

```python
collections_list = client.collections.list()

print(f"Collections list: {collections_list}")
# {'results': [{'collection_id': '123e4567-e89b-12d3-a456-426614174000', 'name': 'Updated Marketing Team', 'description': 'New description for marketing team', 'created_at': '2024-07-16T22:53:47.524794Z', 'updated_at': '2024-07-16T23:15:30.123456Z'}, ...]}
```

## User Management in Collections

### Adding a User to a Collection

To add a user to a collection, you need both the user's ID and the collections's ID:

```python
user_id = '456e789f-g01h-34i5-j678-901234567890'  # This should be a valid user ID
collection_id = '123e4567-e89b-12d3-a456-426614174000' # this should be a collection I own
add_user_result = client.collections.add_user(user_id, collection_id)

print(f"Add user to collection result: {add_user_result}")
# {'results': {'message': 'User successfully added to the collection'}}
```

### Removing a User from a Collections

Similarly, to remove a user from a collection:

```python
remove_user_result = client.collections.remove_user(user_id, collection_id)

print(f"Remove user from collection result: {remove_user_result}")
# {'results': None}
```

### Listing Users in a Collection

To get a list of all users in a specific collection:

```python
users_in_collection = client.collections.list_users(collection_id)

print(f"Users in collection: {users_in_collection}")
# {'results': [{'user_id': '456e789f-g01h-34i5-j678-901234567890', 'email': 'user@example.com', 'name': 'John Doe', ...}, ...]}
```

### Getting Collections for a User

To get all collections that a user is a member of:

```python
user.list_collections = client.user.list_collections(user_id)

print(f"User's collections: {user.list_collections}")
# {'results': [{'collection_id': '123e4567-e89b-12d3-a456-426614174000', 'name': 'Updated Marketing Team', ...}, ...]}
```

## Document Management in Collections

### Assigning a Document to a Collection

To assign a document to a collection:

```python
document_id = '789g012j-k34l-56m7-n890-123456789012'  # This should be a valid document ID
assign_doc_result = client.collections.add_document(collection_id, document_id)

print(f"Assign document to collection result: {assign_doc_result}")
# {'results': {'message': 'Document successfully assigned to the collection'}}
```

### Removing a Document from a Collection

To remove a document from a collection:

```python
remove_doc_result = client.collections.remove_document(collection_id, document_id)

print(f"Remove document from collection result: {remove_doc_result}")
# {'results': {'message': 'Document successfully removed from the collection'}}
```

### Listing Documents in a Collection

To get a list of all documents in a specific collection:

```python
docs_in_collection = client.collections.list_documents(collection_id)

print(f"Documents in collection: {docs_in_collection}")
# {'results': [{'document_id': '789g012j-k34l-56m7-n890-123456789012', 'title': 'Marketing Strategy 2024', ...}, ...]}
```

### Getting Collections for a Document

To get all collections that a document is assigned to:

```python
documents.list_collections = client.documents.list_collections(document_id)

print(f"Document's collections: {documents.list_collections}")
# {'results': [{'collection_id': '123e4567-e89b-12d3-a456-426614174000', 'name': 'Updated Marketing Team', ...}, ...]}
```

## Advanced Collection Management

### Generating Synthetic Descriptions

To have an LLM generate a description for a collection, you can run:

```python
update_result = client.collections.update(
    collection_id,
    generate_description=True
)

print(f"Collection update result: {update_result}")
# {'results': {'collection_id': '123e4567-e89b-12d3-a456-426614174000', 'name': 'Updated Marketing Team', 'description': 'A rich description generated over the summaries of the documents in the collection', 'created_at': '2024-07-16T22:53:47.524794Z', 'updated_at': '2024-07-16T23:15:30.123456Z'}}
```

This is particularly helpful when building graphs as the summary provides high-quality context in the prompt, resulting in better descriptions.

### Collection Overview

To get an overview of collection, including user and document counts:

```python
collections.list = client.collections.list()

print(f"Collections overview: {collections.list}")
# {'results': [{'collection_id': '123e4567-e89b-12d3-a456-426614174000', 'name': 'Updated Marketing Team', 'description': 'New description for marketing team', 'user_count': 5, 'document_count': 10, ...}, ...]}
```

### Deleting a Collection

To delete a collection:

```python
delete_result = client.delete_collection(collection_id)

print(f"Delete collection result: {delete_result}")
# {'results': {'message': 'Collection successfully deleted'}}
```

## Pagination and Filtering

Many of the collection-related methods support pagination and filtering. Here are some examples:

```python
# List collections with pagination
paginated_collection = client.collections.list(offset=10, limit=20)

# Get users in a collection with pagination
paginated_users = client.collections.list_users(collection_id, offset=5, limit=10)

# Get documents in a collection with pagination
paginated_docs = client.collections.list_documents(collection_id, offset=0, limit=50)

# Get collections overview with specific collection IDs
specific_collections.list = client.collections.list(collection_ids=['id1', 'id2', 'id3'])
```

## Security Considerations

When implementing collection permissions, consider the following security best practices:

1. **Least Privilege Principle**: Assign the minimum necessary permissions to users and collections.
2. **Regular Audits**: Periodically review collection memberships and document assignments.
3. **Access Control**: Ensure that only authorized users (e.g., admins) can perform collection management operations.
4. **Logging and Monitoring**: Implement comprehensive logging for all collection-related actions.

## Customizing Collection Permissions

While R2R's current collection system follows a flat hierarchy, you can build more complex permission structures on top of it:

1. **Custom Roles**: Implement application-level roles within collections (e.g., collection admin, editor, viewer).
2. **Hierarchical Collections**: Create a hierarchy by establishing parent-child relationships between collections in your application logic.
3. **Permission Inheritance**: Implement rules for permission inheritance based on collection memberships.

## Troubleshooting

Here are some common issues and their solutions:

1. **Unable to Create/Modify Collections**: Ensure the user has superuser privileges.
2. **User Not Seeing Collection Content**: Verify that the user is correctly added to the collection and that documents are properly assigned.
3. **Performance Issues with Large Collections**: Use pagination when retrieving users or documents in large collections.

## Conclusion

R2R's collection permissioning system provides a foundation for implementing sophisticated access control in your applications. As the feature set evolves, more advanced capabilities will become available. Stay tuned to the R2R documentation for updates and new features related to collection permissions.

For user authentication and individual user management, refer to the [User Auth Cookbook](/documentation/user-auth). For more advanced use cases or custom implementations, consult the R2R documentation or reach out to the community for support.


# Application

&gt; Learn how to set up and use the R2R Application for managing your instance.

R2R offers an [open-source React+Next.js application](https://github.com/SciPhi-AI/R2R-Application) designed to give developers an administrative portal for their R2R deployment, and users an application to communicate with out of the box.

## Setup

### Install PNPM

PNPM is a fast, disk space-efficient package manager. To install PNPM, visit the [official PNPM installation page](https://pnpm.io/installation) or follow these instructions:

<accordiongroup>
  <accordion icon="terminal" title="PNPM Installation">
    For Unix-based systems (Linux, macOS):

    ```zsh
    curl -fsSL https://get.pnpm.io/install.sh | sh -
    ```

    For Windows:

    ```powershell
    iwr https://get.pnpm.io/install.ps1 -useb | iex
    ```

    After installation, you may need to add PNPM to your system's PATH.
  </accordion>
</accordiongroup>

### Installing and Running the R2R Dashboard

If you're running R2R with the Docker, you already have the R2R application running! Just navigate to [http://localhost:7273](http://localhost:7273).

If you're running R2R outside of Docker, run the following commands to install the R2R Dashboard.

1. Clone the project repository and navigate to the project directory:

```zsh
git clone https://github.com/SciPhi-AI/R2R.git
cd R2R-Application
```

2. Install the project dependencies:

```zsh
pnpm install
```

3. Build and start the application for production:

```zsh
pnpm build
pnpm start
```

The dashboard will be available at [http://localhost:3000](http://localhost:3000).

## Features

### Login

To interact with R2R with the dashboard, you must first login. If it's your first time logging in, log in with the default credentials shown.

By default, an R2R instance is hosted on port 7272. The login page will include this URL by default, but be sure to update the URL if your R2R instance is deployed elsewhere. For information about deploying a local R2R application server, see the [quickstart](/self-hosting/quickstart).

![R2R Dashboard Overview](file:77c27df9-1ada-482d-94b0-32bb3fa61cf7)

### Documents

The documents page provides an overview of uploaded documents and their metadata. You can upload new documents and update, download, or delete existing ones. Additionally, you can view information about each document, including the documents' chunks and previews of PDFs.

![Documents Page](file:bb2159ff-fd49-4138-bf1b-158def991689)

### Collections

Collections allow users to create and share sets of documents. The collections page provides a place to manage your existing collections or create new collections.

![Collections Page](file:0069afbe-ad49-4f92-bb60-b7cc0575d3b9)

### Chat

In the chat page, you can stream RAG responses with different models and configurable settings. You can interact with both the RAG Agent and RAG endpoints here.

![Chat Interface](file:993b2b80-0977-419a-904d-35dedd5b8765)

### Users

Manage your users and gain insight into their interactions.

![Users Page](file:ca568c35-a5fe-4647-a0ed-db26d2fcbd58)

### Logs

The Logs page enables tracking of user queries, search results, and LLM responses.

![Logs Page](file:49ed8cd6-ce49-4f9e-a1ee-a0c15384f072)

### Settings

The settings page allows you to view the configuration of and edit the prompts associated with your R2R deployment.

![Logs Page](file:f23e2778-3740-4b86-ba46-bf8c7e96ef58)
![Logs Page](file:88b5ae99-b2e5-416b-a497-af25e0930f90)

## Development

To develop the R2R dashboard:

1. Start the development server:

```zsh
pnpm dev
```

2. Run pre-commit checks (optional but recommended):

```zsh
pnpm format
pnpm lint
```


# Telemetry

&gt; Learn about R2R telemetry and how to manage it

R2R uses telemetry to collect **anonymous** usage information. This data helps us understand how R2R is used, prioritize new features and bug fixes, and improve overall performance and stability.

## Disabling Telemetry

To opt out of telemetry, you can set an environment variable:

```zsh
export TELEMETRY_ENABLED=false
```

<note>
  Valid values to disable telemetry are `false`, `0`, or `f`. When telemetry is disabled, no events will be captured.
</note>

## Collected Information

Our telemetry system collects basic, anonymous information such as:

* **Feature Usage**: Which features are being used and their frequency of use.

## Data Storage

<accordiongroup>
  <accordion icon="database" title="Telemetry Data Storage">
    We use [Posthog](https://posthog.com/) to store and analyze telemetry data. Posthog is an open-source platform for product analytics.

    For more information about Posthog:

    * Visit their website: [posthog.com](https://posthog.com/)
    * Check out their GitHub repository: [github.com/posthog](https://github.com/posthog)
  </accordion>
</accordiongroup>

## Why We Collect Telemetry

Telemetry data helps us:

1. Understand which features are most valuable to users
2. Identify areas for improvement
3. Prioritize development efforts
4. Enhance R2R's overall performance and stability

We appreciate your participation in our telemetry program, as it directly contributes to making R2R better for everyone.

<note>
  We respect your privacy. All collected data is anonymous and used solely for improving R2R.
</note>
</assuming></container_id></container_id></container_id></div></accordion></graphextractionstatus.pending:></ingestionstatus.success:></documenttype.pdf:></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></token></graphextractionstatus.pending:></ingestionstatus.success:></documenttype.pdf:></pre>
  
</body></html>